{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c997915c-d6d3-45ae-acab-1bad29e0beda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/anndata/__init__.py:51: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import anndata\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "adata = sc.read('/tmp/work/RCproject_code/sce_export.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50d5324f-d3b6-4946-ac4e-67cc5e70a9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate numerical values for each batch category\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# set up categories variable\n",
    "categories = adata.obs['batch']\n",
    "# Create a LabelEncoder instance\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit and transform the categories to integers\n",
    "numerical_categories = label_encoder.fit_transform(categories)\n",
    "numerical_categories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a88b4c24-3a34-4b78-9e30-86a4af026473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizaiton\n",
    "gene_expression_data = adata.layers['logcounts']\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Min-max normalization\n",
    "scaler = MinMaxScaler()\n",
    "gene_expression_data = scaler.fit_transform(gene_expression_data)\n",
    "number_samples = adata.shape[0]\n",
    "number_genes = adata.shape[1]\n",
    "input_dim = number_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "237bad59-1013-4602-aa8b-21492f710ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape = gene_expression_data[0, :].shape  # This should be (12165,) if that's the feature count\n",
    "# encoding_dim = 64  # This is the size of the encoded representation\n",
    "\n",
    "input_shape = (12165,)  # Set to match your actual data\n",
    "encoding_dim = 64  # Output dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a768a240-c514-466f-ad99-68c6c388994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (12165,)  # Number of genes\n",
    "\n",
    "# Define the encoder function\n",
    "def build_encoder():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(12165,)))  # Set to match the input shape of 12165\n",
    "    model.add(layers.Dense(256, activation='linear'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(128, activation='linear'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(64, activation='linear'))  # Encoded representation\n",
    "    return model\n",
    "\n",
    "# Define the decoder function (adjusted as discussed)\n",
    "def build_decoder():    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(64,)))  # Input shape should match the output of the encoder\n",
    "    model.add(layers.Dense(128, activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(256, activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(12165, activation='sigmoid'))  # Output layer should match the input shape of the original data\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "260e159f-5f3b-4f7b-a944-3a5101f7b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the domain classifier function\n",
    "def build_domain_classifier(input_shape, num_domains):\n",
    "    model = models.Sequential()    \n",
    "    model.add(layers.Input(shape=input_shape))    \n",
    "    model.add(layers.Dense(128, activation=None))\n",
    "    model.add(layers.LeakyReLU()) \n",
    "    model.add(layers.BatchNormalization())    \n",
    "    model.add(layers.Dense(64, activation=None))\n",
    "    model.add(layers.LeakyReLU()) \n",
    "    model.add(layers.BatchNormalization())    \n",
    "    model.add(layers.Dense(32, activation=None))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())    \n",
    "    model.add(layers.Dense(num_domains, activation='softmax'))  # num_domains is the number of classes\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3352b96-85c0-4d91-b573-b0c0d150c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establishes a gradient reversal class\n",
    "\n",
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, lambda_value=1.0, **kwargs):\n",
    "        self.lambda_value = lambda_value\n",
    "        super(GradientReversalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"lambda_value\": self.lambda_value})\n",
    "        return config\n",
    "\n",
    "    def compute_gradient(self, inputs):\n",
    "        return -self.lambda_value * inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c588400b-1dd9-4a80-bc93-1ac7aa222274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder, decoder, and discriminator (assuming these functions are defined as in previous examples)\n",
    "encoder = build_encoder()  # Assuming build_encoder() is defined\n",
    "decoder = build_decoder()  # Assuming build_decoder() is defined\n",
    "discriminator = build_domain_classifier((64,), 9)  # Assuming build_domain_classifier() is defined\n",
    "\n",
    "# Optimizers for each model\n",
    "encoder_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "decoder_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Assuming you have 1000 training samples as an example\n",
    "num_samples = 1000  # Replace this with your actual number of samples\n",
    "batch_size = 32\n",
    "\n",
    "# Calculate the number of steps per epoch\n",
    "num_steps_per_epoch = num_samples // batch_size\n",
    "\n",
    "# Compile the discriminator\n",
    "discriminator.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create a combined model for the encoder and decoder if training together\n",
    "combined_model = models.Sequential([encoder, decoder])\n",
    "\n",
    "# Compile combined model\n",
    "combined_model.compile(optimizer='adam', loss='mean_squared_error')  # Use an appropriate loss for your case\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e978967-4297-419f-8ad7-a4c2c83e6ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this wasn't actually work with real data, need to replace.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7874ddee-0f51-4c24-9d5d-d7e08eb3c852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 2.405923366546631, Reconstruction Loss: 0.09312045574188232, Domain Loss: 2.312802791595459\n",
      "Epoch 2/100, Loss: 2.3547680377960205, Reconstruction Loss: 0.08487919718027115, Domain Loss: 2.2698888778686523\n",
      "Epoch 3/100, Loss: 2.346843957901001, Reconstruction Loss: 0.08409430086612701, Domain Loss: 2.262749671936035\n",
      "Epoch 4/100, Loss: 2.255073308944702, Reconstruction Loss: 0.08387571573257446, Domain Loss: 2.1711976528167725\n",
      "Epoch 5/100, Loss: 2.3037197589874268, Reconstruction Loss: 0.0839821994304657, Domain Loss: 2.2197375297546387\n",
      "Epoch 6/100, Loss: 2.278475284576416, Reconstruction Loss: 0.08368973433971405, Domain Loss: 2.1947855949401855\n",
      "Epoch 7/100, Loss: 2.3167102336883545, Reconstruction Loss: 0.08371131867170334, Domain Loss: 2.232998847961426\n",
      "Epoch 8/100, Loss: 2.2950069904327393, Reconstruction Loss: 0.08376150578260422, Domain Loss: 2.211245536804199\n",
      "Epoch 9/100, Loss: 2.2754087448120117, Reconstruction Loss: 0.0836806669831276, Domain Loss: 2.191728115081787\n",
      "Epoch 10/100, Loss: 2.324418544769287, Reconstruction Loss: 0.08359738439321518, Domain Loss: 2.240821123123169\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m reconstruction_loss \u001b[38;5;241m+\u001b[39m domain_loss\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Calculate gradients and update weights\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m encoder_gradients \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m decoder_gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(total_loss, decoder\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[1;32m     28\u001b[0m discriminator_gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(domain_loss, discriminator\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1112\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1106\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1107\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1108\u001b[0m           output_gradients))\n\u001b[1;32m   1109\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1110\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1112\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1121\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:157\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    155\u001b[0m     gradient_name_scope \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m forward_pass_name_scope \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/math_grad.py:254\u001b[0m, in \u001b[0;36m_MeanGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (input_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m output_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m input_shape \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m output_shape):\n\u001b[1;32m    253\u001b[0m   input_size \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mprod(input_shape)\n\u001b[0;32m--> 254\u001b[0m   output_size \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m   factor \u001b[38;5;241m=\u001b[39m input_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(output_size, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    256\u001b[0m   factor \u001b[38;5;241m=\u001b[39m constant_op\u001b[38;5;241m.\u001b[39mconstant(factor, dtype\u001b[38;5;241m=\u001b[39msum_grad\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m<__array_function__ internals>:179\u001b[0m, in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2922\u001b[0m, in \u001b[0;36m_prod_dispatcher\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2805\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2806\u001b[0m \u001b[38;5;124;03m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[1;32m   2807\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2916\u001b[0m \u001b[38;5;124;03m    6\u001b[39;00m\n\u001b[1;32m   2917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39mminimum, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out,\n\u001b[1;32m   2919\u001b[0m                           keepdims\u001b[38;5;241m=\u001b[39mkeepdims, initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[0;32m-> 2922\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prod_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2923\u001b[0m                      initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2924\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[1;32m   2927\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_prod_dispatcher)\n\u001b[1;32m   2928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2929\u001b[0m          initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for step in range(num_steps_per_epoch):\n",
    "        # Get a batch of data\n",
    "        X_batch = np.random.rand(batch_size, 12165)  # Replace with your actual data\n",
    "        y_true = np.random.randint(0, 9, size=(batch_size,))  # Replace with actual domain labels\n",
    "        y_true = tf.keras.utils.to_categorical(y_true, num_classes=9)  # Convert labels to categorical\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Encode\n",
    "            encoded_output = encoder(X_batch, training=True)\n",
    "            # Decode\n",
    "            decoded_output = decoder(encoded_output, training=True)\n",
    "\n",
    "            # Domain output\n",
    "            domain_output = discriminator(encoded_output, training=True)\n",
    "\n",
    "            # Calculate losses\n",
    "            reconstruction_loss = tf.reduce_mean(tf.square(X_batch - decoded_output))  # Scalar\n",
    "            domain_loss = tf.keras.losses.categorical_crossentropy(y_true, domain_output)  # Shape: (batch_size, num_classes)\n",
    "            domain_loss = tf.reduce_mean(domain_loss)  # Average over the batch\n",
    "\n",
    "            # Total loss\n",
    "            total_loss = reconstruction_loss + domain_loss\n",
    "\n",
    "        # Calculate gradients and update weights\n",
    "        encoder_gradients = tape.gradient(total_loss, encoder.trainable_variables)\n",
    "        decoder_gradients = tape.gradient(total_loss, decoder.trainable_variables)\n",
    "        discriminator_gradients = tape.gradient(domain_loss, discriminator.trainable_variables)\n",
    "\n",
    "        # Apply gradients\n",
    "        encoder_optimizer.apply_gradients(zip(encoder_gradients, encoder.trainable_variables))\n",
    "        decoder_optimizer.apply_gradients(zip(decoder_gradients, decoder.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "\n",
    "    # Print out scalar loss values\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss.numpy()}, Reconstruction Loss: {reconstruction_loss.numpy()}, Domain Loss: {domain_loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07f0e24c-113b-44a7-b024-a6a0380d77bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 9)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4580af1-6f34-47d2-a763-b2e388f10f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
