{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf24bb4-d77f-476d-9820-3150a572b0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "2024-10-29 13:40:24.689263: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/anndata/__init__.py:51: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from keras.callbacks import EarlyStopping\n",
    "import seaborn as sb\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import pearsonr\n",
    "import time\n",
    "from keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "adata = sc.read('/tmp/work/RCproject_code/sce_export.h5ad')\n",
    "epoch_count = 0\n",
    "\n",
    "#exclude partial\n",
    "adata = adata[~adata.obs['Response'].isin(['partial']), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "989c276a-58db-4ebf-a60b-351f513ed2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_size = 0.2\n",
    "dropout_rate = 0.3\n",
    "balance = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a27d675a-c97a-4b4d-9ecf-7bd6aaea98f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(410,)\n"
     ]
    }
   ],
   "source": [
    "# Create a LabelEncoder instance\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "#set up the response variable\n",
    "categories_outcome = adata.obs['Response']\n",
    "numerical_categories_outcome = label_encoder.fit_transform(categories_outcome)\n",
    "print(numerical_categories_outcome.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fc7dfd2-f9ee-4b85-a79f-86e9304becf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalizaiton\n",
    "gene_expression_data = adata.layers['scalelogcounts']\n",
    "\n",
    "# Min-max normalization\n",
    "scaler = MinMaxScaler()\n",
    "gene_expression_data = scaler.fit_transform(gene_expression_data)\n",
    "number_genes = adata.shape[1]\n",
    "input_dim = number_genes\n",
    "\n",
    "# Initialize PCA and fit it to X_train\n",
    "n_components = 16  # You can adjust this based on your data\n",
    "pca = PCA(n_components=n_components)\n",
    "gene_expression_data_pca = pca.fit_transform(gene_expression_data)\n",
    "\n",
    "gene_expression_data_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d93970-d467-4c23-911d-2fd161797d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the test and train datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(gene_expression_data_pca, numerical_categories_outcome, test_size=test_set_size, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(gene_expression_data, numerical_categories_outcome, test_size=test_set_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0f626da-39f7-418e-8e73-f267b981e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape\n",
    "# input_shape = (gene_expression_data.shape[1],)[0]  # Number of genes\n",
    "\n",
    "input_shape = (X_train.shape[1],)[0]  # Number of genes\n",
    "\n",
    "\n",
    "def build_outcome_classifier():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(input_shape,)))  # Input shape matches your data\n",
    "    \n",
    "    # Add more hidden layers and increase units with Leaky ReLU\n",
    "    model.add(layers.Dense((64), activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.1),kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.1))  # Leaky ReLU helps with vanishing gradients\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(layers.Dense((32),activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.1),kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Reduce units in subsequent layers with Leaky ReLU\n",
    "    model.add(layers.Dense((16),activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.1),kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer for binary classification with sigmoid activation\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e85e8399-811c-42d2-8150-72318a156f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mode\n",
    "outcome_classifier = build_outcome_classifier()\n",
    "\n",
    "# set up the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# determine the sample and batch size\n",
    "num_samples = adata.shape[0]  # Replace this with your actual number of samples\n",
    "\n",
    "batch_size = 16  #determines how many samples are processed per batch, each epoch will process multiple batches\n",
    "\n",
    "# batch_size = adata.shape[0]\n",
    "\n",
    "# Calculate the number of steps per epoch\n",
    "num_steps_per_epoch = num_samples // batch_size\n",
    "\n",
    "# Compile the outcome discriminator\n",
    "outcome_classifier.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352950e-b42d-4d71-8297-8b8db5844d98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Outcome Loss: 2.9833252358436586, Average Accuracy: 0.5, Test Accuracy: 0.6829268336296082\n",
      "Epoch 1000, Average Outcome Loss: 0.6712268733978272, Average Accuracy: 0.61, Test Accuracy: 0.6829268336296082\n",
      "Epoch 2000, Average Outcome Loss: 0.661772015094757, Average Accuracy: 0.625, Test Accuracy: 0.6829268336296082\n",
      "Epoch 3000, Average Outcome Loss: 0.6382047057151794, Average Accuracy: 0.6675, Test Accuracy: 0.6829268336296082\n",
      "Epoch 4000, Average Outcome Loss: 0.6509076285362244, Average Accuracy: 0.645, Test Accuracy: 0.6829268336296082\n",
      "Epoch 5000, Average Outcome Loss: 0.6667433381080627, Average Accuracy: 0.6175, Test Accuracy: 0.6829268336296082\n",
      "Epoch 6000, Average Outcome Loss: 0.6375273442268372, Average Accuracy: 0.67, Test Accuracy: 0.6829268336296082\n",
      "Epoch 7000, Average Outcome Loss: 0.6662913656234741, Average Accuracy: 0.6175, Test Accuracy: 0.6829268336296082\n",
      "Epoch 8000, Average Outcome Loss: 0.6481714773178101, Average Accuracy: 0.65, Test Accuracy: 0.6829268336296082\n",
      "Epoch 9000, Average Outcome Loss: 0.6452739787101746, Average Accuracy: 0.655, Test Accuracy: 0.6829268336296082\n",
      "Epoch 10000, Average Outcome Loss: 0.653569598197937, Average Accuracy: 0.64, Test Accuracy: 0.6829268336296082\n",
      "Epoch 11000, Average Outcome Loss: 0.6364079475402832, Average Accuracy: 0.6675, Test Accuracy: 0.6829268336296082\n",
      "Epoch 12000, Average Outcome Loss: 0.656679995059967, Average Accuracy: 0.635, Test Accuracy: 0.6829268336296082\n",
      "Epoch 13000, Average Outcome Loss: 0.6390625739097595, Average Accuracy: 0.67, Test Accuracy: 0.6829268336296082\n",
      "Epoch 14000, Average Outcome Loss: 0.6438959980010986, Average Accuracy: 0.6575, Test Accuracy: 0.6829268336296082\n",
      "Epoch 15000, Average Outcome Loss: 0.6437207007408142, Average Accuracy: 0.6575, Test Accuracy: 0.6829268336296082\n",
      "Epoch 16000, Average Outcome Loss: 0.6264698648452759, Average Accuracy: 0.69, Test Accuracy: 0.6829268336296082\n",
      "Epoch 17000, Average Outcome Loss: 0.6630134153366088, Average Accuracy: 0.625, Test Accuracy: 0.6829268336296082\n",
      "Epoch 18000, Average Outcome Loss: 0.65353839635849, Average Accuracy: 0.64, Test Accuracy: 0.6829268336296082\n",
      "Epoch 19000, Average Outcome Loss: 0.6378649067878723, Average Accuracy: 0.665, Test Accuracy: 0.6829268336296082\n",
      "Epoch 20000, Average Outcome Loss: 0.6495568680763245, Average Accuracy: 0.6475, Test Accuracy: 0.6829268336296082\n",
      "Epoch 21000, Average Outcome Loss: 0.6377935171127319, Average Accuracy: 0.6725, Test Accuracy: 0.6829268336296082\n",
      "Epoch 22000, Average Outcome Loss: 0.6508534550666809, Average Accuracy: 0.6525, Test Accuracy: 0.6829268336296082\n",
      "Epoch 23000, Average Outcome Loss: 0.6244609975814819, Average Accuracy: 0.6875, Test Accuracy: 0.6829268336296082\n",
      "Epoch 24000, Average Outcome Loss: 0.6657782077789307, Average Accuracy: 0.6175, Test Accuracy: 0.6829268336296082\n",
      "Epoch 25000, Average Outcome Loss: 0.6396696949005127, Average Accuracy: 0.6625, Test Accuracy: 0.6829268336296082\n",
      "Epoch 26000, Average Outcome Loss: 0.6544524025917053, Average Accuracy: 0.64, Test Accuracy: 0.6829268336296082\n",
      "Epoch 27000, Average Outcome Loss: 0.6619365692138672, Average Accuracy: 0.625, Test Accuracy: 0.6829268336296082\n",
      "Epoch 28000, Average Outcome Loss: 0.645782573223114, Average Accuracy: 0.655, Test Accuracy: 0.6829268336296082\n"
     ]
    }
   ],
   "source": [
    "test_accuracy_list = []\n",
    "train_accuracy_list = []\n",
    "\n",
    "num_epochs = 1000000\n",
    "report_frequency = 1000\n",
    "accuracy_threshold = 0.8\n",
    "num_outcomes = len(np.unique(y_test))\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Define the training step for only the outcome classifier\n",
    "def train_step(data, outcome_labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass through the outcome classifier\n",
    "        outcome_predictions = outcome_classifier(data)\n",
    "\n",
    "        # Compute the biological discriminator loss\n",
    "        outcome_loss = tf.keras.losses.binary_crossentropy(outcome_labels, outcome_predictions)\n",
    "        outcome_loss = tf.reduce_mean(outcome_loss)  # Average over the batch\n",
    "\n",
    "    # Compute gradients for the outcome classifier\n",
    "    classifier_grads = tape.gradient(outcome_loss, outcome_classifier.trainable_variables)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.apply_gradients(zip(classifier_grads, outcome_classifier.trainable_variables))\n",
    "\n",
    "    # Calculate accuracy for the outcome classifier\n",
    "    predicted_outcome_labels = tf.cast(outcome_predictions > 0.5, tf.float32)  # Threshold at 0.5\n",
    "    outcome_labels_float = tf.cast(outcome_labels, tf.float32)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_outcome_labels, outcome_labels_float), tf.float32))\n",
    "\n",
    "    return outcome_loss, accuracy\n",
    "\n",
    "\n",
    "outcome_labels = y_train\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0  # To accumulate losses\n",
    "    total_accuracy = 0.0  # To accumulate accuracy\n",
    "\n",
    "    for step in range(num_steps_per_epoch):\n",
    "\n",
    "        batch_indices = []\n",
    "        if balance:\n",
    "            for outcome in range(num_outcomes):\n",
    "                outcome_indices = np.where(outcome_labels == outcome)[0]\n",
    "                outcome_batch_indices = np.random.choice(outcome_indices, size=batch_size // num_outcomes, replace=False)\n",
    "                batch_indices.append(outcome_batch_indices)\n",
    "        else:\n",
    "            # Correct all_indices to span the dataset, not outcomes\n",
    "            all_indices = np.arange(len(X_train))\n",
    "            random_indices = np.random.choice(all_indices, size=batch_size, replace=True)\n",
    "            batch_indices.append(random_indices)\n",
    "        \n",
    "        X_batch = X_train[np.concatenate(batch_indices)]\n",
    "        y_batch = y_train[np.concatenate(batch_indices)]\n",
    "        y_batch = tf.expand_dims(y_batch, axis=-1)  # Adjust labels shape for binary_crossentropy\n",
    "        \n",
    "        # Perform the training step\n",
    "        outcome_loss, accuracy = train_step(X_batch, y_batch)\n",
    "\n",
    "        # Accumulate losses and accuracy\n",
    "        total_loss += outcome_loss.numpy()\n",
    "        total_accuracy += accuracy.numpy()\n",
    "\n",
    "        # # Print the losses and accuracy\n",
    "        # if epoch_count % report_frequency == 0:\n",
    "        #     print(f'Epoch {epoch}, Step {step}, Outcome Loss: {outcome_loss.numpy()}, Accuracy: {accuracy.numpy()}')\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = total_loss / num_steps_per_epoch\n",
    "    avg_accuracy = total_accuracy / num_steps_per_epoch\n",
    "\n",
    "    # Print average accuracy at the end of each epoch and calculate the accuracy for the test set\n",
    "    if epoch  % report_frequency == 0:\n",
    "        # Forward pass of the test data\n",
    "        outcome_predictions = outcome_classifier(X_test)\n",
    "        predicted_outcome_labels = tf.cast(outcome_predictions > 0.5, tf.float32)  # Threshold at 0.5\n",
    "        outcome_labels = tf.expand_dims(y_test, axis=-1)  # Reshape to match the logits shape\n",
    "        outcome_labels_float = tf.cast(outcome_labels, tf.float32)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        test_accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_outcome_labels, outcome_labels_float), tf.float32))\n",
    "        \n",
    "        train_accuracy_list.append(avg_accuracy)\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch}, Average Outcome Loss: {avg_loss}, Average Accuracy: {avg_accuracy}, Test Accuracy: {test_accuracy}')\n",
    "\n",
    "        # Early stopping condition for accuracy\n",
    "        if test_accuracy > accuracy_threshold:\n",
    "            print('Early stopping: test set performance high enough')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c2612-7184-44a1-9acd-9005348dfb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, plot the metrics\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "# Plot train accuracy\n",
    "axs[0].plot(range(1, len(train_accuracy_list)+1), train_accuracy_list, label='Training Accuracy', color='blue')\n",
    "axs[0].set_title('Training accuracy over epochs')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Training Accuracy')\n",
    "axs[0].grid()\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot test accuracay\n",
    "axs[1].plot(range(1, len(test_accuracy_list)+1), test_accuracy_list, label='Test Accuracy', color='orange')\n",
    "axs[1].set_title('Test Accuracy over epochs')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Test Accuracy')\n",
    "axs[1].grid()\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33717e95-2041-4db4-ba5b-749401fa6f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = np.sum(y_test == 0)\n",
    "count/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e84149f-2ef8-4123-bdb5-c23db0f40174",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_prob = clf.predict_proba(X_test)[:, 1]  # Probabilities for the positive class (class 1)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line for random guessing\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c43cd50-42cf-438b-8236-ce1efd501d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f36d19-5360-4f86-abda-bcd5040f64fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
