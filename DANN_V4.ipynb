{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f76712b-d98d-4e30-a92d-db79ba3630f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this version of the DANN utilzes two disciminators, each affecting the weight updates of the encoder\n",
    "# one discriminator, as before is adversarial to the encoder and reflects the ability to discriminate technology\n",
    "# the other one is not and disrciminates based on patient response (yes vs no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e90898c-9455-4328-a421-228f46270fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "2024-10-22 14:29:13.785610: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from keras.callbacks import EarlyStopping\n",
    "import seaborn as sb\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import pearsonr\n",
    "import time\n",
    "from keras import layers, models\n",
    "\n",
    "adata = sc.read('/tmp/work/RCproject_code/sce_export.h5ad')\n",
    "epoch_count = 0\n",
    "\n",
    "#exclude partial\n",
    "adata = adata[~adata.obs['Response'].isin(['partial']), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ddb0f-db7f-4561-9099-801ec053b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "simplifly_categories = True\n",
    "dropout_rate = 0.1  #dropout rate for regulraization\n",
    "low_dim_rep_number = 128\n",
    "current_lambda_value = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fecad75-d1a0-4b8a-b6df-054006fb0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate numerical values for each batch category\n",
    "# set up categories variable\n",
    "categories_technology = adata.obs['batch']\n",
    "\n",
    "#collpse the categories to microARRAY vs sequencing\n",
    "\n",
    "if simplifly_categories:\n",
    "    category_map = {'GSE133057': 'micro', 'GSE145037': 'micro', 'GSE150082': 'micro','GSE190826':'seq','GSE209746':'seq',\n",
    "                    'GSE45404_GPL1': 'micro', 'GSE45404_GPL2': 'micro', 'GSE93375': 'micro','GSE94104': 'micro'}\n",
    "    categories_technology = np.vectorize(category_map.get)(categories_technology)\n",
    "    \n",
    "# Create a LabelEncoder instance\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit and transform the categories to integers\n",
    "numerical_categories_technology = label_encoder.fit_transform(categories_technology)\n",
    "print(numerical_categories_technology.shape)\n",
    "\n",
    "#do the same for the response variable\n",
    "categories_outcome = adata.obs['Response']\n",
    "numerical_categories_outcome = label_encoder.fit_transform(categories_outcome)\n",
    "print(numerical_categories_technology.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c6f70b-0760-42df-89de-a4c6fec0f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizaiton\n",
    "gene_expression_data = adata.layers['scalelogcounts']\n",
    "\n",
    "# Min-max normalization\n",
    "scaler = MinMaxScaler()\n",
    "gene_expression_data = scaler.fit_transform(gene_expression_data)\n",
    "number_samples = adata.shape[0]\n",
    "number_genes = adata.shape[1]\n",
    "input_dim = number_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6fc15f-0fcb-4c4e-8f7a-214b54e553de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_count_tech = len(np.unique(numerical_categories_technology))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca91552-f2ec-4bf8-b0e0-5fa5099748d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current correlations betweent the groups\n",
    "\n",
    "current = gene_expression_data\n",
    "if len(np.unique(numerical_categories_technology)) == 3:\n",
    "    #caculate the average expression array for the categories of interest\n",
    "    seq1_mean = np.mean(current[np.where(categories_technology == 'seq1')],axis = 0, keepdims = True)\n",
    "    seq2_mean = np.mean(current[np.where(categories_technology == 'seq2')],axis = 0, keepdims = True)\n",
    "    micro_mean = np.mean(current[np.where(categories_technology == 'micro')],axis = 0, keepdims = True)\n",
    "    #calculate the pearson correlations\n",
    "    seq1_v_micro, pval = pearsonr(seq1_mean.ravel(), micro_mean.ravel())\n",
    "    seq1_v_seq2, pval = pearsonr(seq1_mean.ravel(), seq2_mean.ravel())\n",
    "    seq2_v_micro, pval = pearsonr(seq2_mean.ravel(), micro_mean.ravel())\n",
    "    \n",
    "    print(seq1_v_micro)\n",
    "    print(seq1_v_seq2)\n",
    "    print(seq2_v_micro)\n",
    "\n",
    "elif len(np.unique(numerical_categories_technology)) == 2:\n",
    "    #caculate the average expression array for the categories of interest\n",
    "    seq_mean = np.mean(current[np.where(categories_technology == 'seq')],axis = 0, keepdims = True)\n",
    "    micro_mean = np.mean(current[np.where(categories_technology == 'micro')],axis = 0, keepdims = True)\n",
    "    #calculate the pearson correlations\n",
    "    seq_v_micro, pval = pearsonr(seq_mean.ravel(), micro_mean.ravel())\n",
    "    \n",
    "    print(seq_v_micro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ce8cf-d8cb-451c-9113-790701f70d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network architecture with functions for the encoder, decoder and discriminator\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (gene_expression_data.shape[1],)[0]  # Number of genes\n",
    "\n",
    "# Define the encoder function\n",
    "def build_encoder():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(input_shape,)))  # Input shape matches your data\n",
    "\n",
    "    # First layer with dropout\n",
    "    model.add(layers.Dense((low_dim_rep_number*8), activation='linear'))  # Increased units\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))  # Add dropout for regularization\n",
    "\n",
    "    # Second layer\n",
    "    model.add(layers.Dense((low_dim_rep_number*4), activation='linear'))  # Increased units\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))  # Add dropout\n",
    "\n",
    "    # Third layer\n",
    "    model.add(layers.Dense((low_dim_rep_number*2), activation='linear'))  # Increased units\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))  # Add dropout\n",
    "\n",
    "    # Encoded representation layer\n",
    "    model.add(layers.Dense(low_dim_rep_number, activation='linear'))  # Output encoded representation\n",
    "    return model\n",
    "\n",
    "# Define the decoder function (adjusted as discussed)\n",
    "def build_decoder():    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(low_dim_rep_number,)))  # Input shape should match the output of the encoder\n",
    "    model.add(layers.Dense((low_dim_rep_number*2), activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense((low_dim_rep_number*4), activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(input_shape, activation='sigmoid'))  # Output layer should match the input shape of the original data\n",
    "    return model\n",
    "\n",
    "## defines is the discriminator function\n",
    "def build_domain_classifier(num_domains):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=low_dim_rep_number))\n",
    "    \n",
    "    #Increased capacity with more units and layers\n",
    "    model.add(layers.Dense((low_dim_rep_number*2), activation='relu'))  # Increased units and changed activation\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))  # Added dropout for regularization\n",
    "    \n",
    "    model.add(layers.Dense((low_dim_rep_number/2), activation='relu'))  # Increased units\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))  # Added dropout for regularization\n",
    "\n",
    "    model.add(layers.Dense((low_dim_rep_number/4), activation='relu'))  # Increased units\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))  # Added dropout for regularization\n",
    "    \n",
    "    model.add(layers.Dense((low_dim_rep_number/8), activation='relu'))  # Maintain units\n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    model.add(layers.Dense(num_domains, activation='softmax'))  # num_domains is the number of classes\n",
    "    \n",
    "    return model\n",
    "\n",
    "#uses a sigmoid output since there are only two outcomes\n",
    "def build_outcome_classifier():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=low_dim_rep_number))\n",
    "    \n",
    "    # Add hidden layers\n",
    "    model.add(layers.Dense((low_dim_rep_number * 2), activation='relu'))  # Increased units\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(layers.Dense((low_dim_rep_number / 2), activation='relu'))  # Decrease units\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer for binary classification with sigmoid activation\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Single unit with sigmoid activation\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "#gradient reversal takes lambda as an argument, will ensure that the encoder is trained to work against the discimatinator when the lambda is positive\n",
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, lambda_value=current_lambda_value, **kwargs):\n",
    "        self.lambda_value = lambda_value\n",
    "        super(GradientReversalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define custom gradient behavior\n",
    "        @tf.custom_gradient\n",
    "        def reverse_gradients(x):\n",
    "            # Forward pass: output is just the input\n",
    "            def grad(dy):\n",
    "                # Gradient computation: reversed and scaled by lambda_value\n",
    "                return -self.lambda_value * dy\n",
    "            return x, grad\n",
    "        \n",
    "        return reverse_gradients(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"lambda_value\": self.lambda_value})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6276e0a3-5d4b-4d27-b17d-9b92518471fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder, decoder, and discriminator (assuming these functions are defined as in previous examples)\n",
    "encoder = build_encoder()  # Assuming build_encoder() is defined\n",
    "decoder = build_decoder()  # Assuming build_decoder() is defined\n",
    "discriminator = build_domain_classifier(cat_count_tech)  # Assuming build_domain_classifier() is defined\n",
    "outcome_discriminator = build_outcome_classifier()\n",
    "\n",
    "# Optimizers for each model\n",
    "encoder_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "decoder_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "outcome_discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "\n",
    "# Assuming you have 1000 training samples as an example\n",
    "num_samples = adata.shape[0]  # Replace this with your actual number of samples\n",
    "batch_size = 128  #determines how many samples are processed per batch, each epoch will process multiple batches\n",
    "\n",
    "# Calculate the number of steps per epoch\n",
    "num_steps_per_epoch = num_samples // batch_size\n",
    "\n",
    "# Compile the discriminator\n",
    "discriminator.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Compile the outcome discriminator\n",
    "outcome_discriminator.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Combine the encoder and decoder into a single model (autoencoder)\n",
    "autoencoder = tf.keras.Model(inputs=encoder.input, outputs=decoder(encoder.output))\n",
    "\n",
    "# Compile the autoencoder with the appropriate loss function (e.g., mean squared error)\n",
    "autoencoder.compile(optimizer=encoder_optimizer, loss='mean_squared_error')\n",
    "\n",
    "num_domains = len(np.unique(categories_technology))\n",
    "\n",
    "X_train = gene_expression_data\n",
    "y_train_disc = numerical_categories_technology\n",
    "y_train_outcome = numerical_categories_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539bae9b-5d44-4d12-8f82-d55e552e6de3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100000\n",
    "\n",
    "# Define the training step\n",
    "def train_step(data, disc_labels, outcome_labels):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Forward pass through the encoder and decoder\n",
    "        encoded_output = encoder(data)\n",
    "        reconstructed_output = decoder(encoded_output)\n",
    "\n",
    "        # Compute the autoencoder loss\n",
    "        ae_loss = tf.keras.losses.mean_squared_error(data, reconstructed_output)\n",
    "        ae_loss = tf.reduce_mean(ae_loss)  # Average over the batch\n",
    "        \n",
    "        # Forward pass through the GRL and discriminator\n",
    "        grl_output = GradientReversalLayer()(encoded_output)\n",
    "        domain_predictions = discriminator(grl_output)\n",
    "\n",
    "        # Compute the discriminator loss\n",
    "        disc_loss = tf.keras.losses.sparse_categorical_crossentropy(disc_labels, domain_predictions)\n",
    "        disc_loss = tf.reduce_mean(disc_loss)  # Average over the batch\n",
    "\n",
    "        # Forward pass through the biological discriminator\n",
    "        outcome_predictions = outcome_discriminator(encoded_output)\n",
    "\n",
    "        # Compute the biological discriminator loss (no gradient reversal)\n",
    "        outcome_loss = tf.keras.losses.binary_crossentropy(outcome_labels, outcome_predictions)\n",
    "        outcome_loss = tf.reduce_mean(outcome_loss)  # Average over the batch\n",
    "\n",
    "    # Compute gradients for the autoencoder (only for ae_loss)\n",
    "    encoder_grads_ae = tape.gradient(ae_loss, encoder.trainable_variables)\n",
    "    decoder_grads = tape.gradient(ae_loss, decoder.trainable_variables)\n",
    "\n",
    "    # Compute gradients for the discriminator\n",
    "    discriminator_grads = tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    # Compute gradients for the outcome discriminator\n",
    "    outcome_discriminator_grads = tape.gradient(outcome_loss, outcome_discriminator.trainable_variables)\n",
    "\n",
    "    # Compute gradients for the encoder from the discriminator loss\n",
    "    encoder_grads_disc = tape.gradient(disc_loss, encoder.trainable_variables)\n",
    "\n",
    "    # Compute gradients for the encoder from the outcome loss (no reversal)\n",
    "    encoder_grads_outcome = tape.gradient(outcome_loss, encoder.trainable_variables)\n",
    "\n",
    "     # Combine encoder gradients from both losses\n",
    "    encoder_grads = [g_ae + g_domain + g_out for g_ae, g_domain, g_out in zip(encoder_grads_ae, encoder_grads_disc, encoder_grads_outcome)]\n",
    "\n",
    "    # Update weights\n",
    "    encoder_optimizer.apply_gradients(zip(encoder_grads, encoder.trainable_variables))\n",
    "    decoder_optimizer.apply_gradients(zip(decoder_grads, decoder.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_grads, discriminator.trainable_variables))\n",
    "    outcome_discriminator_optimizer.apply_gradients(zip(outcome_discriminator_grads, outcome_discriminator.trainable_variables))\n",
    "\n",
    "    # # Update weights\n",
    "    # encoder_optimizer.apply_gradients(zip(encoder_grads, encoder.trainable_variables))\n",
    "    # decoder_optimizer.apply_gradients(zip(decoder_grads, decoder.trainable_variables))\n",
    "    # domain_discriminator_optimizer.apply_gradients(zip(discriminator_grads, discriminator.trainable_variables))\n",
    "    # outcome_discriminator_optimizer.apply_gradients(zip(outcome_discriminator_grads, outcome_discriminator.trainable_variables))\n",
    "\n",
    "    return ae_loss, disc_loss, outcome_loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_count += 1\n",
    "    total_disc_loss = 0.0  # To accumulate discriminator loss\n",
    "    \n",
    "    for step in range(num_steps_per_epoch):\n",
    "        # Get your batch data\n",
    "        data = X_train[step * batch_size:(step + 1) * batch_size]\n",
    "        disc_labels = y_train_disc[step * batch_size:(step + 1) * batch_size]\n",
    "        outcome_labels = y_train_outcome[step * batch_size:(step + 1) * batch_size]\n",
    "        outcome_labels = tf.expand_dims(outcome_labels, axis=-1)  # Reshape to match the logits shape\n",
    "        ae_loss, disc_loss, outcome_loss = train_step(data, disc_labels, outcome_labels)\n",
    "        print(f'Epoch {epoch}, Step {step}, AE Loss: {ae_loss.numpy()}, Disc Loss: {disc_loss.numpy()}, outcome Loss: {outcome_loss.numpy()}')\n",
    "\n",
    "    # Calculate average discriminator loss\n",
    "    avg_disc_loss = total_disc_loss / num_steps_per_epoch\n",
    "\n",
    "    # Check the condition for discriminator loss\n",
    "    if avg_disc_loss > np.log(cat_count_tech) and epoch_count > 500:\n",
    "        print('The discriminator does not have a clue!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d50db9-dd5d-4f27-8c09-baf42b1957b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epoch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4929334c-9e1a-45ed-8ba1-b7cb3d005dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the represetations from the trained model\n",
    "low_dimensional_representation = encoder.predict(gene_expression_data)\n",
    "\n",
    "#verify the shape\n",
    "print(low_dimensional_representation.shape)\n",
    "\n",
    "#set up the umap\n",
    "\n",
    "umap_model = umap.UMAP(n_neighbors=15,   # Controls local vs global structure\n",
    "                        min_dist=0.1,    # Controls how tightly UMAP packs points together\n",
    "                        metric='euclidean')  # Distance metric to use\n",
    "\n",
    "umap_result = umap_model.fit_transform(low_dimensional_representation)\n",
    "\n",
    "#checking batch separation\n",
    "current_label =  adata.obs['batch']\n",
    "umap_data = {'UMAP1': umap_result[:, 0], 'UMAP2': umap_result[:, 1],'batch':current_label}\n",
    "umap_df = pd.DataFrame(data=umap_data)\n",
    "ax = sb.scatterplot(data=umap_df, x='UMAP1', y='UMAP2', hue='batch')\n",
    "sb.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3738bd-a175-4dd6-84df-a2705d29cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the embeddings for random forest classification\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218997c3-2060-458f-93cf-ba5ade4b4ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## random forest with original data\n",
    "indices = [i for i, s in enumerate(adata.obs['Response']) if s in [\"yes\", \"no\"]]\n",
    "y = adata.obs['Response'][indices]\n",
    "y = [1 if x == \"yes\" else 0 for x in y]\n",
    "y = np.array(y)\n",
    "X = X_data[indices]\n",
    "\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_r, y_train_r)\n",
    "\n",
    "y_pred_prob = clf.predict_proba(X_test_r)[:, 1]  # Probabilities for the positive class (class 1)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test_r, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line for random guessing\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e32537-1407-43c1-b68d-bdcbdb486c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## random forest with transformed data\n",
    "indices = [i for i, s in enumerate(adata.obs['Response']) if s in [\"yes\", \"no\"]]\n",
    "y = adata.obs['Response'][indices]\n",
    "y = [1 if x == \"yes\" else 0 for x in y]\n",
    "y = np.array(y)\n",
    "X = low_dimensional_representation[indices]\n",
    "\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_r, y_train_r)\n",
    "\n",
    "y_pred_prob = clf.predict_proba(X_test_r)[:, 1]  # Probabilities for the positive class (class 1)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test_r, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line for random guessing\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd4c424-e0ac-4e42-ad5b-24be4d354252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f843e1f-a4b3-4e80-a1a1-526cf09d5b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
