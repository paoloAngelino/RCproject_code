---
title: "R Notebook"
output: html_notebook
---

```{r, eval=FALSE}
# The following initializes most up to date version of Bioc
BiocManager::install()

BiocManager::install("NanoStringNCTools")
BiocManager::install("GeomxTools")
BiocManager::install("GeoMxWorkflows")
```


```{r}
library(NanoStringNCTools)
library(GeomxTools)
library(GeoMxWorkflows)
if(packageVersion("GeomxTools") < "2.1" & 
   packageVersion("GeoMxWorkflows") >= "1.0.1"){
    stop("GeomxTools and Workflow versions do not match. Please use the same version. 
    This workflow is meant to be used with most current version of packages. 
    If you are using an older version of Bioconductor please reinstall GeoMxWorkflows and use vignette(GeoMxWorkflows) instead")
}

if(packageVersion("GeomxTools") > "2.1" & 
   packageVersion("GeoMxWorkflows") <= "1.0.1"){
    stop("GeomxTools and Workflow versions do not match. 
         Please use the same version, see install instructions above.")
    
    # to remove current package version
        # remove.packages("GeomxTools")
        # remove.packages("GeoMxWorkflows")
    # see install instructions above 
}
library(readxl)
```

Import data

```{r}
discovery <- read_excel("/data/pangelin/HUG/Thibaud/RC_GEOMX/data/Annotation template file_Discovery.xlsx")
validation <- read_excel("/data/pangelin/HUG/Thibaud/RC_GEOMX/data/Annotation template file_Validation.xlsx")
head(discovery)
head(validation)
```

```{r}
table(discovery$Type)
table(validation$Type)
validation$Type[validation$Type=="Biopsy 1"] <- "Biopsy"
table(validation$Type)
```

```{r}
table(discovery$Localisation)
discovery$Type <- as.factor(discovery$Localisation)
levels(discovery$Type) <- c("Biopsy", "Normal Epithelium", "Normal Lymphoide",  "Normal submucosa", "Surgery TC", "Surgery TE")
table(discovery$Type, discovery$Localisation)
```


```{r}
common_annot <- intersect(names(discovery),names(validation))
discovery <- discovery[,common_annot]
discovery$batch <- "discovery"
validation <- validation[,common_annot]
validation$batch <- "validation"
bothcohorts <- rbind(discovery, validation)
xlsx::write.xlsx(bothcohorts, file = "/data/pangelin/HUG/Thibaud/RC_GEOMX/data/Annotation_both.xlsx", sheetName = "Template")
```


```{r}
DCCFiles <- dir("/data/pangelin/HUG/Thibaud/RC_GEOMX/data/", pattern = ".dcc$",
                full.names = TRUE, recursive = TRUE)

PKCFiles <- dir("/data/pangelin/HUG/Thibaud/RC_GEOMX/data/", pattern = ".pkc$",
                                full.names = TRUE, recursive = FALSE)
#AnnotationFile <- "/data/pangelin/HUG/Thibaud/RC_GEOMX/data/Annotation template file_Discovery.xlsx"
AnnotationFile <- "/data/pangelin/HUG/Thibaud/RC_GEOMX/data/Annotation_both.xlsx"

# load data
Data <-
    readNanoStringGeoMxSet(dccFiles = DCCFiles,
                           pkcFiles = PKCFiles,
                           phenoDataFile = AnnotationFile,
                           phenoDataSheet = "Template",
                           phenoDataDccColName = "Sample_ID",
                           protocolDataColNames = c("aoi", "roi"),
                           experimentDataColNames = c("panel"))
```

```{r}
Data
```

## Modules Used

```{r}
library(knitr)
pkcs <- annotation(Data)
modules <- gsub(".pkc", "", pkcs)
kable(data.frame(PKCs = pkcs, modules = modules))
```

## Sample Overview

Before we begin, we will shift any expression counts with a value of 0 to 1 to enable in downstream transformations.

```{r}
# Shift counts to one
Data <- shiftCountsOne(Data, useDALogic = TRUE)
```


```{r}
library(dplyr)
library(ggforce)

# select the annotations we want to show, use `` to surround column names with
# spaces or special symbols
count_mat <- dplyr::count(pData(Data), `slide name`, Type, segment, batch)
# simplify the slide names
# gather the data and plot in order: class, slide name, region, segment
test_gr <- gather_set_data(count_mat, 1:4)
test_gr$x <- factor(test_gr$x)
levels(test_gr$x) <- c("slide name", "Type", "segment", "batch")
# plot Sankey
ggplot(test_gr, aes(x, id = id, split = y, value = n)) +
    geom_parallel_sets(aes(fill = Type), alpha = 0.5, axis.width = 0.1) +
    geom_parallel_sets_axes(axis.width = 0.2) +
    geom_parallel_sets_labels(color = "white", size = 5) +
    theme_classic(base_size = 17) + 
    theme(legend.position = "bottom",
          axis.ticks.y = element_blank(),
          axis.line = element_blank(),
          axis.text.y = element_blank()) +
    scale_y_continuous(expand = expansion(0)) + 
    scale_x_discrete(expand = expansion(0)) +
    labs(x = "", y = "") +
    annotate(geom = "segment", x = 4.25, xend = 4.25,
             y = 20, yend = 120, lwd = 2) +
    annotate(geom = "text", x = 4.19, y = 70, angle = 90, size = 5,
             hjust = 0.5, label = "100 segments")
```

```{r}
names(pData(Data))
AnnotationData <- pData(Data)
AnnotationData$region <- AnnotationData$Localisation 
AnnotationData$class <- AnnotationData$Response
pData(Data) <- AnnotationData
demoData <- Data
```

```{r}
save.image("/data/pangelin/HUG/Thibaud/RC_GEOMX/analysis/var/myGeoMXworkflow_bothCohorts.RData")
```



## Segment QC 

We first assess sequencing quality and adequate tissue sampling for every 
ROI/AOI segment. 

Every ROI/AOI segment will be tested for:

* Raw sequencing reads: segments with >1000 raw reads are removed.
* % Aligned,% Trimmed, or % Stitched sequencing reads: segments below ~80% for 
one or more of these QC parameters are removed.
* % Sequencing saturation ([1-deduplicated reads/aligned reads]%): segments 
below ~50% require additional sequencing to capture full sample diversity and 
are not typically analyzed until improved.
* Negative Count: this is the geometric mean of the several unique negative 
probes in the GeoMx panel that do not target mRNA and establish the background 
count level per segment; segments with low negative counts (1-10) are not 
necessarily removed but may be studied closer for low endogenous gene signal 
and/or insufficient tissue sampling.
* No Template Control (NTC) count: values >1,000 could indicate contamination 
for the segments associated with this NTC; however, in cases where the NTC 
count is between 1,000- 10,000, the segments may be used if the NTC data is 
uniformly low (e.g. 0-2 counts for all probes).
* Nuclei: >100 nuclei per segment is generally recommended; however, this 
cutoff is highly study/tissue dependent and may need to be reduced; what is 
most important is consistency in the nuclei distribution for segments within 
the study.
* Area: generally correlates with nuclei; a strict cutoff is not generally 
applied based on area.

### Select Segment QC 

First, we select the QC parameter cutoffs, against which our ROI/AOI segments 
will be tested and flagged appropriately. We have selected the appropriate 
study-specific parameters for this study. Note: the default QC values 
recommended above are advised when surveying a new dataset for the first time. 

```{r setqcflagupdated,  eval = TRUE}
# Default QC cutoffs are commented in () adjacent to the respective parameters
# study-specific values were selected after visualizing the QC results in more
# detail below
QC_params <-
    list(minSegmentReads = 1000, # Minimum number of reads (1000)
         percentTrimmed = 80,    # Minimum % of reads trimmed (80%)
         percentStitched = 80,   # Minimum % of reads stitched (80%)
         percentAligned = 75,    # Minimum % of reads aligned (80%)
         percentSaturation = 50, # Minimum sequencing saturation (50%)
         minNegativeCount = 1,   # Minimum negative control counts (10)
         maxNTCCount = 9000,     # Maximum counts observed in NTC well (1000)
         minNuclei = 20,         # Minimum # of nuclei estimated (100)
         minArea = 1000)         # Minimum segment area (5000)
demoData <-
    setSegmentQCFlags(demoData, 
                      qcCutoffs = QC_params)        

# Collate QC Results
QCResults <- protocolData(demoData)[["QCFlags"]]
flag_columns <- colnames(QCResults)
QC_Summary <- data.frame(Pass = colSums(!QCResults[, flag_columns]),
                         Warning = colSums(QCResults[, flag_columns]))
QCResults$QCStatus <- apply(QCResults, 1L, function(x) {
    ifelse(sum(x) == 0L, "PASS", "WARNING")
})
QC_Summary["TOTAL FLAGS", ] <-
    c(sum(QCResults[, "QCStatus"] == "PASS"),
      sum(QCResults[, "QCStatus"] == "WARNING"))

```

### Visualize Segment QC

Before excluding any low-performing ROI/AOI segments, we visualize the 
distributions of the data for the different QC parameters. Note that the 
"Select Segment QC" and "Visualize Segment QC" sections are performed in 
parallel to fully understand low-performing segments for a given study. 
Iteration may follow to select the study-specific QC cutoffs.

For QC visualization, we write a quick function to draw histograms of our data.

``` {r qcflagHistogramsCode, eval = TRUE, warning = FALSE, message = FALSE}
library(ggplot2)

col_by <- "segment"

# Graphical summaries of QC statistics plot function
QC_histogram <- function(assay_data = NULL,
                         annotation = NULL,
                         fill_by = NULL,
                         thr = NULL,
                         scale_trans = NULL) {
    plt <- ggplot(assay_data,
                  aes_string(x = paste0("unlist(`", annotation, "`)"),
                             fill = fill_by)) +
        geom_histogram(bins = 50) +
        geom_vline(xintercept = thr, lty = "dashed", color = "black") +
        theme_bw() + guides(fill = "none") +
        facet_wrap(as.formula(paste("~", fill_by)), nrow = 4) +
        labs(x = annotation, y = "Segments, #", title = annotation)
    if(!is.null(scale_trans)) {
        plt <- plt +
            scale_x_continuous(trans = scale_trans)
    }
    plt
}

```

Now we explore each of the QC metrics for the segments.

``` {r plotQCHist, warning = FALSE, message = FALSE}
QC_histogram(sData(demoData), "Trimmed (%)", col_by, 80)
QC_histogram(sData(demoData), "Stitched (%)", col_by, 80)
QC_histogram(sData(demoData), "Aligned (%)", col_by, 75)
QC_histogram(sData(demoData), "Saturated (%)", col_by, 50) +
    labs(title = "Sequencing Saturation (%)",
         x = "Sequencing Saturation (%)")
QC_histogram(sData(demoData), "area", col_by, 1000, scale_trans = "log10")
if("nuclei" %in% names(pData(demoData))) {
  QC_histogram(sData(demoData), "nuclei", col_by, 20)
}

# calculate the negative geometric means for each module
negativeGeoMeans <- 
    esBy(negativeControlSubset(demoData), 
         GROUP = "Module", 
         FUN = function(x) { 
             assayDataApply(x, MARGIN = 2, FUN = ngeoMean, elt = "exprs") 
         }) 
protocolData(demoData)[["NegGeoMean"]] <- negativeGeoMeans

# explicitly copy the Negative geoMeans from sData to pData
negCols <- paste0("NegGeoMean_", modules)
pData(demoData)[, negCols] <- sData(demoData)[["NegGeoMean"]]
for(ann in negCols) {
    plt <- QC_histogram(pData(demoData), ann, col_by, 2, scale_trans = "log10")
    print(plt)
}

# detatch neg_geomean columns ahead of aggregateCounts call
pData(demoData) <- pData(demoData)[, !colnames(pData(demoData)) %in% negCols]

# show all NTC values, Freq = # of Segments with a given NTC count:
if("NTC" %in% names(pData(demoData))) {
  kable(table(NTC_Count = sData(demoData)$NTC),
        col.names = c("NTC Count", "# of Segments"))
}
```

Finally we plot all of the QC Summary information in a table.

```{r QCSummaryTable, results = "asis"}
kable(QC_Summary, caption = "QC Summary Table for each Segment")
```

###  Remove flagged segments

As the final step in Segment QC, we remove flagged segments that do not meet 
our QC cutoffs.

```{r removeQCSampleProbe, eval = TRUE}
demoData <- demoData[, QCResults$QCStatus == "PASS"]

# Subsetting our dataset has removed samples which did not pass QC
dim(demoData)
```

## Probe QC

Before we summarize our data into gene-level count data, we will remove 
low-performing probes. In short, this QC is an outlier removal process, whereby 
probes are either removed entirely from the study (global) or from specific 
segments (local). The QC applies to gene targets for which there are multiple 
distinct probes representing the count for a gene per segment. In WTA data, one 
specific probe exists per target gene; thus, Probe QC does not apply to the 
endogenous genes in the panel. Rather, it is performed on the negative control 
probes; there are multiple probes representing our negative controls, which do 
not target any sequence in the genome. These probes enable calculation of the 
background per segment and will be important for determining gene detection 
downstream.

After Probe QC, there will always remain at least one probe representing every 
gene target. In other words, Probe QC never removes genes from your data.

### Set Probe QC Flags

A probe is removed globally from the dataset if either of the following is true:

* the geometric mean of that probe's counts from all segments divided by the 
geometric mean of all probe counts representing the target from all segments is 
less than 0.1
* the probe is an outlier according to the Grubb's test in at least 20% of the 
segments

A probe is removed locally (from a given segment) if the probe is an outlier 
according to the Grubb's test in that segment.

We do not typically adjust these QC parameters.

```{r setbioprobeqcflag,  eval = TRUE}
# Generally keep the qcCutoffs parameters unchanged. Set removeLocalOutliers to 
# FALSE if you do not want to remove local outliers
demoData <- setBioProbeQCFlags(demoData, 
                               qcCutoffs = list(minProbeRatio = 0.1,
                                                percentFailGrubbs = 20), 
                               removeLocalOutliers = TRUE)

ProbeQCResults <- fData(demoData)[["QCFlags"]]

# Define QC table for Probe QC
qc_df <- data.frame(Passed = sum(rowSums(ProbeQCResults[, -1]) == 0),
                    Global = sum(ProbeQCResults$GlobalGrubbsOutlier),
                    Local = sum(rowSums(ProbeQCResults[, -2:-1]) > 0
                                & !ProbeQCResults$GlobalGrubbsOutlier))
```

We report the number of global and local outlier probes.

```{r bioprobeQCTable, echo = FALSE, results = "asis"}
kable(qc_df, caption = "Probes flagged or passed as outliers")

```

### Exclude Outlier Probes

```{r excludeOutlierProbes}  
#Subset object to exclude all that did not pass Ratio & Global testing
ProbeQCPassed <- 
    subset(demoData, 
           fData(demoData)[["QCFlags"]][,c("LowProbeRatio")] == FALSE &
               fData(demoData)[["QCFlags"]][,c("GlobalGrubbsOutlier")] == FALSE)
dim(ProbeQCPassed)
demoData <- ProbeQCPassed 
```

## Create Gene-level Count Data

With our Probe QC steps complete, we will generate a gene-level count matrix. 
The count for any gene with multiple probes per segment is calculated as the 
geometric mean of those probes.

```{r aggregateCounts, eval = TRUE}
# Check how many unique targets the object has
length(unique(featureData(demoData)[["TargetName"]]))

# collapse to targets
target_demoData <- aggregateCounts(demoData)
dim(target_demoData)
exprs(target_demoData)[1:5, 1:2]

```

## Limit of Quantification

In addition to Segment and Probe QC, we also determine the limit of 
quantification (LOQ) per segment. The LOQ is calculated based on the 
distribution of negative control probes and is intended to approximate the 
quantifiable limit of gene expression per segment. Please note that this 
process is more stable in larger segments. Likewise, the LOQ may not be as 
accurately reflective of true signal detection rates in segments with low 
negative probe counts (ex: <2). The formula for calculating the LOQ in the 
$i^{th}$ segment is: 

$$LOQ_{i} = geomean(NegProbe_{i}) * geoSD(NegProbe_{i})^{n}$$

We typically use 2 geometric standard deviations ($n = 2$) above the geometric 
mean as the LOQ, which is reasonable for most studies. We also recommend that a 
minimum LOQ of 2 be used if the LOQ calculated in a segment is below this 
threshold.

``` {r calculateLOQ, eval = TRUE}
# Define LOQ SD threshold and minimum value
cutoff <- 2
minLOQ <- 2

# Calculate LOQ per module tested
LOQ <- data.frame(row.names = colnames(target_demoData))
for(module in modules) {
    vars <- paste0(c("NegGeoMean_", "NegGeoSD_"),
                   module)
    if(all(vars[1:2] %in% colnames(pData(target_demoData)))) {
        LOQ[, module] <-
            pmax(minLOQ,
                 pData(target_demoData)[, vars[1]] * 
                     pData(target_demoData)[, vars[2]] ^ cutoff)
    }
}
pData(target_demoData)$LOQ <- LOQ
```

## Filtering

After determining the limit of quantification (LOQ) per segment, we recommend 
filtering out either segments and/or genes with abnormally low signal. 
Filtering is an important step to focus on the true biological data of interest.

We determine the number of genes detected in each segment across the dataset.

```{r LOQMat, eval = TRUE}
LOQ_Mat <- c()
for(module in modules) {
    ind <- fData(target_demoData)$Module == module
    Mat_i <- t(esApply(target_demoData[ind, ], MARGIN = 1,
                       FUN = function(x) {
                           x > LOQ[, module]
                       }))
    LOQ_Mat <- rbind(LOQ_Mat, Mat_i)
}
# ensure ordering since this is stored outside of the geomxSet
LOQ_Mat <- LOQ_Mat[fData(target_demoData)$TargetName, ]
```

### Segment Gene Detection

We first filter out segments with exceptionally low signal. These segments will 
have a small fraction of panel genes detected above the LOQ relative to the 
other segments in the study. Let's visualize the distribution of segments with 
respect to their % genes detected:

```{r segDetectionBarplot}
# Save detection rate information to pheno data
pData(target_demoData)$GenesDetected <- 
    colSums(LOQ_Mat, na.rm = TRUE)
pData(target_demoData)$GeneDetectionRate <-
    pData(target_demoData)$GenesDetected / nrow(target_demoData)

# Determine detection thresholds: 1%, 5%, 10%, 15%, >15%
pData(target_demoData)$DetectionThreshold <- 
    cut(pData(target_demoData)$GeneDetectionRate,
        breaks = c(0, 0.01, 0.05, 0.1, 0.15, 1),
        labels = c("<1%", "1-5%", "5-10%", "10-15%", ">15%"))

# stacked bar plot of different cut points (1%, 5%, 10%, 15%)
ggplot(pData(target_demoData),
       aes(x = DetectionThreshold)) +
    geom_bar(aes(fill = segment)) +
    geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5) +
    theme_bw() +
    scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
    labs(x = "Gene Detection Rate",
         y = "Segments, #",
         fill = "Segment Type")
```

```{r}
ggplot(pData(target_demoData),
       aes(x = DetectionThreshold)) +
    geom_bar(aes(fill = Type)) +
    geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5) +
    theme_bw() +
    scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
    labs(x = "Gene Detection Rate",
         y = "Segments, #",
         fill = "Segment Type")
```

We can also create a table to review what kidney tissue type (DKD vs normal) is 
going to be impacted by each threshold:

```{r segTable}
# cut percent genes detected at 1, 5, 10, 15
kable(table(pData(target_demoData)$DetectionThreshold,
            pData(target_demoData)$Type))
```

In this example, we choose to remove segments with less than 10% of the genes 
detected. Generally, 5-10% detection is a reasonable segment filtering 
threshold. However, based on the experimental design (e.g. segment types, size, 
nuclei) and tissue characteristics (e.g. type, age), these guidelines may 
require adjustment.

```{r filterSegments}
target_demoData <-
    target_demoData[, pData(target_demoData)$GeneDetectionRate >= .1]

dim(target_demoData)
```

Let's re-plot the Sankey diagram showing our current working dataset. This is 
now a dataset that no longer contains segments flagged by Segment QC or that 
have low gene detection rates.


``` {r replotSankey, fig.width = 10, fig.height = 8, fig.wide = TRUE, message = FALSE, warning = FALSE}
# select the annotations we want to show, use `` to surround column names with
# spaces or special symbols
count_mat <- dplyr::count(pData(demoData), `slide name`, Type, segment, batch)
# simplify the slide names
# gather the data and plot in order: class, slide name, region, segment
test_gr <- gather_set_data(count_mat, 1:4)
test_gr$x <- factor(test_gr$x)
levels(test_gr$x) <- c("slide name", "Type", "segment", "batch")
# plot Sankey
ggplot(test_gr, aes(x, id = id, split = y, value = n)) +
    geom_parallel_sets(aes(fill = Type), alpha = 0.5, axis.width = 0.1) +
    geom_parallel_sets_axes(axis.width = 0.2) +
    geom_parallel_sets_labels(color = "white", size = 5) +
    theme_classic(base_size = 17) + 
    theme(legend.position = "bottom",
          axis.ticks.y = element_blank(),
          axis.line = element_blank(),
          axis.text.y = element_blank()) +
    scale_y_continuous(expand = expansion(0)) + 
    scale_x_discrete(expand = expansion(0)) +
    labs(x = "", y = "") +
    annotate(geom = "segment", x = 4.25, xend = 4.25,
             y = 20, yend = 120, lwd = 2) +
    annotate(geom = "text", x = 4.19, y = 70, angle = 90, size = 5,
             hjust = 0.5, label = "100 segments")
```

### Gene Detection Rate

Next, we determine the detection rate for genes across the study. To illustrate 
this idea, we create a small gene list (`goi`) to review.


``` {r goi detection}
library(scales) # for percent

# Calculate detection rate:
LOQ_Mat <- LOQ_Mat[, colnames(target_demoData)]
fData(target_demoData)$DetectedSegments <- rowSums(LOQ_Mat, na.rm = TRUE)
fData(target_demoData)$DetectionRate <-
    fData(target_demoData)$DetectedSegments / nrow(pData(target_demoData))

# Gene of interest detection table
goi <- c('CPS1','UNC13A','TDRD1','ADCY2','TENM1','ALOX12B','ZNF300','INHBB','KLK6','NHLH2','SOX2','REG3A','VAX','IGF2','L1CAM') # Chatila.markers
#goi <- c("PDCD1", "CD274", "IFNG", "CD8A", "CD68", "EPCAM",
#         "KRT18", "NPHS1", "NPHS2", "CALB1", "CLDN8")
goi_df <- data.frame(
    Gene = goi,
    Number = fData(target_demoData)[goi, "DetectedSegments"],
    DetectionRate = percent(fData(target_demoData)[goi, "DetectionRate"]))
```

```{r tableGOI, echo = FALSE, results = "asis"}
kable(goi_df, caption = "Detection rate for Genes of Interest", align = "c",
      col.names = c("Gene", "Detection, # Segments", "Detection Rate, % of Segments"))
```

We can see that individual genes are detected to varying degrees in the 
segments, which leads us to the next QC we will perform across the dataset.

### Gene Filtering

We will graph the total number of genes detected in different percentages of 
segments. Based on the visualization below, we can better understand global 
gene detection in our study and select how many low detected genes to filter 
out of the dataset. Gene filtering increases performance of downstream 
statistical tests and improves interpretation of true biological signal.

```{r plotDetectionRate, eval = TRUE}
# Plot detection rate:
plot_detect <- data.frame(Freq = c(1, 5, 10, 20, 30, 50))
plot_detect$Number <-
    unlist(lapply(c(0.01, 0.05, 0.1, 0.2, 0.3, 0.5),
                  function(x) {sum(fData(target_demoData)$DetectionRate >= x)}))
plot_detect$Rate <- plot_detect$Number / nrow(fData(target_demoData))
rownames(plot_detect) <- plot_detect$Freq

ggplot(plot_detect, aes(x = as.factor(Freq), y = Rate, fill = Rate)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = formatC(Number, format = "d", big.mark = ",")),
              vjust = 1.6, color = "black", size = 4) +
    scale_fill_gradient2(low = "orange2", mid = "lightblue",
                         high = "dodgerblue3", midpoint = 0.65,
                         limits = c(0,1),
                         labels = scales::percent) +
    theme_bw() +
    scale_y_continuous(labels = scales::percent, limits = c(0,1),
                       expand = expansion(mult = c(0, 0))) +
    labs(x = "% of Segments",
         y = "Genes Detected, % of Panel > LOQ")
```

We typically set a % Segment cutoff ranging from 5-20% based on the biological 
diversity of our dataset. For this study, we will select 10% as our cutoff. In 
other words, we will focus on the genes detected in at least 10% of our 
segments; we filter out the remainder of the targets.

Note: if we know that a key gene is represented in only a small number of 
segments (<10%) due to biological diversity, we may select a different cutoff 
or keep the target gene by manually selecting it for inclusion in the data 
object. 

``` {r subsetGenes, eval = TRUE}
# Subset to target genes detected in at least 10% of the samples.
#   Also manually include the negative control probe, for downstream use
negativeProbefData <- subset(fData(target_demoData), CodeClass == "Negative")
neg_probes <- unique(negativeProbefData$TargetName)
target_demoData <- 
    target_demoData[fData(target_demoData)$DetectionRate >= 0.1 |
                        fData(target_demoData)$TargetName %in% neg_probes, ]
dim(target_demoData)

# retain only detected genes of interest
goi <- goi[goi %in% rownames(target_demoData)]
```

# Normalization

We will now normalize the GeoMx data for downstream visualizations and 
differential expression. The two common methods for normalization of DSP-NGS 
RNA data are i) quartile 3 (Q3) or ii) background normalization.

Both of these normalization methods estimate a normalization factor per segment 
to bring the segment data distributions together. More advanced methods for 
normalization and modeling are under active development. However, for most 
studies, these methods are sufficient for understanding differences between 
biological classes of segments and samples.

Q3 normalization is typically the preferred normalization strategy for most 
DSP-NGS RNA studies. Given the low negative probe counts in this particular 
dataset as shown during Segment QC, we would further avoid background 
normalization as it may be less stable.

Before normalization, we will explore the relationship between the upper 
quartile (Q3) of the counts in each segment with the geometric mean of the 
negative control probes in the data. Ideally, there should be a separation 
between these two values to ensure we have stable measure of Q3 signal. If you 
do not see sufficient separation between these values, you may consider more 
aggressive filtering of low signal segments/genes. 

``` {r, previewNF, fig.width = 8, fig.height = 8, fig.wide = TRUE, eval = TRUE, warning = FALSE, message = FALSE}
library(reshape2)  # for melt
library(cowplot)   # for plot_grid

# Graph Q3 value vs negGeoMean of Negatives
ann_of_interest <- "Type"
Stat_data <- 
    data.frame(row.names = colnames(exprs(target_demoData)),
               Segment = colnames(exprs(target_demoData)),
               Annotation = pData(target_demoData)[, ann_of_interest],
               Q3 = unlist(apply(exprs(target_demoData), 2,
                                 quantile, 0.75, na.rm = TRUE)),
               NegProbe = exprs(target_demoData)[neg_probes, ])
Stat_data_m <- melt(Stat_data, measure.vars = c("Q3", "NegProbe"),
                    variable.name = "Statistic", value.name = "Value")

plt1 <- ggplot(Stat_data_m,
               aes(x = Value, fill = Statistic)) +
    geom_histogram(bins = 40) + theme_bw() +
    scale_x_continuous(trans = "log2") +
    facet_wrap(~Annotation, nrow = 1) + 
    scale_fill_brewer(palette = 3, type = "qual") +
    labs(x = "Counts", y = "Segments, #")

plt2 <- ggplot(Stat_data,
               aes(x = NegProbe, y = Q3, color = Annotation)) +
    geom_abline(intercept = 0, slope = 1, lty = "dashed", color = "darkgray") +
    geom_point() + guides(color = "none") + theme_bw() +
    scale_x_continuous(trans = "log2") + 
    scale_y_continuous(trans = "log2") +
    theme(aspect.ratio = 1) +
    labs(x = "Negative Probe GeoMean, Counts", y = "Q3 Value, Counts")

plt3 <- ggplot(Stat_data,
               aes(x = NegProbe, y = Q3 / NegProbe, color = Annotation)) +
    geom_hline(yintercept = 1, lty = "dashed", color = "darkgray") +
    geom_point() + theme_bw() +
    scale_x_continuous(trans = "log2") + 
    scale_y_continuous(trans = "log2") +
    theme(aspect.ratio = 1) +
    labs(x = "Negative Probe GeoMean, Counts", y = "Q3/NegProbe Value, Counts")

btm_row <- plot_grid(plt2, plt3, nrow = 1, labels = c("B", ""),
                     rel_widths = c(0.43,0.57))
plot_grid(plt1, btm_row, ncol = 1, labels = c("A", ""))
```


As expected, we see separation of the Q3 and negative probe counts at both the 
distribution (A) and per segment (B) levels. 
For additional conceptual guidance, please refer to our 
[Data Analysis White Paper for DSP-NGS Assays](https://www.nanostring.com/resources/geomx-cta-data-whitepaper/).

Next, we normalize our data. We will use Q3 normalized data moving forward. We 
use the `normalize` function from `NanoStringNCTools` to create normalization 
factors reflecting each data type. Upper quartile (Q3) normalization is 
performed using `norm_method = "quant"` setting the `desiredQuantile` flag to 
0.75. Other quantiles could be specified by changing that value. We save the 
normalized data to a specific slot using `toELT = "q_norm"`. Similarly 
background normalization is performed by setting `norm_method = "neg"` and 
`toElt = "neg_norm"`.

```{r normalizeObject, eval = TRUE}
# Q3 norm (75th percentile) for WTA/CTA  with or without custom spike-ins
target_demoData <- normalize(target_demoData ,
                             norm_method = "quant", 
                             desiredQuantile = .75,
                             toElt = "q_norm")

# Background normalization for WTA/CTA without custom spike-in
target_demoData <- normalize(target_demoData ,
                             norm_method = "neg", 
                             fromElt = "exprs",
                             toElt = "neg_norm")
```

To demonstrate the effects of normalization, we graph representative box plots 
of the data for individual segments before and after normalization.

```{r normplot, fig.small = TRUE}
# visualize the first 10 segments with each normalization method
boxplot(exprs(target_demoData)[,1:10],
        col = "#9EDAE5", main = "Raw Counts",
        log = "y", names = 1:10, xlab = "Segment",
        ylab = "Counts, Raw")

boxplot(assayDataElement(target_demoData[,1:10], elt = "q_norm"),
        col = "#2CA02C", main = "Q3 Norm Counts",
        log = "y", names = 1:10, xlab = "Segment",
        ylab = "Counts, Q3 Normalized")

boxplot(assayDataElement(target_demoData[,1:10], elt = "neg_norm"),
        col = "#FF7F0E", main = "Neg Norm Counts",
        log = "y", names = 1:10, xlab = "Segment",
        ylab = "Counts, Neg. Normalized")
```

# Unsupervised Analysis

## UMAP & t-SNE

One common approach to understanding high-plex data is dimension reduction. Two 
common methods are UMAP and tSNE, which are non-orthogonally constrained 
projections that cluster samples based on overall gene expression. In this 
study, we see by either UMAP (from the `umap` package) or tSNE (from the 
`Rtsne` package), clusters of segments related to structure (glomeruli or 
tubules) and disease status (normal or diabetic kidney disease).


``` {r dimReduction1, eval = TRUE}
library(umap)
library(Rtsne)

# update defaults for umap to contain a stable random_state (seed)
custom_umap <- umap::umap.defaults
custom_umap$random_state <- 42
# run UMAP
umap_out <-
    umap(t(log2(assayDataElement(target_demoData , elt = "q_norm"))),  
         config = custom_umap)
pData(target_demoData)[, c("UMAP1", "UMAP2")] <- umap_out$layout[, c(1,2)]
ggplot(pData(target_demoData),
       aes(x = UMAP1, y = UMAP2, color = batch, shape = segment)) +
    geom_point(size = 3) +
    theme_bw()

# run tSNE
set.seed(42) # set the seed for tSNE as well
tsne_out <-
    Rtsne(t(log2(assayDataElement(target_demoData , elt = "q_norm"))),
          perplexity = ncol(target_demoData)*.15)
pData(target_demoData)[, c("tSNE1", "tSNE2")] <- tsne_out$Y[, c(1,2)]
ggplot(pData(target_demoData),
       aes(x = tSNE1, y = tSNE2, color = batch, shape = segment)) +
    geom_point(size = 3) +
    theme_bw()
```


``` {r dimReduction2, eval = TRUE}
library(umap)
library(Rtsne)

# update defaults for umap to contain a stable random_state (seed)
custom_umap <- umap::umap.defaults
custom_umap$random_state <- 42
# run UMAP
umap_out <-
    umap(t(log2(assayDataElement(target_demoData , elt = "q_norm"))),  
         config = custom_umap)
pData(target_demoData)[, c("UMAP1", "UMAP2")] <- umap_out$layout[, c(1,2)]
ggplot(pData(target_demoData),
       aes(x = UMAP1, y = UMAP2, color = Type, shape = batch)) +
    geom_point(size = 3) +
    theme_bw()

# run tSNE
set.seed(42) # set the seed for tSNE as well
tsne_out <-
    Rtsne(t(log2(assayDataElement(target_demoData , elt = "q_norm"))),
          perplexity = ncol(target_demoData)*.15)
pData(target_demoData)[, c("tSNE1", "tSNE2")] <- tsne_out$Y[, c(1,2)]
ggplot(pData(target_demoData),
       aes(x = tSNE1, y = tSNE2, color = Type, shape = batch)) +
    geom_point(size = 3) +
    theme_bw()
```

```{r}
target_demoData <- target_demoData[,pData(target_demoData)$Type %in% c("Biopsy", "Surgery TC", "Surgery TE")]
```



``` {r dimReduction4, eval = TRUE}
library(umap)
library(Rtsne)

# update defaults for umap to contain a stable random_state (seed)
custom_umap <- umap::umap.defaults
custom_umap$random_state <- 42
# run UMAP
umap_out <-
    umap(t(log2(assayDataElement(target_demoData , elt = "q_norm"))),  
         config = custom_umap)
pData(target_demoData)[, c("UMAP1", "UMAP2")] <- umap_out$layout[, c(1,2)]
ggplot(pData(target_demoData),
       aes(x = UMAP1, y = UMAP2, color = Type, shape = batch)) +
    geom_point(size = 3) +
    theme_bw()

# run tSNE
set.seed(42) # set the seed for tSNE as well
tsne_out <-
    Rtsne(t(log2(assayDataElement(target_demoData , elt = "q_norm"))),
          perplexity = ncol(target_demoData)*.15)
pData(target_demoData)[, c("tSNE1", "tSNE2")] <- tsne_out$Y[, c(1,2)]
ggplot(pData(target_demoData),
       aes(x = tSNE1, y = tSNE2, color = Type, shape = batch)) +
    geom_point(size = 3) +
    theme_bw()
```

``` {r dimReduction3, eval = TRUE}
library(umap)
library(Rtsne)

# update defaults for umap to contain a stable random_state (seed)
custom_umap <- umap::umap.defaults
custom_umap$random_state <- 42
# run UMAP
umap_out <-
    umap(t(log2(assayDataElement(target_demoData , elt = "q_norm"))),  
         config = custom_umap)
pData(target_demoData)[, c("UMAP1", "UMAP2")] <- umap_out$layout[, c(1,2)]
ggplot(pData(target_demoData),
       aes(x = UMAP1, y = UMAP2, color = Type, shape = segment)) +
    geom_point(size = 3) +
    theme_bw()

# run tSNE
set.seed(42) # set the seed for tSNE as well
tsne_out <-
    Rtsne(t(log2(assayDataElement(target_demoData , elt = "q_norm"))),
          perplexity = ncol(target_demoData)*.15)
pData(target_demoData)[, c("tSNE1", "tSNE2")] <- tsne_out$Y[, c(1,2)]
ggplot(pData(target_demoData),
       aes(x = tSNE1, y = tSNE2, color = Type, shape = segment)) +
    geom_point(size = 3) +
    theme_bw()
```

## Clustering high CV Genes

Another approach to explore the data is to calculate the coefficient of 
variation (CV) for each gene ($g$) using the formula $CV_g = SD_g/mean_g$. We 
then identify genes with high CVs that should have large differences across 
the various profiled segments. This unbiased approach can reveal highly 
variable genes across the study. 

We plot the results using unsupervised hierarchical clustering, displayed as a 
heatmap.

``` {r CVheatmap, eval = TRUE, echo = TRUE, fig.width = 8, fig.height = 6.5, fig.wide = TRUE}
library(pheatmap)  # for pheatmap
# create a log2 transform of the data for analysis
assayDataElement(object = target_demoData, elt = "log_q") <-
    assayDataApply(target_demoData, 2, FUN = log, base = 2, elt = "q_norm")

# create CV function
calc_CV <- function(x) {sd(x) / mean(x)}
CV_dat <- assayDataApply(target_demoData,
                         elt = "log_q", MARGIN = 1, calc_CV)
# show the highest CD genes and their CV values
sort(CV_dat, decreasing = TRUE)[1:5]

# Identify genes in the top 3rd of the CV values
GOI <- names(CV_dat)[CV_dat > quantile(CV_dat, 0.8)]
pheatmap(assayDataElement(target_demoData[GOI, ], elt = "log_q"),
         scale = "row", 
         show_rownames = FALSE, show_colnames = FALSE,
         border_color = NA,
         clustering_method = "average",
         clustering_distance_rows = "correlation",
         clustering_distance_cols = "correlation",
         breaks = seq(-3, 3, 0.05),
         color = colorRampPalette(c("purple3", "black", "yellow2"))(120),
         annotation_col = 
             pData(target_demoData)[, c("batch", "segment", "Type")])
```

```{r}
saveRDS(file="/data/pangelin/HUG/Thibaud/RC_GEOMX/data/bothCohorts_raw.Rds", Data)
saveRDS(file="/data/pangelin/HUG/Thibaud/RC_GEOMX/data/bothCohorts.Rds", demoData)
saveRDS(file="/data/pangelin/HUG/Thibaud/RC_GEOMX/data/bothCohorts_target.Rds", target_demoData)
```

# Additional Resources

While this vignette has focused on the full data analysis workflow, we have 
created additional vignettes to describe in detail the functionalaties and 
tools built into the `GeomxTools` package that more advanced users may be 
interested in. Please see the 
[GeomxTools Vignette](https://bioconductor.org/packages/release/bioc/vignettes/GeomxTools/inst/doc/Introduction.html)
for more detailed information on all `GeomxTools` documentation.

## GeoMxSet Object Overview

In the section 'Loading the Demo Data', we see that the `demoData` object is 
stored as a `GeoMxSet Object`. `GeoMxSet` objects are based on `ExpressionSet` 
objects, and have many similar functions as those described 
[on Bioconductor](https://www.bioconductor.org/packages/release/bioc/vignettes/Biobase/inst/doc/ExpressionSetIntroduction.pdf). 
All expression, annotation, and probe information are linked and stored 
together as shown in the schematic below. There are a few key ways to access 
the data after we create a GeoMxSet Object. We use these throughout the 
vignette.

<center>

![](./Images/GeomxSetStructure.png)

</center>

As is shown above the GeoMxSet object consists of 3 or more elements.

* Expression count matrices - accessed with `exprs(object)` to return the 
matrix.
    + As we progress through the analysis we will add new expression matrices 
    to expression slots, or elements (`elt`), which can be accessed with 
    `assayDataElement(object, elt = ...)`
* Segment & Sample annotations - accessed with `pData(object)`
* Probe & Target information - accessed with `fData(object)`
* To learn more about other accessory functions type: `?GeomxTools::sData`

Both eSets and GeoMxSets use `esApply` to extend the
[`apply`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/apply) 
function to such objects. In addition to the `esApply` function defined by the 
`ExpressionSet` class, we have built the `assayDataApply` function to allow you 
to select which expression matrix `elt` you wish to use with the `esApply` 
function. 

## Links & References

GeoMx Background:

* [Kidney Geomx Demo Dataset](http://nanostring-public-share.s3-website-us-west-2.amazonaws.com/GeoScriptHub/Kidney_Dataset_for_GeomxTools.zip)
* [Merritt et al., 2020](https://pubmed.ncbi.nlm.nih.gov/32393914/) - 
Manuscript describing GeoMx Workflow and technology
* [Data Analysis White Paper for DSP-NGS Assays](https://www.nanostring.com/resources/geomx-cta-data-whitepaper/)
* [Zimmerman et al., 2022](https://www.biorxiv.org/content/10.1101/2021.09.29.462442v4) - Preprint on Whole Transcriptome Assay on GeoMx

Statistics & Packages:

* [Bioconductor Wesbite for GeomxTools](https://www.bioconductor.org/packages/release/bioc/html/GeomxTools.html)
* [Introduction to ExpressionSets on Bioconductor](https://www.bioconductor.org/packages/release/bioc/vignettes/Biobase/inst/doc/ExpressionSetIntroduction.pdf)
* [Lme4 package documentation](https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf)
* [Benjamini-Hochberg FDR Manuscript](http://www.math.tau.ac.il/~ybenja/MyPapers/benjamini_hochberg1995.pdf)

Other NanoString R packages:

* SpatialDecon - [Bioconductor](https://bioconductor.org/packages/release/bioc/html/SpatialDecon.html), [manuscript](https://www.nature.com/articles/s41467-022-28020-5)
* SpatialOmicsOverlay - [Bioconductor](https://bioconductor.org/packages/release/bioc/html/SpatialOmicsOverlay.html), [NanoString's GeoScriptHub](https://nanostring.com/products/geomx-digital-spatial-profiler/geoscript-hub/)

# Session Information

```{r sessInfo}
sessionInfo()
```
