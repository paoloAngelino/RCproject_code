{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a24576-a772-488e-a771-54a877db9335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from keras.callbacks import EarlyStopping\n",
    "import seaborn as sb\n",
    "\n",
    "adata = sc.read('/tmp/work/RCproject_code/sce_export.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798c049f-f452-44db-8baa-14c846a42eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "\n",
    "simplifly_categories = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c711a6-bc65-4539-a65d-c737490d8ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate numerical values for each batch category\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# set up categories variable\n",
    "categories = adata.obs['batch']\n",
    "\n",
    "#collpse the categories to microARRAY vs sequencing\n",
    "\n",
    "if simplifly_categories:\n",
    "    category_map = {'GSE133057': 'micro', 'GSE145037': 'micro', 'GSE150082': 'micro','GSE190826':'micro','GSE209746':'micro',\n",
    "                    'GSE45404_GPL1': 'micro', 'GSE45404_GPL2': 'micro', 'GSE93375': 'seq1','GSE94104': 'seq2'}\n",
    "    categories = np.vectorize(category_map.get)(categories)\n",
    "    \n",
    "# Create a LabelEncoder instance\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit and transform the categories to integers\n",
    "numerical_categories = label_encoder.fit_transform(categories)\n",
    "numerical_categories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20c8e7e-014e-498b-abb8-635de7a2ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizaiton\n",
    "gene_expression_data = adata.layers['logcounts']\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Min-max normalization\n",
    "scaler = MinMaxScaler()\n",
    "gene_expression_data = scaler.fit_transform(gene_expression_data)\n",
    "number_samples = adata.shape[0]\n",
    "number_genes = adata.shape[1]\n",
    "input_dim = number_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0e2491-dc30-4297-8a21-af878d4db5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (12165,)  # Set to match your actual data\n",
    "encoding_dim = 64  # Output dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a50af88-b0ef-413e-af9c-f664a67da86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (12165,)  # Number of genes\n",
    "\n",
    "# Define the encoder function\n",
    "def build_encoder():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(12165,)))  # Input shape matches your data\n",
    "\n",
    "    # First layer with dropout\n",
    "    model.add(layers.Dense(512, activation='linear'))  # Increased units\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))  # Add dropout for regularization\n",
    "\n",
    "    # Second layer\n",
    "    model.add(layers.Dense(256, activation='linear'))  # Increased units\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))  # Add dropout\n",
    "\n",
    "    # Third layer\n",
    "    model.add(layers.Dense(128, activation='linear'))  # Increased units\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))  # Add dropout\n",
    "\n",
    "    # Encoded representation layer\n",
    "    model.add(layers.Dense(64, activation='linear'))  # Output encoded representation\n",
    "    return model\n",
    "\n",
    "# Define the decoder function (adjusted as discussed)\n",
    "def build_decoder():    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(64,)))  # Input shape should match the output of the encoder\n",
    "    model.add(layers.Dense(128, activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(256, activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(12165, activation='sigmoid'))  # Output layer should match the input shape of the original data\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a940c8b9-1eb7-44fe-b0f1-b7e68b3310a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_domain_classifier(input_shape, num_domains):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    \n",
    "    # Increased capacity with more units and layers\n",
    "    model.add(layers.Dense(256, activation='relu'))  # Increased units and changed activation\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))  # Added dropout for regularization\n",
    "    \n",
    "    model.add(layers.Dense(128, activation='relu'))  # Increased units\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))  # Added dropout for regularization\n",
    "\n",
    "    model.add(layers.Dense(64, activation='relu'))  # Increased units\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))  # Added dropout for regularization\n",
    "    \n",
    "    model.add(layers.Dense(32, activation='relu'))  # Maintain units\n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    model.add(layers.Dense(num_domains, activation='softmax'))  # num_domains is the number of classes\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46acebae-1090-42c0-aa0f-7bc5f5bcae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, lambda_value=1.0, **kwargs):\n",
    "        self.lambda_value = lambda_value\n",
    "        super(GradientReversalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define custom gradient behavior\n",
    "        @tf.custom_gradient\n",
    "        def reverse_gradients(x):\n",
    "            # Forward pass: output is just the input\n",
    "            def grad(dy):\n",
    "                # Gradient computation: reversed and scaled by lambda_value\n",
    "                return -self.lambda_value * dy\n",
    "            return x, grad\n",
    "        \n",
    "        return reverse_gradients(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"lambda_value\": self.lambda_value})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b40d6b-ff39-4c22-ac57-cda66ec819b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder, decoder, and discriminator (assuming these functions are defined as in previous examples)\n",
    "encoder = build_encoder()  # Assuming build_encoder() is defined\n",
    "decoder = build_decoder()  # Assuming build_decoder() is defined\n",
    "discriminator = build_domain_classifier((64,), len(np.unique(categories)))  # Assuming build_domain_classifier() is defined\n",
    "\n",
    "# Optimizers for each model\n",
    "encoder_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "decoder_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Assuming you have 1000 training samples as an example\n",
    "num_samples = adata.shape[0]  # Replace this with your actual number of samples\n",
    "batch_size = 64\n",
    "\n",
    "# Calculate the number of steps per epoch\n",
    "num_steps_per_epoch = num_samples // batch_size\n",
    "\n",
    "# Compile the discriminator\n",
    "discriminator.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70af47f7-5159-4aa7-b151-21b84f6357ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_domains = len(np.unique(categories))\n",
    "X_data = gene_expression_data\n",
    "y_domains = numerical_categories\n",
    "\n",
    "reconstruction_losses = []  # To store averaged reconstruction loss values per epoch\n",
    "domain_losses = []          # To store averaged domain loss values per epoch\n",
    "distAvg_values = []\n",
    "\n",
    "balance = True\n",
    "\n",
    "total_reconstruction_loss = 0.0\n",
    "total_domain_loss = 0.0\n",
    "num_batches = 0  # To count batches in the current epoch\n",
    "epoch_count = 0\n",
    "\n",
    "# Training loop\n",
    "def time_to_train(num_epochs,lambda_value,freeze_discriminator=False, freeze_encoder=False, disc_training = True):\n",
    "    global epoch_count \n",
    "    global total_reconstruction_loss\n",
    "    global total_domain_loss\n",
    "    global num_batches\n",
    "    grl = GradientReversalLayer(lambda_value=lambda_value)  # Instantiate GRL\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        #code for determiniing if the loop should continue\n",
    "        if epoch > 10:\n",
    "            x = np.linspace(0, len(domain_losses) - 1, len(domain_losses))\n",
    "            y = domain_losses  # Some function of x (e.g., sin(x))\n",
    "            dy_dx = np.diff(y) / np.diff(x)  # Divide by the difference in x for an approximate derivative\n",
    "            domain_loss_running_avg_der = np.mean(dy_dx[-10:])  # takes the average derivative of the last 10 epochs (otherwise can be somewhat noisy)        \n",
    "            if not disc_training:\n",
    "                if domain_losses[-1] > np.log(num_domains):\n",
    "                    print(\"Condition met, exiting loop.\")  # Print before break\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "            if disc_training:\n",
    "                if domain_loss_running_avg_der < -0.0005:\n",
    "                    pass\n",
    "                else:\n",
    "                    print(\"Condition met, exiting loop.\")  # Print before break\n",
    "                    break\n",
    "\n",
    "        epoch_count += 1\n",
    "        \n",
    "        for step in range(num_steps_per_epoch):\n",
    "            # Get a batch of data\n",
    "            batch_indices = []\n",
    "            if balance == True:   #if true all domains are balanced\n",
    "                for domain in range(num_domains):\n",
    "                    domain_indices = np.where(y_domains == domain)[0]\n",
    "                    domain_batch_indices = np.random.choice(domain_indices, size=batch_size // num_domains, replace=True)\n",
    "                    batch_indices.append(domain_batch_indices)\n",
    "            else:\n",
    "                    all_indices = np.arange(len(y_domains))  # Get all indices\n",
    "                    # Randomly sample batch_size indices from all available indices\n",
    "                    domain_batch_indices = np.random.choice(all_indices, size=batch_size, replace=True)\n",
    "                    batch_indices.append(domain_batch_indices)\n",
    "    \n",
    "            X_batch = X_data[np.concatenate(batch_indices)]\n",
    "            y_true_batch = y_domains[np.concatenate(batch_indices)]\n",
    "            y_true = tf.keras.utils.to_categorical(y_true_batch, num_classes=num_domains)  # Convert labels to categorical\n",
    "        \n",
    "            # Train on reconstruction loss\n",
    "            with tf.GradientTape(persistent = True) as autoencoder_tape:\n",
    "                # Encode\n",
    "                encoded_output = encoder(X_batch, training=True)\n",
    "    \n",
    "                # Decode\n",
    "                decoded_output = decoder(encoded_output, training=True)\n",
    "    \n",
    "                # Calculate reconstruction loss\n",
    "                reconstruction_loss = tf.reduce_mean(tf.square(X_batch - decoded_output))\n",
    "    \n",
    "            # Calculate gradients for encoder and decoder using the same tape\n",
    "            encoder_gradients = autoencoder_tape.gradient(reconstruction_loss, encoder.trainable_variables)\n",
    "            decoder_gradients = autoencoder_tape.gradient(reconstruction_loss, decoder.trainable_variables)\n",
    "    \n",
    "            #update the encoder and the decoder\n",
    "            encoder_optimizer.apply_gradients(zip(encoder_gradients, encoder.trainable_variables))\n",
    "            decoder_optimizer.apply_gradients(zip(decoder_gradients, decoder.trainable_variables))\n",
    "    \n",
    "            # Now, train on domain loss using a new tape\n",
    "            with tf.GradientTape(persistent = True) as discriminator_tape:\n",
    "    \n",
    "                # Encode\n",
    "                encoded_output = encoder(X_batch, training=True)\n",
    "                \n",
    "                # Apply gradient reversal\n",
    "                encoded_output_grl = grl(encoded_output)\n",
    "    \n",
    "                # Domain output\n",
    "                domain_output = discriminator(encoded_output_grl, training=True)\n",
    "    \n",
    "                # Calculate domain loss\n",
    "                domain_loss = tf.keras.losses.categorical_crossentropy(y_true, domain_output)\n",
    "                domain_loss = tf.reduce_mean(domain_loss)  # Average over the batch\n",
    "    \n",
    "            # Calculate gradients for the discriminator and the encdoer\n",
    "    \n",
    "            # Only update discriminator if lambda_value is non-positive, essentially freezing the weight updates when lambda is positive\n",
    "            if freeze_discriminator == False:\n",
    "                # Update discriminator\n",
    "                discriminator_gradients = discriminator_tape.gradient(domain_loss, discriminator.trainable_variables)\n",
    "                discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "\n",
    "            if freeze_encoder == False:\n",
    "                encoder_gradients = discriminator_tape.gradient(domain_loss, encoder.trainable_variables)\n",
    "                encoder_optimizer.apply_gradients(zip(encoder_gradients, encoder.trainable_variables))\n",
    "\n",
    "            # Accumulate losses\n",
    "            total_reconstruction_loss += reconstruction_loss.numpy()\n",
    "            total_domain_loss += domain_loss.numpy()\n",
    "            num_batches += 1\n",
    "    \n",
    "        # Calculate average losses for the epoch (same as before)\n",
    "        average_reconstruction_loss = total_reconstruction_loss / num_batches\n",
    "        average_domain_loss = total_domain_loss / num_batches\n",
    "    \n",
    "        # Store and print average losses\n",
    "        reconstruction_losses.append(average_reconstruction_loss)\n",
    "        domain_losses.append(average_domain_loss)\n",
    "\n",
    "        #store the average encoder distances\n",
    "        encoded_data = encoder.predict(gene_expression_data)\n",
    "        distAvg_values.append(np.mean(pdist(encoded_data)))\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Average Reconstruction Loss: {average_reconstruction_loss}\")\n",
    "        print(f\"Epoch {epoch + 1}, Average Domain Loss: {average_domain_loss}\")\n",
    "        print(f\"Epoch {epoch + 1}, Average Representation Distance: {distAvg_values[-1]}\")\n",
    "\n",
    "    low_dimensional_representation = encoder.predict(gene_expression_data)\n",
    "\n",
    "    return(low_dimensional_representation)\n",
    "\n",
    "total_epochs = 1000\n",
    "\n",
    "#First train the discriminator and the encoder together\n",
    "\n",
    "first_rep = time_to_train(num_epochs=1000, lambda_value = -1, freeze_discriminator= False, freeze_encoder=False, disc_training = True)\n",
    "\n",
    "while epoch_count < 1000:\n",
    "    # beat the discriminator\n",
    "    print('Fooling the disciminiator')\n",
    "    low_dimensional_representation = time_to_train(num_epochs =1000, lambda_value = 1, freeze_discriminator= True, freeze_encoder=False, disc_training = False)\n",
    "    pre_epoch_count = epoch_count\n",
    "    # allow the discriminator to imrpove but freeze the encoder\n",
    "    print('Learning to distinguish the domains')\n",
    "    low_dimensional_representation = time_to_train(num_epochs = 1000, lambda_value = 1, freeze_discriminator= False, freeze_encoder=True, disc_training = True)\n",
    "    epoch_delta = epoch_count - pre_epoch_count\n",
    "    if epoch_delta <= 11:  # this means that the benchmarks are reached as soon as possibly for both training types\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154f64b-cebc-425d-81ac-37f3f7eef44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of the domain losses\n",
    "\n",
    "x = np.linspace(0, len(domain_losses), len(domain_losses))  # 100 points between 0 and 10\n",
    "y = domain_losses  # Some function of x (e.g., sin(x))\n",
    "\n",
    "# Calculate the running derivative using numpy.diff\n",
    "dy_dx = np.diff(y) / np.diff(x)  # Divide by the difference in x for an approximate derivative\n",
    "\n",
    "# Plot the original function\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y, label='Original domain losses')\n",
    "plt.title('Domain losses')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "# For plotting the derivative, adjust x to match the size of the derivative (which has n-1 points)\n",
    "x_mid = (x[:-1] + x[1:]) / 2  # Midpoints of x for the derivative plot\n",
    "\n",
    "# Plot the derivative\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_mid, dy_dx, label='Derivative of domain losses', color='r')\n",
    "plt.title('Derivative of Domain Losses')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('dy/dx')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b93d932-1b1f-4902-aa6c-415fb5e870b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, plot the losses\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 8))\n",
    "\n",
    "# Plot Reconstruction Loss\n",
    "axs[0].plot(range(1, epoch_count + 1), reconstruction_losses, label='Reconstruction Loss', color='blue')\n",
    "axs[0].set_title('Reconstruction Loss Over Epochs')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].grid()\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot Domain Loss\n",
    "axs[1].plot(range(1, epoch_count + 1), domain_losses, label='Domain Loss', color='orange')\n",
    "axs[1].set_title('Domain Loss Over Epochs')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].grid()\n",
    "axs[1].legend()\n",
    "\n",
    "# Plot the average distance values\n",
    "axs[2].plot(range(1, epoch_count + 1), distAvg_values, label='Aveerage distance between representations', color='red')\n",
    "axs[2].set_title('Average distance between representations across Epochs')\n",
    "axs[2].set_xlabel('Epoch')\n",
    "axs[2].set_ylabel('Distance')\n",
    "axs[2].grid()\n",
    "axs[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3268dd3d-948c-4b97-a974-a780b875264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the represetations from the trained model\n",
    "low_dimensional_representation = encoder.predict(gene_expression_data)\n",
    "\n",
    "#verify the shape\n",
    "print(low_dimensional_representation.shape)\n",
    "\n",
    "#set up the umap\n",
    "\n",
    "umap_model = umap.UMAP(n_neighbors=15,   # Controls local vs global structure\n",
    "                        min_dist=0.1,    # Controls how tightly UMAP packs points together\n",
    "                        metric='euclidean')  # Distance metric to use\n",
    "\n",
    "umap_result = umap_model.fit_transform(low_dimensional_representation)\n",
    "\n",
    "#checking batch separation\n",
    "current_label =  adata.obs['batch']\n",
    "umap_data = {'UMAP1': umap_result[:, 0], 'UMAP2': umap_result[:, 1],'batch':current_label}\n",
    "umap_df = pd.DataFrame(data=umap_data)\n",
    "ax = sb.scatterplot(data=umap_df, x='UMAP1', y='UMAP2', hue='batch')\n",
    "sb.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a37c51-6ad4-4c9d-8a67-d22c4d4d06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking response variable separation\n",
    "current_label =  adata.obs['Response']\n",
    "umap_data = {'UMAP1': umap_result[:, 0], 'UMAP2': umap_result[:, 1],'Response':current_label}\n",
    "umap_df = pd.DataFrame(data=umap_data)\n",
    "ax = sb.scatterplot(data=umap_df, x='UMAP1', y='UMAP2', hue='Response')\n",
    "sb.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f1ca4b-a661-41e9-8f15-531bd7970ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the embeddings for random forest classification\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1cd6dd-1038-4665-b038-3fc4514a268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## random forest with original data\n",
    "indices = [i for i, s in enumerate(adata.obs['Response']) if s in [\"yes\", \"no\"]]\n",
    "y = adata.obs['Response'][indices]\n",
    "y = [1 if x == \"yes\" else 0 for x in y]\n",
    "y = np.array(y)\n",
    "X = X_data[indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_prob = clf.predict_proba(X_test)[:, 1]  # Probabilities for the positive class (class 1)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line for random guessing\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6edef-f192-4342-82fa-669e9db1d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "## random forest with transformed data\n",
    "indices = [i for i, s in enumerate(adata.obs['Response']) if s in [\"yes\", \"no\"]]\n",
    "y = adata.obs['Response'][indices]\n",
    "y = [1 if x == \"yes\" else 0 for x in y]\n",
    "y = np.array(y)\n",
    "X = low_dimensional_representation[indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_prob = clf.predict_proba(X_test)[:, 1]  # Probabilities for the positive class (class 1)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line for random guessing\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f94445-87c0-4897-a320-eacd70555fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #set up the umap\n",
    "\n",
    "# all_sets = np.vstack((first_rep,second_rep,third_rep,low_dimensional_representation))\n",
    "\n",
    "# umap_model = umap.UMAP(n_neighbors=15,   # Controls local vs global structure\n",
    "#                         min_dist=0.1,    # Controls how tightly UMAP packs points together\n",
    "#                         metric='euclidean')  # Distance metric to use\n",
    "\n",
    "# umap_result = umap_model.fit_transform(all_sets)\n",
    "\n",
    "# first = np.repeat(1,first_rep.shape[0])\n",
    "# training_stage = np.concatenate((first,first*2,first*3,first*4))\n",
    "\n",
    "# #checking batch separation\n",
    "# current_label =  adata.obs['batch']\n",
    "# umap_data = {'UMAP1': umap_result[:, 0], 'UMAP2': umap_result[:, 1],'stage':training_stage}\n",
    "# umap_df = pd.DataFrame(data=umap_data)\n",
    "# ax = sb.scatterplot(data=umap_df, x='UMAP1', y='UMAP2', hue='stage')\n",
    "# sb.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8341f67-d016-4f09-a97f-747cb79cd02b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
