{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf24bb4-d77f-476d-9820-3150a572b0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "2024-11-12 08:24:27.583341: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import rpy2.robjects as ro\n",
    "import rpy2.robjects.packages as rpackages\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from keras.callbacks import EarlyStopping\n",
    "import seaborn as sb\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import pearsonr\n",
    "import time\n",
    "from keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import random\n",
    "import rpy2.robjects as ro\n",
    "from rpy2 import robjects\n",
    "from rpy2.robjects import r\n",
    "from rpy2.robjects import pandas2ri\n",
    "import pandas as pd\n",
    "\n",
    "# adata = sc.read('/tmp/work/RCproject_code/sce_export.h5ad')\n",
    "epoch_count = 0\n",
    "\n",
    "# Enable automatic conversion between R objects and pandas DataFrames\n",
    "pandas2ri.activate()\n",
    "\n",
    "#exclude partial\n",
    "# adata = adata[~adata.obs['Response'].isin(['partial']), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb21723-a836-4597-927b-04267ec8d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_size = 0.1\n",
    "dropout_rate = 0.3 #0.1\n",
    "balance = True\n",
    "l2_reg = 0.2\n",
    "batch_size = 16  #determines how many samples are processed per batch, each epoch will process multiple batches\n",
    "# learning_rate = 0.00001\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100000\n",
    "report_frequency = 1\n",
    "accuracy_threshold = 0.9\n",
    "auc_threshold = 0.89\n",
    "clipnorm = 2.0\n",
    "simplifly_categories = True\n",
    "holdout_size = 0.5\n",
    "\n",
    "use_gene_list = True\n",
    "current_gene_list = 'de_intersect_plus_bulk_genes'#'sig9_0.05'\n",
    "\n",
    "PCA_reduce = False\n",
    "n_comp_PCA = 16\n",
    "\n",
    "#dynamic learning rate parameters\n",
    "\n",
    "lr_dict = {\n",
    "    0.6: 0.001,\n",
    "    0.7: 0.0001,   # Learning rate if AUC < 0.7\n",
    "    0.8: 0.00001,  # Learning rate if 0.7 <= AUC < 0.8\n",
    "    0.85: 0.000001,  # Learning rate if 0.8 <= AUC < 0.9\n",
    "    0.88:  0.0000005,\n",
    "    0.89:  0.0000001,\n",
    "    0.9: 0.00000005,\n",
    "    0.9: 0.00000001# Learning rate if AUC >= 0.9\n",
    "}\n",
    "\n",
    "auc_thresholds = [0.6, 0.7, 0.8, 0.85, 0.88,0.90,0.95]  # AUC values at which the learning rate should be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f6015e9-d6a4-4680-b92d-3303d6f13915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['meta', 'DE4', 'DEall', 'meta_intersect_unionDE', 'de_intersect', 'de_intersect_plus_bulk_genes', '5k', 'CV', 'CV300', 'CV300_4MA', 'Boruta_4MA', 'Boruta_6', 'CVBig', 'final300', 'final350', 'final', 'sig_4_6_0.05', 'sig_4_6_0.1', 'sig9_0.05', 'sig9_0.01', 'sig9', 'GEOMX_4_6_0.05_'])\n"
     ]
    }
   ],
   "source": [
    "# Load the RDS file\n",
    "rds_path = '/tmp/work/RCproject/gene_lists.rds'\n",
    "rds_data = r.readRDS(rds_path)\n",
    "\n",
    "# Extract the names of the lists and their contents\n",
    "gene_lists = {}\n",
    "for name, item in zip(rds_data.names, rds_data):\n",
    "    # Each 'item' is a list associated with the 'name'\n",
    "    inner_list = list(item)  # Convert the inner R list to a Python list\n",
    "    gene_lists[name] = inner_list\n",
    "\n",
    "# Now `python_data` is a dictionary with names as keys and lists as values\n",
    "print(gene_lists.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d29fdb4d-c7cc-4693-910f-ef519d59bf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366\n"
     ]
    }
   ],
   "source": [
    "# select a gene_list\n",
    "\n",
    "current_genes = gene_lists[current_gene_list]\n",
    "print(len(current_genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db574c6-719c-4623-a50d-a80053d5eb57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: SingleCellExperiment\n",
      "\n",
      "R[write to console]: Loading required package: SummarizedExperiment\n",
      "\n",
      "R[write to console]: Loading required package: MatrixGenerics\n",
      "\n",
      "R[write to console]: Loading required package: matrixStats\n",
      "\n",
      "R[write to console]: \n",
      "Attaching package: ‘MatrixGenerics’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:matrixStats’:\n",
      "\n",
      "    colAlls, colAnyNAs, colAnys, colAvgsPerRowSet, colCollapse,\n",
      "    colCounts, colCummaxs, colCummins, colCumprods, colCumsums,\n",
      "    colDiffs, colIQRDiffs, colIQRs, colLogSumExps, colMadDiffs,\n",
      "    colMads, colMaxs, colMeans2, colMedians, colMins, colOrderStats,\n",
      "    colProds, colQuantiles, colRanges, colRanks, colSdDiffs, colSds,\n",
      "    colSums2, colTabulates, colVarDiffs, colVars, colWeightedMads,\n",
      "    colWeightedMeans, colWeightedMedians, colWeightedSds,\n",
      "    colWeightedVars, rowAlls, rowAnyNAs, rowAnys, rowAvgsPerColSet,\n",
      "    rowCollapse, rowCounts, rowCummaxs, rowCummins, rowCumprods,\n",
      "    rowCumsums, rowDiffs, rowIQRDiffs, rowIQRs, rowLogSumExps,\n",
      "    rowMadDiffs, rowMads, rowMaxs, rowMeans2, rowMedians, rowMins,\n",
      "    rowOrderStats, rowProds, rowQuantiles, rowRanges, rowRanks,\n",
      "    rowSdDiffs, rowSds, rowSums2, rowTabulates, rowVarDiffs, rowVars,\n",
      "    rowWeightedMads, rowWeightedMeans, rowWeightedMedians,\n",
      "    rowWeightedSds, rowWeightedVars\n",
      "\n",
      "\n",
      "R[write to console]: Loading required package: GenomicRanges\n",
      "\n",
      "R[write to console]: Loading required package: stats4\n",
      "\n",
      "R[write to console]: Loading required package: BiocGenerics\n",
      "\n",
      "R[write to console]: \n",
      "Attaching package: ‘BiocGenerics’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    IQR, mad, sd, var, xtabs\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n",
      "    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n",
      "    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n",
      "    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n",
      "    Position, rank, rbind, Reduce, rownames, sapply, setdiff, sort,\n",
      "    table, tapply, union, unique, unsplit, which.max, which.min\n",
      "\n",
      "\n",
      "R[write to console]: Loading required package: S4Vectors\n",
      "\n",
      "R[write to console]: \n",
      "Attaching package: ‘S4Vectors’\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from ‘package:utils’:\n",
      "\n",
      "    findMatches\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    expand.grid, I, unname\n",
      "\n",
      "\n",
      "R[write to console]: Loading required package: IRanges\n",
      "\n",
      "R[write to console]: Loading required package: GenomeInfoDb\n",
      "\n",
      "R[write to console]: Loading required package: Biobase\n",
      "\n",
      "R[write to console]: Welcome to Bioconductor\n",
      "\n",
      "    Vignettes contain introductory material; view with\n",
      "    'browseVignettes()'. To cite Bioconductor, see\n",
      "    'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n",
      "\n",
      "\n",
      "R[write to console]: \n",
      "Attaching package: ‘Biobase’\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from ‘package:MatrixGenerics’:\n",
      "\n",
      "    rowMedians\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:matrixStats’:\n",
      "\n",
      "    anyMissing, rowMedians\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts DataFrame:\n",
      "        GSM3899156_GSE133057  GSM3899157_GSE133057  GSM3899158_GSE133057  \\\n",
      "A1CF                0.057669             -0.027535              0.447468   \n",
      "A2M                 0.392288             -0.233300             -0.691535   \n",
      "A2ML1              -0.448972             -0.560902             -0.670516   \n",
      "A4GALT             -0.218099              0.224682             -0.247762   \n",
      "AAAS               -1.148204             -0.252932              0.053839   \n",
      "\n",
      "        GSM3899159_GSE133057  GSM3899160_GSE133057  GSM3899161_GSE133057  \\\n",
      "A1CF                0.045675              0.014587              1.406735   \n",
      "A2M                 0.293834             -1.091569             -0.816868   \n",
      "A2ML1               0.532968              0.516362              1.266745   \n",
      "A4GALT             -1.663294             -0.673025              0.513021   \n",
      "AAAS                0.442668              0.006890              0.582186   \n",
      "\n",
      "        GSM3899162_GSE133057  GSM3899163_GSE133057  GSM3899164_GSE133057  \\\n",
      "A1CF               -0.390043             -0.141597             -0.939894   \n",
      "A2M                 0.115628              0.384196              1.236704   \n",
      "A2ML1               1.132630              0.017735              0.335514   \n",
      "A4GALT              0.452215              1.025558              1.812865   \n",
      "AAAS                0.456027              0.470222             -0.351307   \n",
      "\n",
      "        GSM3899165_GSE133057  ...  GSM6390453_GSE209746  GSM6390454_GSE209746  \\\n",
      "A1CF                0.496517  ...              0.481642             -0.494055   \n",
      "A2M                 0.721205  ...              1.271784              2.545357   \n",
      "A2ML1               0.367068  ...             -1.763592             -1.603974   \n",
      "A4GALT             -0.386278  ...             -0.792048             -0.029957   \n",
      "AAAS                0.276047  ...              0.178485              0.211358   \n",
      "\n",
      "        GSM6390455_GSE209746  GSM6390456_GSE209746  GSM6390457_GSE209746  \\\n",
      "A1CF                0.692194              0.141950              0.554797   \n",
      "A2M                 1.705187              2.176357              2.173685   \n",
      "A2ML1              -1.897987             -1.911380             -1.666886   \n",
      "A4GALT             -0.723102             -0.532311             -0.472129   \n",
      "AAAS                0.189983              0.189037             -0.142190   \n",
      "\n",
      "        GSM6390458_GSE209746  GSM6390459_GSE209746  GSM6390460_GSE209746  \\\n",
      "A1CF                0.756641              0.757147              0.263922   \n",
      "A2M                 1.465240              1.147636              2.230243   \n",
      "A2ML1              -1.513177             -1.772573             -1.714675   \n",
      "A4GALT             -1.560049             -1.291245             -0.483064   \n",
      "AAAS               -0.114270              0.214479             -0.019179   \n",
      "\n",
      "        GSM6390461_GSE209746  GSM6390462_GSE209746  \n",
      "A1CF                0.054849              1.130573  \n",
      "A2M                 2.129601              1.589073  \n",
      "A2ML1              -2.031892             -1.777141  \n",
      "A4GALT             -0.167779             -0.986447  \n",
      "AAAS                0.088624             -0.014329  \n",
      "\n",
      "[5 rows x 450 columns]\n",
      "\n",
      "Metadata DataFrame:\n",
      "                     Response  TRG therapy Treatment Platform      batch\n",
      "GSM3899156_GSE133057      yes  1,2     CRT       pre  GPL6102  GSE133057\n",
      "GSM3899157_GSE133057  partial    3     CRT       pre  GPL6102  GSE133057\n",
      "GSM3899158_GSE133057      yes  1,2     CRT       pre  GPL6102  GSE133057\n",
      "GSM3899159_GSE133057  partial    3     CRT       pre  GPL6102  GSE133057\n",
      "GSM3899160_GSE133057      yes  1,2     CRT       pre  GPL6102  GSE133057\n"
     ]
    }
   ],
   "source": [
    "#import the data\n",
    "\n",
    "# Function to read RDS file and extract counts and metadata\n",
    "def read_rds_to_matrix_and_metadata(file_path):\n",
    "    # Load the RDS file in R\n",
    "    ro.r(f\"sce <- readRDS('{file_path}')\")\n",
    "\n",
    "    # Extract count data (assumed to be stored in assays)\n",
    "    counts = ro.r('assay(sce, \"scalelogcounts\")')\n",
    "    # Extract row (gene) and column (cell) names\n",
    "    gene_names = ro.r('rownames(sce)')\n",
    "    cell_names = ro.r('colnames(sce)')\n",
    "    \n",
    "    # Convert to a NumPy array\n",
    "    counts_np = ro.conversion.rpy2py(counts)\n",
    "\n",
    "    # Convert the counts matrix to a pandas DataFrame\n",
    "    counts_df = pd.DataFrame(counts_np, index=gene_names, columns=cell_names)\n",
    "\n",
    "    # Extract metadata from colData and convert to a pandas DataFrame directly\n",
    "    metadata = ro.r('as.data.frame(colData(sce))')  # Get the colData as an R data frame\n",
    "    metadata_df = pd.DataFrame(metadata)  # Convert R data frame to pandas DataFrame directly\n",
    "\n",
    "    return counts_df, metadata_df\n",
    "\n",
    "# Usage example\n",
    "file_path = '/tmp/work/RCproject/GEO_singlecellexperiment.rds'\n",
    "counts_df, metadata_df = read_rds_to_matrix_and_metadata(file_path)\n",
    "\n",
    "# Display the results\n",
    "print(\"Counts DataFrame:\")\n",
    "print(counts_df.head())  # Show the first few rows of the counts DataFrame\n",
    "print(\"\\nMetadata DataFrame:\")\n",
    "print(metadata_df.head())  # Show the first few rows of the metadata DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6583f32-fa79-4bc3-b575-e0cd98b82eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gene_list:\n",
    "    counts_df = counts_df.loc[current_genes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcdfc63c-ef87-4636-a16f-89c97a7568db",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_metadata_df = metadata_df[~metadata_df['Response'].isin(['partial'])].copy()\n",
    "\n",
    "row_names = filtered_metadata_df.index.tolist()  # Convert index to a list\n",
    "\n",
    "filtered_counts_df = counts_df[row_names].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7a6582-5a3a-4174-83e2-6e0d55f0d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata = metadata_df[~adata.obs['Response'].isin(['partial']), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f915216b-b70c-4c2d-a76d-a43fad99f14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(410,)\n",
      "(410,)\n"
     ]
    }
   ],
   "source": [
    "# generate numerical values for each batch category\n",
    "# set up categories variable\n",
    "\n",
    "# categories_technology = adata.obs['batch']\n",
    "categories_technology = filtered_metadata_df['batch']\n",
    "\n",
    "#collpse the categories to microARRAY vs sequencing\n",
    "\n",
    "if simplifly_categories:\n",
    "    category_map = {'GSE133057': 'micro', 'GSE145037': 'micro', 'GSE150082': 'micro','GSE190826':'seq','GSE209746':'seq',\n",
    "                    'GSE45404_GPL1': 'micro', 'GSE45404_GPL2': 'micro', 'GSE93375': 'micro','GSE94104': 'micro'}\n",
    "    categories_technology = np.vectorize(category_map.get)(categories_technology)\n",
    "    \n",
    "# Create a LabelEncoder instance\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit and transform the categories to integers\n",
    "numerical_categories_technology = label_encoder.fit_transform(categories_technology)\n",
    "print(numerical_categories_technology.shape)\n",
    "\n",
    "#do the same for the response variable\n",
    "\n",
    "# categories_outcome = adata.obs['Response']\n",
    "categories_outcome = filtered_metadata_df['Response']\n",
    "\n",
    "numerical_categories_outcome = label_encoder.fit_transform(categories_outcome)\n",
    "print(numerical_categories_technology.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8664a4b-9266-4cf4-b692-2c0d4ed39a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technology frequencies\n",
      "0    209\n",
      "1    201\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Outcome frequencies\n",
      "0    267\n",
      "1    143\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_counts = pd.Series(numerical_categories_technology).value_counts()\n",
    "print('Technology frequencies')\n",
    "print(frequency_counts)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Outcome frequencies')\n",
    "frequency_counts = pd.Series(numerical_categories_outcome).value_counts()\n",
    "print(frequency_counts)\n",
    "\n",
    "unique_combinations_array = (numerical_categories_outcome + (numerical_categories_technology+1)*2)-2\n",
    "\n",
    "np.unique(unique_combinations_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "056bf30b-3e46-4f4c-a4af-84b512dacb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming filtered_metadata_df is your filtered DataFrame\n",
    "filtered_metadata_df.loc[:, 'numerical_categories_technology'] = numerical_categories_technology\n",
    "filtered_metadata_df.loc[:, 'numerical_categories_outcome'] = numerical_categories_outcome\n",
    "filtered_metadata_df.loc[:, 'combination_tech_outcome'] = unique_combinations_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b4334e7-f769-4c03-937b-e148872ee0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizaiton\n",
    "# gene_expression_data = adata.layers['scalelogcounts']\n",
    "\n",
    "gene_expression_data = filtered_counts_df.T\n",
    "\n",
    "#Min-max normalization\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "gene_expression_data = scaler.fit_transform(gene_expression_data)\n",
    "\n",
    "number_genes = gene_expression_data.shape[1]\n",
    "input_dim = number_genes\n",
    "\n",
    "if PCA_reduce:\n",
    "# Initialize PCA and fit it to X_train\n",
    "    n_components = n_comp_PCA  # You can adjust this based on your data\n",
    "    pca = PCA(n_components=n_components)\n",
    "    gene_expression_data = pca.fit_transform(gene_expression_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d46fe7c-8faf-4f29-82f8-f2d00be059e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the test and train datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(gene_expression_data, filtered_metadata_df, test_size=test_set_size, random_state=1)\n",
    "\n",
    "y_train_outcome = y_train['numerical_categories_outcome']\n",
    "y_test_outcome = y_test['numerical_categories_outcome']\n",
    "\n",
    "y_train_tech = y_train['numerical_categories_technology']\n",
    "y_test_tech = y_test['numerical_categories_technology']\n",
    "\n",
    "y_train_comb = y_train['combination_tech_outcome']\n",
    "y_test_comb = y_test['combination_tech_outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0f626da-39f7-418e-8e73-f267b981e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the input shape\n",
    "# # input_shape = (gene_expression_data.shape[1],)[0]  # Number of genes\n",
    "\n",
    "input_shape = (X_train.shape[1],)[0]  # Number of genes\n",
    "\n",
    "def build_outcome_classifier():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(input_shape,)))  # Input shape matches your data\n",
    "    \n",
    "    model.add(layers.Dense((512),kernel_regularizer=tf.keras.regularizers.l2(l2_reg),kernel_initializer='he_normal'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.1))  # Leaky ReLU helps with vanishing gradients\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(layers.Dense((256),kernel_regularizer=tf.keras.regularizers.l2(l2_reg),kernel_initializer='he_normal'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Dense((128),kernel_regularizer=tf.keras.regularizers.l2(l2_reg),kernel_initializer='he_normal'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(layers.Dense((64),kernel_regularizer=tf.keras.regularizers.l2(l2_reg),kernel_initializer='he_normal'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Dense((32),kernel_initializer='he_normal'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.1))\n",
    "    \n",
    "    # Output layer for binary classification with sigmoid activation\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f07cdd3d-9030-4220-93f4-f1bbf2e8713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "outcome_classifier = build_outcome_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a566cb12-0842-4253-9379-d32fdebb4225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the callback-like function to adjust learning rate based on AUC\n",
    "def adjust_learning_rate_by_auc(epoch, model, X_test, y_test_outcome, lr_dict, auc_thresholds,test_auc):\n",
    "    # Get model's current learning rate\n",
    "    current_lr = tf.keras.backend.get_value(model.optimizer.lr)\n",
    "    \n",
    "    # # Make predictions on the test set\n",
    "    # outcome_predictions = model(X_test)\n",
    "    # outcome_predictions_np = outcome_predictions.numpy().flatten()  # Convert to numpy for roc_auc_score\n",
    "    # outcome_labels_float = tf.cast(y_test_outcome, tf.float32).numpy().flatten()\n",
    "\n",
    "    # # Calculate AUC\n",
    "    # test_auc = roc_auc_score(outcome_labels_float, outcome_predictions_np)\n",
    "    # print(f\"Epoch {epoch + 1}: Test AUC = {test_auc:.4f}\")\n",
    "\n",
    "    # Adjust learning rate based on AUC thresholds\n",
    "    new_lr = current_lr\n",
    "    for threshold in auc_thresholds:\n",
    "        if test_auc >= threshold:\n",
    "            new_lr = lr_dict[threshold]\n",
    "    \n",
    "    # Set new learning rate\n",
    "    if new_lr != current_lr:\n",
    "        tf.keras.backend.set_value(model.optimizer.lr, new_lr)\n",
    "        print(f\"Epoch {epoch + 1}: Adjusting learning rate to {new_lr:.6f}\")\n",
    "    \n",
    "    return test_auc\n",
    "\n",
    "# # Define the callback-like function to adjust learning rate based on AUC\n",
    "# def adjust_learning_rate_by_auc(epoch, model, X_test, y_test_outcome, lr_dict, auc_thresholds, previous_auc):\n",
    "#     \"\"\"\n",
    "#     Adjust the learning rate based on the AUC score only if there is a significant improvement or decline.\n",
    "#     If AUC is unchanged or within the margin, do nothing.\n",
    "\n",
    "#     Parameters:\n",
    "#     - epoch: Current epoch number\n",
    "#     - model: The trained model\n",
    "#     - X_test: The test features\n",
    "#     - y_test_outcome: The test labels (outcome)\n",
    "#     - lr_dict: Dictionary containing learning rate values for different AUC thresholds\n",
    "#     - auc_thresholds: List of AUC thresholds to determine learning rate changes\n",
    "#     - previous_auc: The AUC from the previous epoch to compare for significant changes\n",
    "\n",
    "#     Returns:\n",
    "#     - Updated AUC value after evaluation\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Get model's current learning rate\n",
    "#     current_lr = tf.keras.backend.get_value(model.optimizer.lr)\n",
    "    \n",
    "#     # Make predictions on the test set\n",
    "#     outcome_predictions = model(X_test)\n",
    "#     outcome_predictions_np = outcome_predictions.numpy().flatten()  # Convert to numpy for AUC\n",
    "#     outcome_labels_float = tf.cast(y_test_outcome, tf.float32).numpy().flatten()\n",
    "\n",
    "#     # Calculate AUC\n",
    "#     test_auc = roc_auc_score(outcome_labels_float, outcome_predictions_np)\n",
    "#     print(f\"Epoch {epoch + 1}: Test AUC = {test_auc:.4f}\")\n",
    "\n",
    "#     # Initialize new learning rate to the current one\n",
    "#     new_lr = current_lr\n",
    "    \n",
    "#     # Adjust learning rate based on AUC thresholds, only if AUC has changed meaningfully\n",
    "#     if test_auc > previous_auc + auc_thresholds.get('improvement', 0.01):\n",
    "#         # If AUC has significantly improved, increase learning rate\n",
    "#         new_lr = lr_dict.get('increase', current_lr)\n",
    "#     elif test_auc < previous_auc - auc_thresholds.get('decrease', 0.01):\n",
    "#         # If AUC has decreased significantly, decrease learning rate\n",
    "#         new_lr = lr_dict.get('decrease', current_lr)\n",
    "    \n",
    "#     # Set new learning rate if there is a change\n",
    "#     if new_lr != current_lr:\n",
    "#         tf.keras.backend.set_value(model.optimizer.lr, new_lr)\n",
    "#         print(f\"Epoch {epoch + 1}: Adjusting learning rate to {new_lr:.6f}\")\n",
    "\n",
    "#     # Return the AUC for the current epoch to update the previous AUC for the next epoch\n",
    "#     return test_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e85e8399-811c-42d2-8150-72318a156f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set up the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm = clipnorm)\n",
    "\n",
    "# determine the sample and batch size\n",
    "num_samples = math.floor(X_train.shape[0]* (1-holdout_size))  # number of samples used in each training epoch\n",
    "\n",
    "# batch_size = adata.shape[0]\n",
    "\n",
    "# Calculate the number of steps per epoch\n",
    "num_steps_per_epoch = num_samples // batch_size\n",
    "\n",
    "# Compile the outcome discriminator\n",
    "outcome_classifier.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0ecf815-9b2c-4a7d-bd55-af6e2326a740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Outcome Loss: 0.7064828385006298, Average Accuracy: 0.5170454545454546, Test AUC: 0.3821, Test Accuracy: 0.3659\n",
      "Epoch 1, Average Outcome Loss: 7.272948568517512, Average Accuracy: 0.5, Test AUC: 0.6308, Test Accuracy: 0.6341\n",
      "Epoch 2: Adjusting learning rate to 0.001000\n",
      "Epoch 2, Average Outcome Loss: 1.9471855597062544, Average Accuracy: 0.5, Test AUC: 0.6538, Test Accuracy: 0.6341\n",
      "Epoch 3: Adjusting learning rate to 0.001000\n",
      "Epoch 3, Average Outcome Loss: 0.8233749162067067, Average Accuracy: 0.5, Test AUC: 0.3923, Test Accuracy: 0.3659\n",
      "Epoch 4, Average Outcome Loss: 0.7943621223623102, Average Accuracy: 0.5, Test AUC: 0.3667, Test Accuracy: 0.3659\n",
      "Epoch 5, Average Outcome Loss: 0.8237812248143283, Average Accuracy: 0.5, Test AUC: 0.4949, Test Accuracy: 0.3659\n",
      "Epoch 6, Average Outcome Loss: 0.6936056288805875, Average Accuracy: 0.5, Test AUC: 0.6769, Test Accuracy: 0.6341\n",
      "Epoch 7: Adjusting learning rate to 0.001000\n",
      "Epoch 7, Average Outcome Loss: 0.9204108877615496, Average Accuracy: 0.5, Test AUC: 0.6974, Test Accuracy: 0.6341\n",
      "Epoch 8: Adjusting learning rate to 0.001000\n",
      "Epoch 8, Average Outcome Loss: 0.8399704532189802, Average Accuracy: 0.5, Test AUC: 0.5026, Test Accuracy: 0.6341\n",
      "Epoch 9, Average Outcome Loss: 0.6897797205231406, Average Accuracy: 0.5454545454545454, Test AUC: 0.4179, Test Accuracy: 0.3659\n",
      "Epoch 10, Average Outcome Loss: 0.7004045085473494, Average Accuracy: 0.5, Test AUC: 0.4282, Test Accuracy: 0.3659\n",
      "Epoch 11, Average Outcome Loss: 0.7255307815291665, Average Accuracy: 0.5, Test AUC: 0.4308, Test Accuracy: 0.3659\n",
      "Epoch 12, Average Outcome Loss: 0.7062960158694874, Average Accuracy: 0.5, Test AUC: 0.4769, Test Accuracy: 0.6585\n",
      "Epoch 13, Average Outcome Loss: 0.6901304613460194, Average Accuracy: 0.5284090909090909, Test AUC: 0.6487, Test Accuracy: 0.6341\n",
      "Epoch 14: Adjusting learning rate to 0.001000\n",
      "Epoch 14, Average Outcome Loss: 0.7068657604130831, Average Accuracy: 0.5, Test AUC: 0.6564, Test Accuracy: 0.6341\n",
      "Epoch 15: Adjusting learning rate to 0.001000\n",
      "Epoch 15, Average Outcome Loss: 0.7089647596532648, Average Accuracy: 0.5, Test AUC: 0.5769, Test Accuracy: 0.6341\n",
      "Epoch 16, Average Outcome Loss: 0.6938730424100702, Average Accuracy: 0.5, Test AUC: 0.4872, Test Accuracy: 0.3659\n",
      "Epoch 17, Average Outcome Loss: 0.6934188008308411, Average Accuracy: 0.5, Test AUC: 0.4641, Test Accuracy: 0.3659\n",
      "Epoch 18, Average Outcome Loss: 0.7010683525692333, Average Accuracy: 0.5, Test AUC: 0.4846, Test Accuracy: 0.3659\n",
      "Epoch 19, Average Outcome Loss: 0.6959454579786821, Average Accuracy: 0.5, Test AUC: 0.5077, Test Accuracy: 0.4390\n",
      "Epoch 20, Average Outcome Loss: 0.6881696744398638, Average Accuracy: 0.5795454545454546, Test AUC: 0.5923, Test Accuracy: 0.6341\n",
      "Epoch 21, Average Outcome Loss: 0.6888737028295343, Average Accuracy: 0.5, Test AUC: 0.6487, Test Accuracy: 0.6341\n",
      "Epoch 22: Adjusting learning rate to 0.001000\n",
      "Epoch 22, Average Outcome Loss: 0.6975131793455644, Average Accuracy: 0.5, Test AUC: 0.6410, Test Accuracy: 0.6341\n",
      "Epoch 23: Adjusting learning rate to 0.001000\n",
      "Epoch 23, Average Outcome Loss: 0.6921489130366932, Average Accuracy: 0.5, Test AUC: 0.5846, Test Accuracy: 0.5366\n",
      "Epoch 24, Average Outcome Loss: 0.6853553977879611, Average Accuracy: 0.5909090909090909, Test AUC: 0.5359, Test Accuracy: 0.3659\n",
      "Epoch 25, Average Outcome Loss: 0.6886932037093423, Average Accuracy: 0.5056818181818182, Test AUC: 0.5308, Test Accuracy: 0.3659\n",
      "Epoch 26, Average Outcome Loss: 0.691235135902058, Average Accuracy: 0.5, Test AUC: 0.5564, Test Accuracy: 0.4146\n",
      "Epoch 27, Average Outcome Loss: 0.6823538054119457, Average Accuracy: 0.5568181818181818, Test AUC: 0.5846, Test Accuracy: 0.6585\n",
      "Epoch 28, Average Outcome Loss: 0.6865330067547885, Average Accuracy: 0.5, Test AUC: 0.6462, Test Accuracy: 0.6341\n",
      "Epoch 29: Adjusting learning rate to 0.001000\n",
      "Epoch 29, Average Outcome Loss: 0.6867442510344766, Average Accuracy: 0.5, Test AUC: 0.6256, Test Accuracy: 0.6585\n",
      "Epoch 30: Adjusting learning rate to 0.001000\n",
      "Epoch 30, Average Outcome Loss: 0.683077178218148, Average Accuracy: 0.5113636363636364, Test AUC: 0.5872, Test Accuracy: 0.4390\n",
      "Epoch 31, Average Outcome Loss: 0.6825322508811951, Average Accuracy: 0.5681818181818182, Test AUC: 0.5846, Test Accuracy: 0.4146\n",
      "Epoch 32, Average Outcome Loss: 0.6823562708767977, Average Accuracy: 0.5738636363636364, Test AUC: 0.6359, Test Accuracy: 0.6098\n",
      "Epoch 33: Adjusting learning rate to 0.001000\n",
      "Epoch 33, Average Outcome Loss: 0.6720806902105158, Average Accuracy: 0.6534090909090909, Test AUC: 0.6795, Test Accuracy: 0.6585\n",
      "Epoch 34: Adjusting learning rate to 0.001000\n",
      "Epoch 34, Average Outcome Loss: 0.6788700439713218, Average Accuracy: 0.5397727272727273, Test AUC: 0.6718, Test Accuracy: 0.6585\n",
      "Epoch 35: Adjusting learning rate to 0.001000\n",
      "Epoch 35, Average Outcome Loss: 0.67583152922717, Average Accuracy: 0.5852272727272727, Test AUC: 0.6385, Test Accuracy: 0.5610\n",
      "Epoch 36: Adjusting learning rate to 0.001000\n",
      "Epoch 36, Average Outcome Loss: 0.6758573272011497, Average Accuracy: 0.6420454545454546, Test AUC: 0.6026, Test Accuracy: 0.4146\n",
      "Epoch 37: Adjusting learning rate to 0.001000\n",
      "Epoch 37, Average Outcome Loss: 0.674097093668851, Average Accuracy: 0.5568181818181818, Test AUC: 0.6436, Test Accuracy: 0.4634\n",
      "Epoch 38: Adjusting learning rate to 0.001000\n",
      "Epoch 38, Average Outcome Loss: 0.6621329188346863, Average Accuracy: 0.6534090909090909, Test AUC: 0.6897, Test Accuracy: 0.6585\n",
      "Epoch 39: Adjusting learning rate to 0.001000\n",
      "Epoch 39, Average Outcome Loss: 0.663816587491469, Average Accuracy: 0.7159090909090909, Test AUC: 0.7103, Test Accuracy: 0.6585\n",
      "Epoch 40: Adjusting learning rate to 0.000100\n",
      "Epoch 40, Average Outcome Loss: 0.6790378093719482, Average Accuracy: 0.5681818181818182, Test AUC: 0.7077, Test Accuracy: 0.6585\n",
      "Epoch 41: Adjusting learning rate to 0.000100\n",
      "Epoch 41, Average Outcome Loss: 0.6617889621041038, Average Accuracy: 0.5909090909090909, Test AUC: 0.7051, Test Accuracy: 0.6585\n",
      "Epoch 42: Adjusting learning rate to 0.000100\n",
      "Epoch 42, Average Outcome Loss: 0.6638883460651744, Average Accuracy: 0.6079545454545454, Test AUC: 0.7000, Test Accuracy: 0.6341\n",
      "Epoch 43: Adjusting learning rate to 0.000100\n",
      "Epoch 43, Average Outcome Loss: 0.6634721159934998, Average Accuracy: 0.6420454545454546, Test AUC: 0.6949, Test Accuracy: 0.5610\n",
      "Epoch 44: Adjusting learning rate to 0.001000\n",
      "Epoch 44, Average Outcome Loss: 0.6747697808525779, Average Accuracy: 0.6420454545454546, Test AUC: 0.6154, Test Accuracy: 0.3902\n",
      "Epoch 45: Adjusting learning rate to 0.001000\n",
      "Epoch 45, Average Outcome Loss: 0.679965382272547, Average Accuracy: 0.5113636363636364, Test AUC: 0.6487, Test Accuracy: 0.4146\n",
      "Epoch 46: Adjusting learning rate to 0.001000\n",
      "Epoch 46, Average Outcome Loss: 0.6660290414636786, Average Accuracy: 0.6022727272727273, Test AUC: 0.7282, Test Accuracy: 0.6585\n",
      "Epoch 47: Adjusting learning rate to 0.000100\n",
      "Epoch 47, Average Outcome Loss: 0.6638722582296892, Average Accuracy: 0.5511363636363636, Test AUC: 0.7308, Test Accuracy: 0.6585\n",
      "Epoch 48: Adjusting learning rate to 0.000100\n",
      "Epoch 48, Average Outcome Loss: 0.6693956472656943, Average Accuracy: 0.5738636363636364, Test AUC: 0.7282, Test Accuracy: 0.6585\n",
      "Epoch 49: Adjusting learning rate to 0.000100\n",
      "Epoch 49, Average Outcome Loss: 0.6563000949946317, Average Accuracy: 0.6193181818181818, Test AUC: 0.7231, Test Accuracy: 0.6585\n",
      "Epoch 50: Adjusting learning rate to 0.000100\n",
      "Epoch 50, Average Outcome Loss: 0.6750681942159479, Average Accuracy: 0.5454545454545454, Test AUC: 0.7179, Test Accuracy: 0.6829\n",
      "Epoch 51: Adjusting learning rate to 0.000100\n",
      "Epoch 51, Average Outcome Loss: 0.6653566035357389, Average Accuracy: 0.5965909090909091, Test AUC: 0.7179, Test Accuracy: 0.5610\n",
      "Epoch 52: Adjusting learning rate to 0.000100\n",
      "Epoch 52, Average Outcome Loss: 0.6521448601375927, Average Accuracy: 0.7272727272727273, Test AUC: 0.7103, Test Accuracy: 0.6585\n",
      "Epoch 53: Adjusting learning rate to 0.000100\n",
      "Epoch 53, Average Outcome Loss: 0.6570676294240084, Average Accuracy: 0.6363636363636364, Test AUC: 0.7026, Test Accuracy: 0.4878\n",
      "Epoch 54: Adjusting learning rate to 0.000100\n",
      "Epoch 54, Average Outcome Loss: 0.6456282680684869, Average Accuracy: 0.6818181818181818, Test AUC: 0.7000, Test Accuracy: 0.4634\n",
      "Epoch 55: Adjusting learning rate to 0.000100\n",
      "Epoch 55, Average Outcome Loss: 0.6692838072776794, Average Accuracy: 0.6136363636363636, Test AUC: 0.7000, Test Accuracy: 0.4634\n",
      "Epoch 56: Adjusting learning rate to 0.000100\n",
      "Epoch 56, Average Outcome Loss: 0.6531547253782098, Average Accuracy: 0.6363636363636364, Test AUC: 0.7026, Test Accuracy: 0.4634\n",
      "Epoch 57: Adjusting learning rate to 0.000100\n",
      "Epoch 57, Average Outcome Loss: 0.6645640676671808, Average Accuracy: 0.6534090909090909, Test AUC: 0.7103, Test Accuracy: 0.4878\n",
      "Epoch 58: Adjusting learning rate to 0.000100\n",
      "Epoch 58, Average Outcome Loss: 0.6601692221381448, Average Accuracy: 0.6306818181818182, Test AUC: 0.7154, Test Accuracy: 0.5854\n",
      "Epoch 59: Adjusting learning rate to 0.000100\n",
      "Epoch 59, Average Outcome Loss: 0.6701914072036743, Average Accuracy: 0.6022727272727273, Test AUC: 0.7205, Test Accuracy: 0.7073\n",
      "Epoch 60: Adjusting learning rate to 0.000100\n",
      "Epoch 60, Average Outcome Loss: 0.6632639169692993, Average Accuracy: 0.6136363636363636, Test AUC: 0.7231, Test Accuracy: 0.5854\n",
      "Epoch 61: Adjusting learning rate to 0.000100\n",
      "Epoch 61, Average Outcome Loss: 0.6555810028856451, Average Accuracy: 0.6988636363636364, Test AUC: 0.7231, Test Accuracy: 0.5854\n",
      "Epoch 62: Adjusting learning rate to 0.000100\n",
      "Epoch 62, Average Outcome Loss: 0.658197365023873, Average Accuracy: 0.6875, Test AUC: 0.7231, Test Accuracy: 0.6341\n",
      "Epoch 63: Adjusting learning rate to 0.000100\n",
      "Epoch 63, Average Outcome Loss: 0.6652818538925864, Average Accuracy: 0.6875, Test AUC: 0.7256, Test Accuracy: 0.6341\n",
      "Epoch 64: Adjusting learning rate to 0.000100\n",
      "Epoch 64, Average Outcome Loss: 0.6620641350746155, Average Accuracy: 0.6079545454545454, Test AUC: 0.7256, Test Accuracy: 0.6341\n",
      "Epoch 65: Adjusting learning rate to 0.000100\n",
      "Epoch 65, Average Outcome Loss: 0.6566785573959351, Average Accuracy: 0.6306818181818182, Test AUC: 0.7231, Test Accuracy: 0.6098\n",
      "Epoch 66: Adjusting learning rate to 0.000100\n",
      "Epoch 66, Average Outcome Loss: 0.6701398058371111, Average Accuracy: 0.6477272727272727, Test AUC: 0.7256, Test Accuracy: 0.5610\n",
      "Epoch 67: Adjusting learning rate to 0.000100\n",
      "Epoch 67, Average Outcome Loss: 0.6522507667541504, Average Accuracy: 0.6647727272727273, Test AUC: 0.7256, Test Accuracy: 0.6341\n",
      "Epoch 68: Adjusting learning rate to 0.000100\n",
      "Epoch 68, Average Outcome Loss: 0.6505962989547036, Average Accuracy: 0.7045454545454546, Test AUC: 0.7256, Test Accuracy: 0.6829\n",
      "Epoch 69: Adjusting learning rate to 0.000100\n",
      "Epoch 69, Average Outcome Loss: 0.6414597847244956, Average Accuracy: 0.7159090909090909, Test AUC: 0.7205, Test Accuracy: 0.7073\n",
      "Epoch 70: Adjusting learning rate to 0.000100\n",
      "Epoch 70, Average Outcome Loss: 0.6543152765794233, Average Accuracy: 0.6931818181818182, Test AUC: 0.7205, Test Accuracy: 0.6585\n",
      "Epoch 71: Adjusting learning rate to 0.000100\n",
      "Epoch 71, Average Outcome Loss: 0.6557162729176608, Average Accuracy: 0.6477272727272727, Test AUC: 0.7231, Test Accuracy: 0.6585\n",
      "Epoch 72: Adjusting learning rate to 0.000100\n",
      "Epoch 72, Average Outcome Loss: 0.637918082150546, Average Accuracy: 0.7102272727272727, Test AUC: 0.7256, Test Accuracy: 0.7073\n",
      "Epoch 73: Adjusting learning rate to 0.000100\n",
      "Epoch 73, Average Outcome Loss: 0.6617606553164396, Average Accuracy: 0.6988636363636364, Test AUC: 0.7256, Test Accuracy: 0.7073\n",
      "Epoch 74: Adjusting learning rate to 0.000100\n",
      "Epoch 74, Average Outcome Loss: 0.6491443027149547, Average Accuracy: 0.6988636363636364, Test AUC: 0.7282, Test Accuracy: 0.7073\n",
      "Epoch 75: Adjusting learning rate to 0.000100\n",
      "Epoch 75, Average Outcome Loss: 0.6497626900672913, Average Accuracy: 0.7102272727272727, Test AUC: 0.7308, Test Accuracy: 0.6341\n",
      "Epoch 76: Adjusting learning rate to 0.000100\n",
      "Epoch 76, Average Outcome Loss: 0.6430553143674677, Average Accuracy: 0.7386363636363636, Test AUC: 0.7308, Test Accuracy: 0.6098\n",
      "Epoch 77: Adjusting learning rate to 0.000100\n",
      "Epoch 77, Average Outcome Loss: 0.6540510329333219, Average Accuracy: 0.7102272727272727, Test AUC: 0.7282, Test Accuracy: 0.5854\n",
      "Epoch 78: Adjusting learning rate to 0.000100\n",
      "Epoch 78, Average Outcome Loss: 0.6576851010322571, Average Accuracy: 0.6647727272727273, Test AUC: 0.7282, Test Accuracy: 0.5854\n",
      "Epoch 79: Adjusting learning rate to 0.000100\n",
      "Epoch 79, Average Outcome Loss: 0.6435416991060431, Average Accuracy: 0.7045454545454546, Test AUC: 0.7282, Test Accuracy: 0.5854\n",
      "Epoch 80: Adjusting learning rate to 0.000100\n",
      "Epoch 80, Average Outcome Loss: 0.6503640413284302, Average Accuracy: 0.6988636363636364, Test AUC: 0.7282, Test Accuracy: 0.5854\n",
      "Epoch 81: Adjusting learning rate to 0.000100\n",
      "Epoch 81, Average Outcome Loss: 0.6587851697748358, Average Accuracy: 0.6647727272727273, Test AUC: 0.7282, Test Accuracy: 0.6098\n",
      "Epoch 82: Adjusting learning rate to 0.000100\n",
      "Epoch 82, Average Outcome Loss: 0.648855366490104, Average Accuracy: 0.7443181818181818, Test AUC: 0.7308, Test Accuracy: 0.6098\n",
      "Epoch 83: Adjusting learning rate to 0.000100\n",
      "Epoch 83, Average Outcome Loss: 0.6562669819051569, Average Accuracy: 0.6363636363636364, Test AUC: 0.7282, Test Accuracy: 0.6341\n",
      "Epoch 84: Adjusting learning rate to 0.000100\n",
      "Epoch 84, Average Outcome Loss: 0.6449668136509982, Average Accuracy: 0.6818181818181818, Test AUC: 0.7282, Test Accuracy: 0.6341\n",
      "Epoch 85: Adjusting learning rate to 0.000100\n",
      "Epoch 85, Average Outcome Loss: 0.6533648588440635, Average Accuracy: 0.6931818181818182, Test AUC: 0.7282, Test Accuracy: 0.6829\n",
      "Epoch 86: Adjusting learning rate to 0.000100\n",
      "Epoch 86, Average Outcome Loss: 0.6383721232414246, Average Accuracy: 0.75, Test AUC: 0.7282, Test Accuracy: 0.7073\n",
      "Epoch 87: Adjusting learning rate to 0.000100\n",
      "Epoch 87, Average Outcome Loss: 0.6564394885843451, Average Accuracy: 0.6988636363636364, Test AUC: 0.7231, Test Accuracy: 0.7073\n",
      "Epoch 88: Adjusting learning rate to 0.000100\n",
      "Epoch 88, Average Outcome Loss: 0.6445718949491327, Average Accuracy: 0.6534090909090909, Test AUC: 0.7256, Test Accuracy: 0.6098\n",
      "Epoch 89: Adjusting learning rate to 0.000100\n",
      "Epoch 89, Average Outcome Loss: 0.6414247371933677, Average Accuracy: 0.6761363636363636, Test AUC: 0.7256, Test Accuracy: 0.6098\n",
      "Epoch 90: Adjusting learning rate to 0.000100\n",
      "Epoch 90, Average Outcome Loss: 0.6545790433883667, Average Accuracy: 0.6704545454545454, Test AUC: 0.7256, Test Accuracy: 0.6098\n",
      "Epoch 91: Adjusting learning rate to 0.000100\n",
      "Epoch 91, Average Outcome Loss: 0.6493275761604309, Average Accuracy: 0.6420454545454546, Test AUC: 0.7282, Test Accuracy: 0.6098\n",
      "Epoch 92: Adjusting learning rate to 0.000100\n",
      "Epoch 92, Average Outcome Loss: 0.6490734653039412, Average Accuracy: 0.6704545454545454, Test AUC: 0.7282, Test Accuracy: 0.6341\n",
      "Epoch 93: Adjusting learning rate to 0.000100\n",
      "Epoch 93, Average Outcome Loss: 0.6375319253314625, Average Accuracy: 0.6988636363636364, Test AUC: 0.7282, Test Accuracy: 0.6829\n",
      "Epoch 94: Adjusting learning rate to 0.000100\n",
      "Epoch 94, Average Outcome Loss: 0.6633595986799761, Average Accuracy: 0.6193181818181818, Test AUC: 0.7282, Test Accuracy: 0.6829\n",
      "Epoch 95: Adjusting learning rate to 0.000100\n",
      "Epoch 95, Average Outcome Loss: 0.6270306002009999, Average Accuracy: 0.75, Test AUC: 0.7282, Test Accuracy: 0.6829\n",
      "Epoch 96: Adjusting learning rate to 0.000100\n",
      "Epoch 96, Average Outcome Loss: 0.6351668726314198, Average Accuracy: 0.7159090909090909, Test AUC: 0.7282, Test Accuracy: 0.6829\n",
      "Epoch 97: Adjusting learning rate to 0.000100\n",
      "Epoch 97, Average Outcome Loss: 0.635901315645738, Average Accuracy: 0.6931818181818182, Test AUC: 0.7282, Test Accuracy: 0.7073\n",
      "Epoch 98: Adjusting learning rate to 0.000100\n",
      "Epoch 98, Average Outcome Loss: 0.6425405415621671, Average Accuracy: 0.6818181818181818, Test AUC: 0.7282, Test Accuracy: 0.6829\n",
      "Epoch 99: Adjusting learning rate to 0.000100\n",
      "Epoch 99, Average Outcome Loss: 0.6264093626629222, Average Accuracy: 0.7443181818181818, Test AUC: 0.7256, Test Accuracy: 0.6829\n",
      "Epoch 100: Adjusting learning rate to 0.000100\n",
      "Epoch 100, Average Outcome Loss: 0.6229179068045183, Average Accuracy: 0.7727272727272727, Test AUC: 0.7256, Test Accuracy: 0.6585\n",
      "Epoch 101: Adjusting learning rate to 0.000100\n",
      "Epoch 101, Average Outcome Loss: 0.6260280771688982, Average Accuracy: 0.7386363636363636, Test AUC: 0.7256, Test Accuracy: 0.6098\n",
      "Epoch 102: Adjusting learning rate to 0.000100\n",
      "Epoch 102, Average Outcome Loss: 0.6221070885658264, Average Accuracy: 0.7613636363636364, Test AUC: 0.7282, Test Accuracy: 0.6341\n",
      "Epoch 103: Adjusting learning rate to 0.000100\n",
      "Epoch 103, Average Outcome Loss: 0.6315265081145547, Average Accuracy: 0.7329545454545454, Test AUC: 0.7282, Test Accuracy: 0.6341\n",
      "Epoch 104: Adjusting learning rate to 0.000100\n",
      "Epoch 104, Average Outcome Loss: 0.6319606520912864, Average Accuracy: 0.7215909090909091, Test AUC: 0.7282, Test Accuracy: 0.6341\n",
      "Epoch 105: Adjusting learning rate to 0.000100\n",
      "Epoch 105, Average Outcome Loss: 0.6375350139357827, Average Accuracy: 0.75, Test AUC: 0.7282, Test Accuracy: 0.6341\n",
      "Epoch 106: Adjusting learning rate to 0.000100\n",
      "Epoch 106, Average Outcome Loss: 0.6463577855717052, Average Accuracy: 0.6931818181818182, Test AUC: 0.7308, Test Accuracy: 0.6585\n",
      "Epoch 107: Adjusting learning rate to 0.000100\n",
      "Epoch 107, Average Outcome Loss: 0.6305851502852007, Average Accuracy: 0.6988636363636364, Test AUC: 0.7308, Test Accuracy: 0.6829\n",
      "Epoch 108: Adjusting learning rate to 0.000100\n",
      "Epoch 108, Average Outcome Loss: 0.6464011344042692, Average Accuracy: 0.6477272727272727, Test AUC: 0.7308, Test Accuracy: 0.7073\n",
      "Epoch 109: Adjusting learning rate to 0.000100\n",
      "Epoch 109, Average Outcome Loss: 0.6233838850801642, Average Accuracy: 0.7159090909090909, Test AUC: 0.7308, Test Accuracy: 0.6829\n",
      "Epoch 110: Adjusting learning rate to 0.000100\n",
      "Epoch 110, Average Outcome Loss: 0.6381202123381875, Average Accuracy: 0.6761363636363636, Test AUC: 0.7359, Test Accuracy: 0.7073\n",
      "Epoch 111: Adjusting learning rate to 0.000100\n",
      "Epoch 111, Average Outcome Loss: 0.6429082534529946, Average Accuracy: 0.6988636363636364, Test AUC: 0.7436, Test Accuracy: 0.6341\n",
      "Epoch 112: Adjusting learning rate to 0.000100\n",
      "Epoch 112, Average Outcome Loss: 0.6180058934471824, Average Accuracy: 0.7670454545454546, Test AUC: 0.7462, Test Accuracy: 0.6341\n",
      "Epoch 113: Adjusting learning rate to 0.000100\n",
      "Epoch 113, Average Outcome Loss: 0.6191013780507174, Average Accuracy: 0.75, Test AUC: 0.7462, Test Accuracy: 0.6341\n",
      "Epoch 114: Adjusting learning rate to 0.000100\n",
      "Epoch 114, Average Outcome Loss: 0.6304068998856978, Average Accuracy: 0.6931818181818182, Test AUC: 0.7462, Test Accuracy: 0.6341\n",
      "Epoch 115: Adjusting learning rate to 0.000100\n",
      "Epoch 115, Average Outcome Loss: 0.6339908946644176, Average Accuracy: 0.6818181818181818, Test AUC: 0.7487, Test Accuracy: 0.6341\n",
      "Epoch 116: Adjusting learning rate to 0.000100\n",
      "Epoch 116, Average Outcome Loss: 0.6290502331473611, Average Accuracy: 0.6875, Test AUC: 0.7487, Test Accuracy: 0.6341\n",
      "Epoch 117: Adjusting learning rate to 0.000100\n",
      "Epoch 117, Average Outcome Loss: 0.6316716616803949, Average Accuracy: 0.7329545454545454, Test AUC: 0.7487, Test Accuracy: 0.6341\n",
      "Epoch 118: Adjusting learning rate to 0.000100\n",
      "Epoch 118, Average Outcome Loss: 0.6296472386880354, Average Accuracy: 0.6931818181818182, Test AUC: 0.7487, Test Accuracy: 0.6585\n",
      "Epoch 119: Adjusting learning rate to 0.000100\n",
      "Epoch 119, Average Outcome Loss: 0.6098164103247903, Average Accuracy: 0.7215909090909091, Test AUC: 0.7487, Test Accuracy: 0.6341\n",
      "Epoch 120: Adjusting learning rate to 0.000100\n",
      "Epoch 120, Average Outcome Loss: 0.6402712193402377, Average Accuracy: 0.6590909090909091, Test AUC: 0.7487, Test Accuracy: 0.6585\n",
      "Epoch 121: Adjusting learning rate to 0.000100\n",
      "Epoch 121, Average Outcome Loss: 0.6275293556126681, Average Accuracy: 0.6306818181818182, Test AUC: 0.7487, Test Accuracy: 0.6829\n",
      "Epoch 122: Adjusting learning rate to 0.000100\n",
      "Epoch 122, Average Outcome Loss: 0.6267101818864996, Average Accuracy: 0.6761363636363636, Test AUC: 0.7487, Test Accuracy: 0.6829\n",
      "Epoch 123: Adjusting learning rate to 0.000100\n",
      "Epoch 123, Average Outcome Loss: 0.6488448760726235, Average Accuracy: 0.6136363636363636, Test AUC: 0.7487, Test Accuracy: 0.6341\n",
      "Epoch 124: Adjusting learning rate to 0.000100\n",
      "Epoch 124, Average Outcome Loss: 0.6414062922651117, Average Accuracy: 0.6647727272727273, Test AUC: 0.7513, Test Accuracy: 0.6585\n",
      "Epoch 125: Adjusting learning rate to 0.000100\n",
      "Epoch 125, Average Outcome Loss: 0.6244585405696522, Average Accuracy: 0.6818181818181818, Test AUC: 0.7538, Test Accuracy: 0.6585\n",
      "Epoch 126: Adjusting learning rate to 0.000100\n",
      "Epoch 126, Average Outcome Loss: 0.6470840519124811, Average Accuracy: 0.6931818181818182, Test AUC: 0.7513, Test Accuracy: 0.6341\n",
      "Epoch 127: Adjusting learning rate to 0.000100\n",
      "Epoch 127, Average Outcome Loss: 0.6255943233316595, Average Accuracy: 0.6931818181818182, Test AUC: 0.7538, Test Accuracy: 0.6341\n",
      "Epoch 128: Adjusting learning rate to 0.000100\n",
      "Epoch 128, Average Outcome Loss: 0.6279924349351362, Average Accuracy: 0.7556818181818182, Test AUC: 0.7564, Test Accuracy: 0.6585\n",
      "Epoch 129: Adjusting learning rate to 0.000100\n",
      "Epoch 129, Average Outcome Loss: 0.6336993033235724, Average Accuracy: 0.6534090909090909, Test AUC: 0.7564, Test Accuracy: 0.6585\n",
      "Epoch 130: Adjusting learning rate to 0.000100\n",
      "Epoch 130, Average Outcome Loss: 0.6304442069747231, Average Accuracy: 0.6931818181818182, Test AUC: 0.7564, Test Accuracy: 0.7073\n",
      "Epoch 131: Adjusting learning rate to 0.000100\n",
      "Epoch 131, Average Outcome Loss: 0.6400943344289606, Average Accuracy: 0.6193181818181818, Test AUC: 0.7564, Test Accuracy: 0.6585\n",
      "Epoch 132: Adjusting learning rate to 0.000100\n",
      "Epoch 132, Average Outcome Loss: 0.6376012726263567, Average Accuracy: 0.6704545454545454, Test AUC: 0.7564, Test Accuracy: 0.6585\n",
      "Epoch 133: Adjusting learning rate to 0.000100\n",
      "Epoch 133, Average Outcome Loss: 0.6409022537144747, Average Accuracy: 0.6647727272727273, Test AUC: 0.7590, Test Accuracy: 0.6341\n",
      "Epoch 134: Adjusting learning rate to 0.000100\n",
      "Epoch 134, Average Outcome Loss: 0.6204535148360513, Average Accuracy: 0.7102272727272727, Test AUC: 0.7590, Test Accuracy: 0.6829\n",
      "Epoch 135: Adjusting learning rate to 0.000100\n",
      "Epoch 135, Average Outcome Loss: 0.6237190690907565, Average Accuracy: 0.7159090909090909, Test AUC: 0.7615, Test Accuracy: 0.6829\n",
      "Epoch 136: Adjusting learning rate to 0.000100\n",
      "Epoch 136, Average Outcome Loss: 0.6132689118385315, Average Accuracy: 0.7386363636363636, Test AUC: 0.7615, Test Accuracy: 0.6829\n",
      "Epoch 137: Adjusting learning rate to 0.000100\n",
      "Epoch 137, Average Outcome Loss: 0.6320108554579995, Average Accuracy: 0.6988636363636364, Test AUC: 0.7615, Test Accuracy: 0.6585\n",
      "Epoch 138: Adjusting learning rate to 0.000100\n",
      "Epoch 138, Average Outcome Loss: 0.6297857111150568, Average Accuracy: 0.7670454545454546, Test AUC: 0.7564, Test Accuracy: 0.6829\n",
      "Epoch 139: Adjusting learning rate to 0.000100\n",
      "Epoch 139, Average Outcome Loss: 0.6200059652328491, Average Accuracy: 0.7159090909090909, Test AUC: 0.7564, Test Accuracy: 0.6341\n",
      "Epoch 140: Adjusting learning rate to 0.000100\n",
      "Epoch 140, Average Outcome Loss: 0.6382265741174872, Average Accuracy: 0.5965909090909091, Test AUC: 0.7564, Test Accuracy: 0.6341\n",
      "Epoch 141: Adjusting learning rate to 0.000100\n",
      "Epoch 141, Average Outcome Loss: 0.6223913647911765, Average Accuracy: 0.6363636363636364, Test AUC: 0.7590, Test Accuracy: 0.6341\n",
      "Epoch 142: Adjusting learning rate to 0.000100\n",
      "Epoch 142, Average Outcome Loss: 0.6279106952927329, Average Accuracy: 0.6647727272727273, Test AUC: 0.7615, Test Accuracy: 0.6585\n",
      "Epoch 143: Adjusting learning rate to 0.000100\n",
      "Epoch 143, Average Outcome Loss: 0.6231289343400435, Average Accuracy: 0.6477272727272727, Test AUC: 0.7564, Test Accuracy: 0.6585\n",
      "Epoch 144: Adjusting learning rate to 0.000100\n",
      "Epoch 144, Average Outcome Loss: 0.6232090592384338, Average Accuracy: 0.6818181818181818, Test AUC: 0.7692, Test Accuracy: 0.6829\n",
      "Epoch 145: Adjusting learning rate to 0.000100\n",
      "Epoch 145, Average Outcome Loss: 0.6257992495190013, Average Accuracy: 0.6875, Test AUC: 0.7692, Test Accuracy: 0.6829\n",
      "Epoch 146: Adjusting learning rate to 0.000100\n",
      "Epoch 146, Average Outcome Loss: 0.618561408736489, Average Accuracy: 0.7272727272727273, Test AUC: 0.7615, Test Accuracy: 0.6829\n",
      "Epoch 147: Adjusting learning rate to 0.000100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_256824/2849908393.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust labels shape for binary_crossentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Perform the training step and collect gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0moutcome_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Accumulate gradients and losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moutcome_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_256824/2849908393.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(data, outcome_labels)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutcome_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutcome_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutcome_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutcome_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutcome_loss\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Average over the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Compute gradients for the outcome classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mclassifier_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutcome_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutcome_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Calculate accuracy for the outcome classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mpredicted_outcome_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutcome_predictions\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Threshold at 0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1108\u001b[0m               output_gradients))\n\u001b[1;32m   1109\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[1;32m   1110\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise ValueError(\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient_tape/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[0mgy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmust_reduce_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0mgy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m     \u001b[0mgy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m   \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m   \"\"\"\n\u001b[0;32m--> 199\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   8539\u001b[0m         _ctx, \"Reshape\", name, tensor, shape)\n\u001b[1;32m   8540\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8541\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8542\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8543\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8544\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8545\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8546\u001b[0m       return reshape_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_accuracy_list = []\n",
    "train_accuracy_list = []\n",
    "test_auc_list = []\n",
    "\n",
    "num_outcomes = len(np.unique(y_test_outcome))\n",
    "num_conditions = len(np.unique(unique_combinations_array))\n",
    "\n",
    "# Define the training step for only the outcome classifier\n",
    "def train_step(data, outcome_labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass through the outcome classifier\n",
    "        outcome_predictions = outcome_classifier(data)\n",
    "\n",
    "        # Compute the biological discriminator loss\n",
    "        outcome_loss = tf.keras.losses.binary_crossentropy(outcome_labels, outcome_predictions)\n",
    "        outcome_loss = tf.reduce_mean(outcome_loss)  # Average over the batch\n",
    "\n",
    "    # Compute gradients for the outcome classifier\n",
    "    classifier_grads = tape.gradient(outcome_loss, outcome_classifier.trainable_variables)\n",
    "    \n",
    "    # Calculate accuracy for the outcome classifier\n",
    "    predicted_outcome_labels = tf.cast(outcome_predictions > 0.5, tf.float32)  # Threshold at 0.5\n",
    "    outcome_labels_float = tf.cast(outcome_labels, tf.float32)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_outcome_labels, outcome_labels_float), tf.float32))\n",
    "\n",
    "    return outcome_loss, accuracy, classifier_grads\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0  # To accumulate losses\n",
    "    total_accuracy = 0.0  # To accumulate accuracy\n",
    "    accumulated_grads = [tf.zeros_like(var) for var in outcome_classifier.trainable_variables]  # Initialize gradient accumulator\n",
    "\n",
    "    # Split train data randomly, holding out a portion for generalization\n",
    "    X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(X_train, y_train, test_size=holdout_size, random_state=None)\n",
    "    y_train_comb_temp = y_train_temp['combination_tech_outcome']\n",
    "    y_train_temp = y_train_temp['numerical_categories_outcome']\n",
    "    \n",
    "    # Mini-batch training loop\n",
    "    for step in range(num_steps_per_epoch):\n",
    "        # Balance batches if necessary\n",
    "        batch_indices = []\n",
    "        if balance:\n",
    "            for condition in range(num_conditions):\n",
    "                condition_indices = np.where(y_train_comb_temp == condition)[0]\n",
    "                condition_batch_indices = np.random.choice(condition_indices, size=batch_size // num_conditions, replace=True)\n",
    "                batch_indices.append(condition_batch_indices)\n",
    "        else:\n",
    "            all_indices = np.arange(len(X_train_temp))\n",
    "            random_indices = np.random.choice(all_indices, size=batch_size, replace=True)\n",
    "            batch_indices.append(random_indices)\n",
    "        X_batch = X_train_temp[np.concatenate(batch_indices)]\n",
    "        #y_batch = y_train_temp[np.concatenate(batch_indices)]\n",
    "        y_batch = y_train_temp.iloc[np.concatenate(batch_indices)]\n",
    "        y_batch = tf.expand_dims(y_batch, axis=-1)  # Adjust labels shape for binary_crossentropy\n",
    "                \n",
    "        # Perform the training step and collect gradients\n",
    "        outcome_loss, accuracy, classifier_grads = train_step(X_batch, y_batch)\n",
    "        \n",
    "        # Accumulate gradients and losses\n",
    "        total_loss += outcome_loss.numpy()\n",
    "        total_accuracy += accuracy.numpy()\n",
    "        accumulated_grads = [acc_grad + grad for acc_grad, grad in zip(accumulated_grads, classifier_grads)]\n",
    "\n",
    "    # Average the accumulated gradients\n",
    "    averaged_grads = [grad / num_steps_per_epoch for grad in accumulated_grads]\n",
    "\n",
    "    # Apply averaged gradients to update model weights\n",
    "    optimizer.apply_gradients(zip(averaged_grads, outcome_classifier.trainable_variables))\n",
    "\n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    avg_loss = total_loss / num_steps_per_epoch\n",
    "    avg_accuracy = total_accuracy / num_steps_per_epoch\n",
    "\n",
    "    # # Print average accuracy at the end of each epoch and calculate the accuracy for the test set\n",
    "    # if epoch % report_frequency == 0:\n",
    "    #     # Evaluate on test data\n",
    "    #     outcome_predictions = outcome_classifier(X_test)\n",
    "    #     predicted_outcome_labels = tf.cast(outcome_predictions > 0.5, tf.float32)  # Threshold at 0.5\n",
    "    #     outcome_labels = tf.expand_dims(y_test_outcome, axis=-1)  # Reshape to match logits shape\n",
    "    #     outcome_labels_float = tf.cast(outcome_labels, tf.float32)\n",
    "\n",
    "    #     # Calculate accuracy\n",
    "    #     test_accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_outcome_labels, outcome_labels_float), tf.float32))\n",
    "        \n",
    "    #     # Store and print metrics\n",
    "    #     train_accuracy_list.append(avg_accuracy)\n",
    "    #     test_accuracy_list.append(test_accuracy)\n",
    "    #     print(f'Epoch {epoch}, Average Outcome Loss: {avg_loss}, Average Accuracy: {avg_accuracy}, Test Accuracy: {test_accuracy}')\n",
    "\n",
    "    #     # Early stopping condition for accuracy\n",
    "    #     if test_accuracy > accuracy_threshold:\n",
    "    #         print('Early stopping: test set performance high enough')\n",
    "    #         break\n",
    "\n",
    "    # Print average accuracy and calculate the ROC AUC for the test set\n",
    "    if epoch % report_frequency == 0:\n",
    "\n",
    "        #adjust the learning rate depending on test set performance\n",
    "        # adjust_learning_rate_by_auc(epoch, outcome_classifier, X_test_temp, y_test_temp['numerical_categories_outcome'], lr_dict, auc_thresholds)\n",
    "        \n",
    "        # Evaluate on test data\n",
    "        outcome_predictions = outcome_classifier(X_test)\n",
    "        outcome_labels = tf.expand_dims(y_test_outcome, axis=-1)  # Reshape to match logits shape\n",
    "        outcome_labels_float = tf.cast(outcome_labels, tf.float32)\n",
    "    \n",
    "        # Calculate AUC\n",
    "        outcome_predictions_np = outcome_predictions.numpy().flatten()  # Convert predictions to numpy for roc_auc_score\n",
    "        outcome_labels_np = outcome_labels_float.numpy().flatten()  # Convert labels to numpy for roc_auc_score\n",
    "        test_auc = roc_auc_score(outcome_labels_np, outcome_predictions_np)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted_outcome_labels = tf.cast(outcome_predictions > 0.5, tf.float32)  # Threshold at 0.5\n",
    "        outcome_labels_float = tf.cast(outcome_labels, tf.float32)\n",
    "        test_accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_outcome_labels, outcome_labels_float), tf.float32))\n",
    "    \n",
    "        # Store and print metrics\n",
    "        train_accuracy_list.append(avg_accuracy)\n",
    "        test_auc_list.append(test_auc)\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "        print(f'Epoch {epoch}, Average Outcome Loss: {avg_loss}, Average Accuracy: {avg_accuracy}, Test AUC: {test_auc:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "        adjust_learning_rate_by_auc(epoch, outcome_classifier, X_test_temp, y_test_temp['numerical_categories_outcome'], lr_dict, auc_thresholds, test_auc)\n",
    "    \n",
    "        # Early stopping condition for AUC (if needed)\n",
    "        if test_auc > auc_threshold:  # Define auc_threshold as desired\n",
    "            print('Early stopping triggered based on AUC')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4a2eb6-9cc3-485a-b0eb-96ece867a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(test_auc_list))\n",
    "print(max(test_accuracy_list))\n",
    "print(max(train_accuracy_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c2612-7184-44a1-9acd-9005348dfb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, plot the metrics\n",
    "\n",
    "frequency_counts = pd.Series(y_test_outcome).value_counts()\n",
    "test_chance_level = frequency_counts[0]/len(y_test_outcome)\n",
    "\n",
    "frequency_counts = pd.Series(y_train_outcome).value_counts()\n",
    "train_chance_level = frequency_counts[0]/len(y_train_outcome)\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12, 6))\n",
    "\n",
    "x_values = np.arange(1, len(train_accuracy_list) + 1) * report_frequency\n",
    "\n",
    "# Plot train accuracy\n",
    "axs[0].plot(x_values, train_accuracy_list, label='Training Accuracy', color='blue')\n",
    "axs[0].axhline(train_chance_level, color='black',linestyle ='--')\n",
    "axs[0].set_title('Training set accuracy over epochs')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Training Accuracy')\n",
    "axs[0].grid()\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot test accuracay\n",
    "axs[1].plot(x_values, test_accuracy_list, label='Test Accuracy', color='orange')\n",
    "axs[1].axhline(test_chance_level, color='black',linestyle ='--')\n",
    "axs[1].set_title('Test set accuracy over epochs')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Test Accuracy')\n",
    "axs[1].grid()\n",
    "axs[1].legend()\n",
    "\n",
    "# Plot test accuracay\n",
    "axs[2].plot(x_values, test_auc_list, label='Test AUC', color='orange')\n",
    "# axs[2].axhline(0.5, color='black',linestyle ='--')\n",
    "axs[2].set_title('Test set AUC over epochs')\n",
    "axs[2].set_xlabel('Epoch')\n",
    "axs[2].set_ylabel('Test AUC')\n",
    "axs[2].grid()\n",
    "axs[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4e4636-c91e-4a73-be93-40b6cc7cc28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After loop: Calculate ROC and AUC using continuous probabilities\n",
    "fpr, tpr, thresholds = roc_curve(y_test_outcome, outcome_predictions)  # Use probabilities, not thresholded labels\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line for random guessing\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for ANN')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8e9960-942a-448c-9444-2a7fbd4c9d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# clf.fit(X_train, y_train_outcome)\n",
    "\n",
    "# y_pred_prob = clf.predict_proba(X_test)[:, 1]  # Probabilities for the positive class (class 1)\n",
    "\n",
    "# ROC curve for neural netowrk\n",
    "\n",
    "# fpr, tpr, thresholds = roc_curve(y_test_outcome, predicted_outcome_labels)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line for random guessing\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic (ROC) Curve for ANN')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e84149f-2ef8-4123-bdb5-c23db0f40174",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train_outcome)\n",
    "\n",
    "y_pred_prob = clf.predict_proba(X_test)[:, 1]  # Probabilities for the positive class (class 1)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test_outcome, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line for random guessing\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for random forest')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85afc79c-18af-41da-a9eb-72e6583191cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467ed253-d193-4b73-a1f4-9f659406f4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137e9c6-8f01-4e7b-bc2f-bb08b500f107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa81e2-5e22-4549-88c0-a52e1050e3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
