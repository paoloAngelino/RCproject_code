{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf24bb4-d77f-476d-9820-3150a572b0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "2024-11-12 13:08:16.563285: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import rpy2.robjects as ro\n",
    "import rpy2.robjects.packages as rpackages\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from keras.callbacks import EarlyStopping\n",
    "import seaborn as sb\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import pearsonr\n",
    "import time\n",
    "from keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import random\n",
    "import rpy2.robjects as ro\n",
    "from rpy2 import robjects\n",
    "from rpy2.robjects import r\n",
    "from rpy2.robjects import pandas2ri\n",
    "import pandas as pd\n",
    "\n",
    "# adata = sc.read('/tmp/work/RCproject_code/sce_export.h5ad')\n",
    "epoch_count = 0\n",
    "\n",
    "# Enable automatic conversion between R objects and pandas DataFrames\n",
    "pandas2ri.activate()\n",
    "\n",
    "#exclude partial\n",
    "# adata = adata[~adata.obs['Response'].isin(['partial']), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb21723-a836-4597-927b-04267ec8d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_size = 0.1\n",
    "dropout_rate = 0.3 #0.1\n",
    "balance = True\n",
    "l2_reg = 0.2\n",
    "batch_size = 16  #determines how many samples are processed per batch, each epoch will process multiple batches\n",
    "# learning_rate = 0.00001\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100000\n",
    "report_frequency = 1\n",
    "accuracy_threshold = 0.9  #currently not used\n",
    "auc_threshold = 0.92\n",
    "clipnorm = 2.0\n",
    "simplifly_categories = True\n",
    "holdout_size = 0.5\n",
    "\n",
    "use_gene_list = True\n",
    "current_gene_list = 'de_intersect_plus_bulk_genes'#'sig9_0.05'\n",
    "\n",
    "PCA_reduce = False\n",
    "n_comp_PCA = 16\n",
    "\n",
    "multiplier = 3 #0.91 performance with 3\n",
    "\n",
    "#dynamic learning rate parameters\n",
    "\n",
    "lr_dict = {\n",
    "    0.6:  0.005,\n",
    "    0.7:  0.001,\n",
    "    0.8:  0.0005,\n",
    "    0.85: 0.0001,\n",
    "    0.88: 0.00005,\n",
    "    0.89: 0.00001,\n",
    "    0.9:  0.000005,\n",
    "    0.91: 0.000001,\n",
    "    0.92: 0.0000005\n",
    "}\n",
    "\n",
    "auc_thresholds = [0.6, 0.7, 0.8, 0.85, 0.88,0.89,0.90,0.91,0.92]  # AUC values at which the learning rate should be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f6015e9-d6a4-4680-b92d-3303d6f13915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['meta', 'DE4', 'DEall', 'meta_intersect_unionDE', 'de_intersect', 'de_intersect_plus_bulk_genes', '5k', 'CV', 'CV300', 'CV300_4MA', 'Boruta_4MA', 'Boruta_6', 'CVBig', 'final300', 'final350', 'final', 'sig_4_6_0.05', 'sig_4_6_0.1', 'sig9_0.05', 'sig9_0.01', 'sig9', 'GEOMX_4_6_0.05_'])\n"
     ]
    }
   ],
   "source": [
    "# Load the RDS file\n",
    "rds_path = '/tmp/work/RCproject/gene_lists.rds'\n",
    "rds_data = r.readRDS(rds_path)\n",
    "\n",
    "# Extract the names of the lists and their contents\n",
    "gene_lists = {}\n",
    "for name, item in zip(rds_data.names, rds_data):\n",
    "    # Each 'item' is a list associated with the 'name'\n",
    "    inner_list = list(item)  # Convert the inner R list to a Python list\n",
    "    gene_lists[name] = inner_list\n",
    "\n",
    "# Now `python_data` is a dictionary with names as keys and lists as values\n",
    "print(gene_lists.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d29fdb4d-c7cc-4693-910f-ef519d59bf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366\n"
     ]
    }
   ],
   "source": [
    "# select a gene_list\n",
    "\n",
    "current_genes = gene_lists[current_gene_list]\n",
    "print(len(current_genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db574c6-719c-4623-a50d-a80053d5eb57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: SingleCellExperiment\n",
      "\n",
      "R[write to console]: Loading required package: SummarizedExperiment\n",
      "\n",
      "R[write to console]: Loading required package: MatrixGenerics\n",
      "\n",
      "R[write to console]: Loading required package: matrixStats\n",
      "\n",
      "R[write to console]: \n",
      "Attaching package: ‘MatrixGenerics’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:matrixStats’:\n",
      "\n",
      "    colAlls, colAnyNAs, colAnys, colAvgsPerRowSet, colCollapse,\n",
      "    colCounts, colCummaxs, colCummins, colCumprods, colCumsums,\n",
      "    colDiffs, colIQRDiffs, colIQRs, colLogSumExps, colMadDiffs,\n",
      "    colMads, colMaxs, colMeans2, colMedians, colMins, colOrderStats,\n",
      "    colProds, colQuantiles, colRanges, colRanks, colSdDiffs, colSds,\n",
      "    colSums2, colTabulates, colVarDiffs, colVars, colWeightedMads,\n",
      "    colWeightedMeans, colWeightedMedians, colWeightedSds,\n",
      "    colWeightedVars, rowAlls, rowAnyNAs, rowAnys, rowAvgsPerColSet,\n",
      "    rowCollapse, rowCounts, rowCummaxs, rowCummins, rowCumprods,\n",
      "    rowCumsums, rowDiffs, rowIQRDiffs, rowIQRs, rowLogSumExps,\n",
      "    rowMadDiffs, rowMads, rowMaxs, rowMeans2, rowMedians, rowMins,\n",
      "    rowOrderStats, rowProds, rowQuantiles, rowRanges, rowRanks,\n",
      "    rowSdDiffs, rowSds, rowSums2, rowTabulates, rowVarDiffs, rowVars,\n",
      "    rowWeightedMads, rowWeightedMeans, rowWeightedMedians,\n",
      "    rowWeightedSds, rowWeightedVars\n",
      "\n",
      "\n",
      "R[write to console]: Loading required package: GenomicRanges\n",
      "\n",
      "R[write to console]: Loading required package: stats4\n",
      "\n",
      "R[write to console]: Loading required package: BiocGenerics\n",
      "\n",
      "R[write to console]: \n",
      "Attaching package: ‘BiocGenerics’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    IQR, mad, sd, var, xtabs\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n",
      "    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n",
      "    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n",
      "    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n",
      "    Position, rank, rbind, Reduce, rownames, sapply, setdiff, sort,\n",
      "    table, tapply, union, unique, unsplit, which.max, which.min\n",
      "\n",
      "\n",
      "R[write to console]: Loading required package: S4Vectors\n",
      "\n",
      "R[write to console]: \n",
      "Attaching package: ‘S4Vectors’\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from ‘package:utils’:\n",
      "\n",
      "    findMatches\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    expand.grid, I, unname\n",
      "\n",
      "\n",
      "R[write to console]: Loading required package: IRanges\n",
      "\n",
      "R[write to console]: Loading required package: GenomeInfoDb\n",
      "\n",
      "R[write to console]: Loading required package: Biobase\n",
      "\n",
      "R[write to console]: Welcome to Bioconductor\n",
      "\n",
      "    Vignettes contain introductory material; view with\n",
      "    'browseVignettes()'. To cite Bioconductor, see\n",
      "    'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n",
      "\n",
      "\n",
      "R[write to console]: \n",
      "Attaching package: ‘Biobase’\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from ‘package:MatrixGenerics’:\n",
      "\n",
      "    rowMedians\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:matrixStats’:\n",
      "\n",
      "    anyMissing, rowMedians\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts DataFrame:\n",
      "        GSM3899156_GSE133057  GSM3899157_GSE133057  GSM3899158_GSE133057  \\\n",
      "A1CF                0.057669             -0.027535              0.447468   \n",
      "A2M                 0.392288             -0.233300             -0.691535   \n",
      "A2ML1              -0.448972             -0.560902             -0.670516   \n",
      "A4GALT             -0.218099              0.224682             -0.247762   \n",
      "AAAS               -1.148204             -0.252932              0.053839   \n",
      "\n",
      "        GSM3899159_GSE133057  GSM3899160_GSE133057  GSM3899161_GSE133057  \\\n",
      "A1CF                0.045675              0.014587              1.406735   \n",
      "A2M                 0.293834             -1.091569             -0.816868   \n",
      "A2ML1               0.532968              0.516362              1.266745   \n",
      "A4GALT             -1.663294             -0.673025              0.513021   \n",
      "AAAS                0.442668              0.006890              0.582186   \n",
      "\n",
      "        GSM3899162_GSE133057  GSM3899163_GSE133057  GSM3899164_GSE133057  \\\n",
      "A1CF               -0.390043             -0.141597             -0.939894   \n",
      "A2M                 0.115628              0.384196              1.236704   \n",
      "A2ML1               1.132630              0.017735              0.335514   \n",
      "A4GALT              0.452215              1.025558              1.812865   \n",
      "AAAS                0.456027              0.470222             -0.351307   \n",
      "\n",
      "        GSM3899165_GSE133057  ...  GSM6390453_GSE209746  GSM6390454_GSE209746  \\\n",
      "A1CF                0.496517  ...              0.481642             -0.494055   \n",
      "A2M                 0.721205  ...              1.271784              2.545357   \n",
      "A2ML1               0.367068  ...             -1.763592             -1.603974   \n",
      "A4GALT             -0.386278  ...             -0.792048             -0.029957   \n",
      "AAAS                0.276047  ...              0.178485              0.211358   \n",
      "\n",
      "        GSM6390455_GSE209746  GSM6390456_GSE209746  GSM6390457_GSE209746  \\\n",
      "A1CF                0.692194              0.141950              0.554797   \n",
      "A2M                 1.705187              2.176357              2.173685   \n",
      "A2ML1              -1.897987             -1.911380             -1.666886   \n",
      "A4GALT             -0.723102             -0.532311             -0.472129   \n",
      "AAAS                0.189983              0.189037             -0.142190   \n",
      "\n",
      "        GSM6390458_GSE209746  GSM6390459_GSE209746  GSM6390460_GSE209746  \\\n",
      "A1CF                0.756641              0.757147              0.263922   \n",
      "A2M                 1.465240              1.147636              2.230243   \n",
      "A2ML1              -1.513177             -1.772573             -1.714675   \n",
      "A4GALT             -1.560049             -1.291245             -0.483064   \n",
      "AAAS               -0.114270              0.214479             -0.019179   \n",
      "\n",
      "        GSM6390461_GSE209746  GSM6390462_GSE209746  \n",
      "A1CF                0.054849              1.130573  \n",
      "A2M                 2.129601              1.589073  \n",
      "A2ML1              -2.031892             -1.777141  \n",
      "A4GALT             -0.167779             -0.986447  \n",
      "AAAS                0.088624             -0.014329  \n",
      "\n",
      "[5 rows x 450 columns]\n",
      "\n",
      "Metadata DataFrame:\n",
      "                     Response  TRG therapy Treatment Platform      batch\n",
      "GSM3899156_GSE133057      yes  1,2     CRT       pre  GPL6102  GSE133057\n",
      "GSM3899157_GSE133057  partial    3     CRT       pre  GPL6102  GSE133057\n",
      "GSM3899158_GSE133057      yes  1,2     CRT       pre  GPL6102  GSE133057\n",
      "GSM3899159_GSE133057  partial    3     CRT       pre  GPL6102  GSE133057\n",
      "GSM3899160_GSE133057      yes  1,2     CRT       pre  GPL6102  GSE133057\n"
     ]
    }
   ],
   "source": [
    "#import the data\n",
    "\n",
    "# Function to read RDS file and extract counts and metadata\n",
    "def read_rds_to_matrix_and_metadata(file_path):\n",
    "    # Load the RDS file in R\n",
    "    ro.r(f\"sce <- readRDS('{file_path}')\")\n",
    "\n",
    "    # Extract count data (assumed to be stored in assays)\n",
    "    counts = ro.r('assay(sce, \"scalelogcounts\")')\n",
    "    # Extract row (gene) and column (cell) names\n",
    "    gene_names = ro.r('rownames(sce)')\n",
    "    cell_names = ro.r('colnames(sce)')\n",
    "    \n",
    "    # Convert to a NumPy array\n",
    "    counts_np = ro.conversion.rpy2py(counts)\n",
    "\n",
    "    # Convert the counts matrix to a pandas DataFrame\n",
    "    counts_df = pd.DataFrame(counts_np, index=gene_names, columns=cell_names)\n",
    "\n",
    "    # Extract metadata from colData and convert to a pandas DataFrame directly\n",
    "    metadata = ro.r('as.data.frame(colData(sce))')  # Get the colData as an R data frame\n",
    "    metadata_df = pd.DataFrame(metadata)  # Convert R data frame to pandas DataFrame directly\n",
    "\n",
    "    return counts_df, metadata_df\n",
    "\n",
    "# Usage example\n",
    "file_path = '/tmp/work/RCproject/GEO_singlecellexperiment.rds'\n",
    "counts_df, metadata_df = read_rds_to_matrix_and_metadata(file_path)\n",
    "\n",
    "# Display the results\n",
    "print(\"Counts DataFrame:\")\n",
    "print(counts_df.head())  # Show the first few rows of the counts DataFrame\n",
    "print(\"\\nMetadata DataFrame:\")\n",
    "print(metadata_df.head())  # Show the first few rows of the metadata DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6583f32-fa79-4bc3-b575-e0cd98b82eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gene_list:\n",
    "    counts_df = counts_df.loc[current_genes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcdfc63c-ef87-4636-a16f-89c97a7568db",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_metadata_df = metadata_df[~metadata_df['Response'].isin(['partial'])].copy()\n",
    "\n",
    "row_names = filtered_metadata_df.index.tolist()  # Convert index to a list\n",
    "\n",
    "filtered_counts_df = counts_df[row_names].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7a6582-5a3a-4174-83e2-6e0d55f0d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata = metadata_df[~adata.obs['Response'].isin(['partial']), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f915216b-b70c-4c2d-a76d-a43fad99f14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(410,)\n",
      "(410,)\n"
     ]
    }
   ],
   "source": [
    "# generate numerical values for each batch category\n",
    "# set up categories variable\n",
    "\n",
    "# categories_technology = adata.obs['batch']\n",
    "categories_technology = filtered_metadata_df['batch']\n",
    "\n",
    "#collpse the categories to microARRAY vs sequencing\n",
    "\n",
    "if simplifly_categories:\n",
    "    category_map = {'GSE133057': 'micro', 'GSE145037': 'micro', 'GSE150082': 'micro','GSE190826':'seq','GSE209746':'seq',\n",
    "                    'GSE45404_GPL1': 'micro', 'GSE45404_GPL2': 'micro', 'GSE93375': 'micro','GSE94104': 'micro'}\n",
    "    categories_technology = np.vectorize(category_map.get)(categories_technology)\n",
    "    \n",
    "# Create a LabelEncoder instance\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit and transform the categories to integers\n",
    "numerical_categories_technology = label_encoder.fit_transform(categories_technology)\n",
    "print(numerical_categories_technology.shape)\n",
    "\n",
    "#do the same for the response variable\n",
    "\n",
    "# categories_outcome = adata.obs['Response']\n",
    "categories_outcome = filtered_metadata_df['Response']\n",
    "\n",
    "numerical_categories_outcome = label_encoder.fit_transform(categories_outcome)\n",
    "print(numerical_categories_technology.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8664a4b-9266-4cf4-b692-2c0d4ed39a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technology frequencies\n",
      "0    209\n",
      "1    201\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Outcome frequencies\n",
      "0    267\n",
      "1    143\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_counts = pd.Series(numerical_categories_technology).value_counts()\n",
    "print('Technology frequencies')\n",
    "print(frequency_counts)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Outcome frequencies')\n",
    "frequency_counts = pd.Series(numerical_categories_outcome).value_counts()\n",
    "print(frequency_counts)\n",
    "\n",
    "unique_combinations_array = (numerical_categories_outcome + (numerical_categories_technology+1)*2)-2\n",
    "\n",
    "np.unique(unique_combinations_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "056bf30b-3e46-4f4c-a4af-84b512dacb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming filtered_metadata_df is your filtered DataFrame\n",
    "filtered_metadata_df.loc[:, 'numerical_categories_technology'] = numerical_categories_technology\n",
    "filtered_metadata_df.loc[:, 'numerical_categories_outcome'] = numerical_categories_outcome\n",
    "filtered_metadata_df.loc[:, 'combination_tech_outcome'] = unique_combinations_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b4334e7-f769-4c03-937b-e148872ee0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizaiton\n",
    "# gene_expression_data = adata.layers['scalelogcounts']\n",
    "\n",
    "gene_expression_data = filtered_counts_df.T\n",
    "\n",
    "#Min-max normalization\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "gene_expression_data = scaler.fit_transform(gene_expression_data)\n",
    "\n",
    "number_genes = gene_expression_data.shape[1]\n",
    "input_dim = number_genes\n",
    "\n",
    "if PCA_reduce:\n",
    "# Initialize PCA and fit it to X_train\n",
    "    n_components = n_comp_PCA  # You can adjust this based on your data\n",
    "    pca = PCA(n_components=n_components)\n",
    "    gene_expression_data = pca.fit_transform(gene_expression_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d46fe7c-8faf-4f29-82f8-f2d00be059e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the test and train datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(gene_expression_data, filtered_metadata_df, test_size=test_set_size, random_state=1)\n",
    "\n",
    "y_train_outcome = y_train['numerical_categories_outcome']\n",
    "y_test_outcome = y_test['numerical_categories_outcome']\n",
    "\n",
    "y_train_tech = y_train['numerical_categories_technology']\n",
    "y_test_tech = y_test['numerical_categories_technology']\n",
    "\n",
    "y_train_comb = y_train['combination_tech_outcome']\n",
    "y_test_comb = y_test['combination_tech_outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0f626da-39f7-418e-8e73-f267b981e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the input shape\n",
    "# # input_shape = (gene_expression_data.shape[1],)[0]  # Number of genes\n",
    "\n",
    "input_shape = (X_train.shape[1],)[0]  # Number of genes\n",
    "\n",
    "def build_outcome_classifier():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(input_shape,)))  # Input shape matches your data\n",
    "    \n",
    "    model.add(layers.Dense((512*multiplier),kernel_regularizer=tf.keras.regularizers.l2(l2_reg),kernel_initializer='he_normal'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.1))  # Leaky ReLU helps with vanishing gradients\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Dense((256*multiplier),kernel_regularizer=tf.keras.regularizers.l2(l2_reg),kernel_initializer='he_normal'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Dense((128*multiplier),kernel_regularizer=tf.keras.regularizers.l2(l2_reg),kernel_initializer='he_normal'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(layers.Dense((64*multiplier),kernel_regularizer=tf.keras.regularizers.l2(l2_reg),kernel_initializer='he_normal'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Dense((32),kernel_initializer='he_normal'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.1))\n",
    "    \n",
    "    # Output layer for binary classification with sigmoid activation\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f07cdd3d-9030-4220-93f4-f1bbf2e8713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "outcome_classifier = build_outcome_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a566cb12-0842-4253-9379-d32fdebb4225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the callback-like function to adjust learning rate based on AUC\n",
    "def adjust_learning_rate_by_auc(epoch, model, X_test, y_test_outcome, lr_dict, auc_thresholds,test_auc):\n",
    "    # Get model's current learning rate\n",
    "    current_lr = tf.keras.backend.get_value(model.optimizer.lr)\n",
    "\n",
    "    # Adjust learning rate based on AUC thresholds\n",
    "    new_lr = current_lr\n",
    "    for threshold in auc_thresholds:\n",
    "        if test_auc >= threshold:\n",
    "            new_lr = lr_dict[threshold]\n",
    "    \n",
    "    # Set new learning rate\n",
    "    if new_lr != current_lr:\n",
    "        tf.keras.backend.set_value(model.optimizer.lr, new_lr)\n",
    "        #print(f\"Epoch {epoch + 1}: Adjusting learning rate to {new_lr:.6f}\")\n",
    "    \n",
    "    return test_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e85e8399-811c-42d2-8150-72318a156f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set up the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm = clipnorm)\n",
    "\n",
    "# determine the sample and batch size\n",
    "num_samples = math.floor(X_train.shape[0]* (1-holdout_size))  # number of samples used in each training epoch\n",
    "\n",
    "# batch_size = adata.shape[0]\n",
    "\n",
    "# Calculate the number of steps per epoch\n",
    "num_steps_per_epoch = num_samples // batch_size\n",
    "\n",
    "# Compile the outcome discriminator\n",
    "outcome_classifier.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ecf815-9b2c-4a7d-bd55-af6e2326a740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Outcome Loss: 0.6957475922324441, Average Accuracy: 0.5113636363636364, Test AUC: 0.5000, Test Accuracy: 0.6341\n",
      "Epoch 100, Average Outcome Loss: 0.6457759196108038, Average Accuracy: 0.6420454545454546, Test AUC: 0.7436, Test Accuracy: 0.4634\n",
      "Epoch 200, Average Outcome Loss: 0.4951503710313277, Average Accuracy: 0.7329545454545454, Test AUC: 0.8154, Test Accuracy: 0.7073\n",
      "Epoch 300, Average Outcome Loss: 0.8590619564056396, Average Accuracy: 0.6306818181818182, Test AUC: 0.8436, Test Accuracy: 0.4634\n",
      "Epoch 400, Average Outcome Loss: 0.36511552604761993, Average Accuracy: 0.8920454545454546, Test AUC: 0.8564, Test Accuracy: 0.7561\n",
      "Epoch 500, Average Outcome Loss: 0.37748258086768066, Average Accuracy: 0.8522727272727273, Test AUC: 0.8564, Test Accuracy: 0.7805\n",
      "Epoch 600, Average Outcome Loss: 0.35201046006246045, Average Accuracy: 0.8636363636363636, Test AUC: 0.8821, Test Accuracy: 0.7561\n",
      "Epoch 700, Average Outcome Loss: 0.2802031094377691, Average Accuracy: 0.8863636363636364, Test AUC: 0.8718, Test Accuracy: 0.8049\n",
      "Epoch 800, Average Outcome Loss: 0.2879399995912205, Average Accuracy: 0.8579545454545454, Test AUC: 0.8641, Test Accuracy: 0.8049\n",
      "Epoch 900, Average Outcome Loss: 0.250055578621951, Average Accuracy: 0.8977272727272727, Test AUC: 0.8641, Test Accuracy: 0.8049\n",
      "Epoch 1000, Average Outcome Loss: 0.24095928533510727, Average Accuracy: 0.9318181818181818, Test AUC: 0.8590, Test Accuracy: 0.8049\n"
     ]
    }
   ],
   "source": [
    "test_accuracy_list = []\n",
    "train_accuracy_list = []\n",
    "test_auc_list = []\n",
    "\n",
    "num_outcomes = len(np.unique(y_test_outcome))\n",
    "num_conditions = len(np.unique(unique_combinations_array))\n",
    "\n",
    "# Define the training step for only the outcome classifier\n",
    "def train_step(data, outcome_labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass through the outcome classifier\n",
    "        outcome_predictions = outcome_classifier(data)\n",
    "\n",
    "        # Compute the biological discriminator loss\n",
    "        outcome_loss = tf.keras.losses.binary_crossentropy(outcome_labels, outcome_predictions)\n",
    "        outcome_loss = tf.reduce_mean(outcome_loss)  # Average over the batch\n",
    "\n",
    "    # Compute gradients for the outcome classifier\n",
    "    classifier_grads = tape.gradient(outcome_loss, outcome_classifier.trainable_variables)\n",
    "    \n",
    "    # Calculate accuracy for the outcome classifier\n",
    "    predicted_outcome_labels = tf.cast(outcome_predictions > 0.5, tf.float32)  # Threshold at 0.5\n",
    "    outcome_labels_float = tf.cast(outcome_labels, tf.float32)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_outcome_labels, outcome_labels_float), tf.float32))\n",
    "\n",
    "    return outcome_loss, accuracy, classifier_grads\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0  # To accumulate losses\n",
    "    total_accuracy = 0.0  # To accumulate accuracy\n",
    "    accumulated_grads = [tf.zeros_like(var) for var in outcome_classifier.trainable_variables]  # Initialize gradient accumulator\n",
    "\n",
    "    # Split train data randomly, holding out a portion for generalization\n",
    "    X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(X_train, y_train, test_size=holdout_size, random_state=None)\n",
    "    y_train_comb_temp = y_train_temp['combination_tech_outcome']\n",
    "    y_train_temp = y_train_temp['numerical_categories_outcome']\n",
    "    \n",
    "    # Mini-batch training loop\n",
    "    for step in range(num_steps_per_epoch):\n",
    "        # Balance batches if necessary\n",
    "        batch_indices = []\n",
    "        if balance:\n",
    "            for condition in range(num_conditions):\n",
    "                condition_indices = np.where(y_train_comb_temp == condition)[0]\n",
    "                condition_batch_indices = np.random.choice(condition_indices, size=batch_size // num_conditions, replace=True)\n",
    "                batch_indices.append(condition_batch_indices)\n",
    "        else:\n",
    "            all_indices = np.arange(len(X_train_temp))\n",
    "            random_indices = np.random.choice(all_indices, size=batch_size, replace=True)\n",
    "            batch_indices.append(random_indices)\n",
    "        X_batch = X_train_temp[np.concatenate(batch_indices)]\n",
    "        #y_batch = y_train_temp[np.concatenate(batch_indices)]\n",
    "        y_batch = y_train_temp.iloc[np.concatenate(batch_indices)]\n",
    "        y_batch = tf.expand_dims(y_batch, axis=-1)  # Adjust labels shape for binary_crossentropy\n",
    "                \n",
    "        # Perform the training step and collect gradients\n",
    "        outcome_loss, accuracy, classifier_grads = train_step(X_batch, y_batch)\n",
    "        \n",
    "        # Accumulate gradients and losses\n",
    "        total_loss += outcome_loss.numpy()\n",
    "        total_accuracy += accuracy.numpy()\n",
    "        accumulated_grads = [acc_grad + grad for acc_grad, grad in zip(accumulated_grads, classifier_grads)]\n",
    "\n",
    "    # Average the accumulated gradients\n",
    "    averaged_grads = [grad / num_steps_per_epoch for grad in accumulated_grads]\n",
    "\n",
    "    # Apply averaged gradients to update model weights\n",
    "    optimizer.apply_gradients(zip(averaged_grads, outcome_classifier.trainable_variables))\n",
    "\n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    avg_loss = total_loss / num_steps_per_epoch\n",
    "    avg_accuracy = total_accuracy / num_steps_per_epoch\n",
    "\n",
    "    if epoch % report_frequency == 0:\n",
    "\n",
    "        #adjust the learning rate depending on test set performance\n",
    "        # adjust_learning_rate_by_auc(epoch, outcome_classifier, X_test_temp, y_test_temp['numerical_categories_outcome'], lr_dict, auc_thresholds)\n",
    "        \n",
    "        # Evaluate on test data\n",
    "        outcome_predictions = outcome_classifier(X_test)\n",
    "        outcome_labels = tf.expand_dims(y_test_outcome, axis=-1)  # Reshape to match logits shape\n",
    "        outcome_labels_float = tf.cast(outcome_labels, tf.float32)\n",
    "    \n",
    "        # Calculate AUC\n",
    "        outcome_predictions_np = outcome_predictions.numpy().flatten()  # Convert predictions to numpy for roc_auc_score\n",
    "        outcome_labels_np = outcome_labels_float.numpy().flatten()  # Convert labels to numpy for roc_auc_score\n",
    "        test_auc = roc_auc_score(outcome_labels_np, outcome_predictions_np)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted_outcome_labels = tf.cast(outcome_predictions > 0.5, tf.float32)  # Threshold at 0.5\n",
    "        outcome_labels_float = tf.cast(outcome_labels, tf.float32)\n",
    "        test_accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_outcome_labels, outcome_labels_float), tf.float32))\n",
    "    \n",
    "        # Store and print metrics\n",
    "        train_accuracy_list.append(avg_accuracy)\n",
    "        test_auc_list.append(test_auc)\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "        \n",
    "        if epoch % (report_frequency*100) == 0:\n",
    "            print(f'Epoch {epoch}, Average Outcome Loss: {avg_loss}, Average Accuracy: {avg_accuracy}, Test AUC: {test_auc:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "        adjust_learning_rate_by_auc(epoch, outcome_classifier, X_test_temp, y_test_temp['numerical_categories_outcome'], lr_dict, auc_thresholds, test_auc)\n",
    "    \n",
    "        # Early stopping condition for AUC (if needed)\n",
    "        if test_auc > auc_threshold:  # Define auc_threshold as desired\n",
    "            print('Early stopping triggered based on AUC')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617fa863-e671-4f71-8457-4918e949a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(test_auc_list))\n",
    "print(max(test_accuracy_list))\n",
    "print(max(train_accuracy_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c2612-7184-44a1-9acd-9005348dfb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, plot the metrics\n",
    "\n",
    "frequency_counts = pd.Series(y_test_outcome).value_counts()\n",
    "test_chance_level = frequency_counts[0]/len(y_test_outcome)\n",
    "\n",
    "frequency_counts = pd.Series(y_train_outcome).value_counts()\n",
    "train_chance_level = frequency_counts[0]/len(y_train_outcome)\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12, 6))\n",
    "\n",
    "x_values = np.arange(1, len(train_accuracy_list) + 1) * report_frequency\n",
    "\n",
    "# Plot train accuracy\n",
    "axs[0].plot(x_values, train_accuracy_list, label='Training Accuracy', color='blue')\n",
    "axs[0].axhline(train_chance_level, color='black',linestyle ='--')\n",
    "axs[0].set_title('Training set accuracy over epochs')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Training Accuracy')\n",
    "axs[0].grid()\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot test accuracay\n",
    "axs[1].plot(x_values, test_accuracy_list, label='Test Accuracy', color='orange')\n",
    "axs[1].axhline(test_chance_level, color='black',linestyle ='--')\n",
    "axs[1].set_title('Test set accuracy over epochs')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Test Accuracy')\n",
    "axs[1].grid()\n",
    "axs[1].legend()\n",
    "\n",
    "# Plot test accuracay\n",
    "axs[2].plot(x_values, test_auc_list, label='Test AUC', color='orange')\n",
    "# axs[2].axhline(0.5, color='black',linestyle ='--')\n",
    "axs[2].set_title('Test set AUC over epochs')\n",
    "axs[2].set_xlabel('Epoch')\n",
    "axs[2].set_ylabel('Test AUC')\n",
    "axs[2].grid()\n",
    "axs[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4e4636-c91e-4a73-be93-40b6cc7cc28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After loop: Calculate ROC and AUC using continuous probabilities\n",
    "fpr, tpr, thresholds = roc_curve(y_test_outcome, outcome_predictions)  # Use probabilities, not thresholded labels\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line for random guessing\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for ANN')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e84149f-2ef8-4123-bdb5-c23db0f40174",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train_outcome)\n",
    "\n",
    "y_pred_prob = clf.predict_proba(X_test)[:, 1]  # Probabilities for the positive class (class 1)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test_outcome, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line for random guessing\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for random forest')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137e9c6-8f01-4e7b-bc2f-bb08b500f107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa81e2-5e22-4549-88c0-a52e1050e3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
