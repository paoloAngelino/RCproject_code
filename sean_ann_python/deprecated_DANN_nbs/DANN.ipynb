{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35661c9e-9594-486e-af1a-51de75178a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "2024-10-14 07:45:10.254338: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/anndata/__init__.py:51: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import anndata\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "adata = sc.read('/tmp/work/RCproject_code/sce_export.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6fd8df6-5a8e-4cc6-9b01-26d1225907cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# set up OneHotEncocer for differeing samples\n",
    "\n",
    "# Get domain labels from the 'batch' column in your AnnData object (as a pandas Series)\n",
    "domain_labels = adata.obs['batch'].values  # Convert Series to NumPy array\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit and transform the domain labels to get a one-hot encoded matrix\n",
    "domain_one_hot = encoder.fit_transform(domain_labels.reshape(-1, 1))\n",
    "\n",
    "# Print the one-hot encoded matrix\n",
    "print(domain_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ac5e49f-832e-443b-955e-fdcd050315e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizaiton\n",
    "\n",
    "gene_expression_data = adata.layers['logcounts']\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Min-max normalization\n",
    "scaler = MinMaxScaler()\n",
    "gene_expression_data = scaler.fit_transform(gene_expression_data)\n",
    "\n",
    "number_samples = adata.shape[0]\n",
    "number_genes = adata.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4afd8afe-4c5a-4141-8614-ba41a736aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the encooder\n",
    "\n",
    "input_dim = number_genes\n",
    "encoding_dim = 64  # Dimensionality of the encoding space\n",
    "\n",
    "# Encoder Model\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "encoded = layers.Dense(128, activation='relu')(input_layer)\n",
    "encoded = layers.BatchNormalization()(encoded)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "encoded = layers.BatchNormalization()(encoded)\n",
    "encoded_output = layers.Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "# Build the encoder model\n",
    "encoder = models.Model(inputs=input_layer, outputs=encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17375a9a-0a10-4de3-ae39-3bb14811cef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the decoder\n",
    "\n",
    "# Decoder Model\n",
    "decoded = layers.Dense(64, activation='relu')(encoded_output)\n",
    "decoded = layers.BatchNormalization()(decoded)\n",
    "decoded = layers.Dense(128, activation='relu')(decoded)\n",
    "decoded_output = layers.Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# Build the autoencoder model (encoder + decoder)\n",
    "autoencoder = models.Model(inputs=input_layer, outputs=decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db7fc6d2-9de3-4b95-a13a-ab48a694b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the domain discriminator\n",
    "\n",
    "# Get the number of unique domains (batches)\n",
    "num_domains = len(domain_labels)\n",
    "\n",
    "# Domain Discriminator Model\n",
    "discriminator_input = layers.Input(shape=(encoding_dim,))\n",
    "discriminator_hidden = layers.Dense(64, activation='relu')(discriminator_input)\n",
    "discriminator_hidden = layers.BatchNormalization()(discriminator_hidden)\n",
    "discriminator_hidden = layers.Dropout(0.2)(discriminator_hidden)\n",
    "\n",
    "# Use 'num_domains' to specify the number of output units (one for each domain)\n",
    "discriminator_output = layers.Dense(num_domains, activation='softmax')(discriminator_hidden)\n",
    "\n",
    "# Build the discriminator model\n",
    "discriminator = models.Model(inputs=discriminator_input, outputs=discriminator_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "823d8808-fbce-46b8-9f17-0c4a4cf49d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the autoencoder (reconstruction task)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Compile the domain discriminator (domain classification task)\n",
    "discriminator.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ea0ab1b-0adc-49e4-819d-01fbb11bbd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the discriminator while training the encoder\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Adversarial Model (Encoder + Discriminator)\n",
    "encoded_repr = encoder(input_layer)  # Shared encoder\n",
    "domain_pred = discriminator(encoded_repr)  # Domain prediction from the encoder\n",
    "\n",
    "# Build the adversarial model (encoder tries to fool the discriminator)\n",
    "adversarial_model = models.Model(inputs=input_layer, outputs=[decoded_output, domain_pred])\n",
    "adversarial_model.compile(optimizer='adam', loss=['mse', 'categorical_crossentropy'], \n",
    "                          loss_weights=[1, 0.1])  # Weighted losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33a832e8-4622-486b-94f3-f7171a71b00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 1s 44ms/step - loss: 0.0971 - val_loss: 0.0788\n",
      "12/12 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 9) and (32, 450) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m encoded_data \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mpredict(X_train)  \u001b[38;5;66;03m# The neural network encoder model\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Train the domain discriminator using the encoded data and one-hot encoded domain labels\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_domains_np_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Train the adversarial model (encoder trying to fool the discriminator)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m adversarial_model\u001b[38;5;241m.\u001b[39mfit(X_train, [X_train, y_train_domains_np_onehot], epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filedl2igrim.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 9) and (32, 450) are incompatible\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# OneHotEncode domain labels (not the neural network encoder)\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train_domains, y_test_domains = train_test_split(\n",
    "    gene_expression_data, domain_labels, test_size=0.2\n",
    ")\n",
    "\n",
    "# Convert y_train_domains and y_test_domains to NumPy arrays (if needed)\n",
    "y_train_domains_np = np.array(y_train_domains)\n",
    "y_test_domains_np = np.array(y_test_domains)\n",
    "\n",
    "# Apply OneHotEncoding to the domain labels\n",
    "y_train_domains_np_onehot = one_hot_encoder.fit_transform(y_train_domains_np.reshape(-1, 1))\n",
    "y_test_domains_np_onehot = one_hot_encoder.transform(y_test_domains_np.reshape(-1, 1))\n",
    "\n",
    "# Define the neural network encoder (this is separate from the OneHotEncoder)\n",
    "encoding_dim = 64  # Example latent space dimension\n",
    "input_dim = X_train.shape[1]  # Number of features in your input data\n",
    "\n",
    "encoder_input = layers.Input(shape=(input_dim,))\n",
    "encoder_hidden = layers.Dense(128, activation='relu')(encoder_input)\n",
    "encoder_output = layers.Dense(encoding_dim, activation='relu')(encoder_hidden)\n",
    "encoder = models.Model(inputs=encoder_input, outputs=encoder_output)\n",
    "\n",
    "# Early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    autoencoder.fit(X_train, X_train, epochs=1, batch_size=batch_size, shuffle=True, validation_split=0.2, callbacks=[early_stopping])\n",
    "    \n",
    "    # Get encoded data from the neural network encoder, not the OneHotEncoder\n",
    "    encoded_data = encoder.predict(X_train)  # The neural network encoder model\n",
    "    \n",
    "    # Train the domain discriminator using the encoded data and one-hot encoded domain labels\n",
    "    discriminator.fit(encoded_data, y_train_domains_np_onehot, epochs=1, batch_size=batch_size, shuffle=True, validation_split=0.2)\n",
    "    \n",
    "    # Train the adversarial model (encoder trying to fool the discriminator)\n",
    "    adversarial_model.fit(X_train, [X_train, y_train_domains_np_onehot], epochs=1, batch_size=batch_size, validation_split=0.2, shuffle=True)\n",
    "\n",
    "# After training, get the encoded representations\n",
    "encoded_representations = encoder.predict(gene_expression_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e0a4c35-2806-438f-816f-af79c82db4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6e40df9-52d1-4678-87e3-6076d05126ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m9\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.add(Dense(9, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37614984-795f-442f-934f-540abb0dfcac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37208c65-1faf-4c93-89f8-97c24e568eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 9)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_domains_np_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a462f74e-655a-449a-91f0-93e8b0c24f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_domains_np_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3802f3-1a02-4592-a321-2ee4e921df69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
