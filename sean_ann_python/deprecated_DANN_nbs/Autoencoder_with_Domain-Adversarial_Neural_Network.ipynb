{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "128f6d1c-7bfb-4186-bf04-c0b51a044c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/anndata/__init__.py:51: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import anndata\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "adata = sc.read('/tmp/work/RCproject_code/sce_export.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "feb87840-b0d2-4aed-9944-e46206526b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Get domain labels from the 'batch' column in your AnnData object (as a pandas Series)\n",
    "domain_labels = adata.obs['batch'].values  # Convert Series to NumPy array\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit and transform the domain labels to get a one-hot encoded matrix\n",
    "domain_one_hot = encoder.fit_transform(domain_labels.reshape(-1, 1))\n",
    "\n",
    "# Print the one-hot encoded matrix\n",
    "print(domain_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a2b8e92-daf0-465a-9b65-3d2a71b658be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizaiton\n",
    "\n",
    "gene_expression_data = adata.layers['logcounts']\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Min-max normalization\n",
    "scaler = MinMaxScaler()\n",
    "gene_expression_data = scaler.fit_transform(gene_expression_data)\n",
    "\n",
    "number_samples = adata.shape[0]\n",
    "number_genes = adata.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab012fea-8d25-4e86-af09-233919519eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the encooder\n",
    "\n",
    "input_dim = number_genes\n",
    "encoding_dim = 32  # Dimensionality of the encoding space\n",
    "\n",
    "# Encoder Model\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "encoded = layers.Dense(128, activation='relu')(input_layer)\n",
    "encoded = layers.BatchNormalization()(encoded)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "encoded = layers.BatchNormalization()(encoded)\n",
    "encoded_output = layers.Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "# Build the encoder model\n",
    "encoder = models.Model(inputs=input_layer, outputs=encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de759e02-0fce-40b3-8814-5083006914af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the decoder\n",
    "\n",
    "# Decoder Model\n",
    "decoded = layers.Dense(64, activation='relu')(encoded_output)\n",
    "decoded = layers.BatchNormalization()(decoded)\n",
    "decoded = layers.Dense(128, activation='relu')(decoded)\n",
    "decoded_output = layers.Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# Build the autoencoder model (encoder + decoder)\n",
    "autoencoder = models.Model(inputs=input_layer, outputs=decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c2a1ca-c8d8-4c49-85af-3d8336cb9b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the domain discriminator\n",
    "\n",
    "# Get the number of unique domains (batches)\n",
    "num_domains = len(domain_labels)\n",
    "\n",
    "# Domain Discriminator Model\n",
    "discriminator_input = layers.Input(shape=(encoding_dim,))\n",
    "discriminator_hidden = layers.Dense(64, activation='relu')(discriminator_input)\n",
    "discriminator_hidden = layers.BatchNormalization()(discriminator_hidden)\n",
    "discriminator_hidden = layers.Dropout(0.2)(discriminator_hidden)\n",
    "\n",
    "# Use 'num_domains' to specify the number of output units (one for each domain)\n",
    "discriminator_output = layers.Dense(num_domains, activation='softmax')(discriminator_hidden)\n",
    "\n",
    "# Build the discriminator model\n",
    "discriminator = models.Model(inputs=discriminator_input, outputs=discriminator_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81446190-9372-4637-b3ab-0959de732630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the autoencoder (reconstruction task)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Compile the domain discriminator (domain classification task)\n",
    "discriminator.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50fe2b47-1023-4d77-94f8-4cabf0b7eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the discriminator while training the encoder\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Adversarial Model (Encoder + Discriminator)\n",
    "encoded_repr = encoder(input_layer)  # Shared encoder\n",
    "domain_pred = discriminator(encoded_repr)  # Domain prediction from the encoder\n",
    "\n",
    "# Build the adversarial model (encoder tries to fool the discriminator)\n",
    "adversarial_model = models.Model(inputs=input_layer, outputs=[decoded_output, domain_pred])\n",
    "adversarial_model.compile(optimizer='adam', loss=['mse', 'categorical_crossentropy'], \n",
    "                          loss_weights=[1, 0.1])  # Weighted losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea7296c6-e53a-4f89-858a-044a96c2ebe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 1s 47ms/step - loss: 0.0982 - val_loss: 0.0822\n",
      "12/12 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 32), found shape=(32, 64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m encoded_data \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mpredict(X_train)  \u001b[38;5;66;03m# The neural network encoder model\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Train the domain discriminator using the encoded data and one-hot encoded domain labels\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_domains_np_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Train the adversarial model (encoder trying to fool the discriminator)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m adversarial_model\u001b[38;5;241m.\u001b[39mfit(X_train, [X_train, y_train_domains_np_onehot], epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filebzewudvw.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 32), found shape=(32, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# OneHotEncode domain labels (not the neural network encoder)\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train_domains, y_test_domains = train_test_split(\n",
    "    gene_expression_data, domain_labels, test_size=0.2\n",
    ")\n",
    "\n",
    "# Convert y_train_domains and y_test_domains to NumPy arrays (if needed)\n",
    "y_train_domains_np = np.array(y_train_domains)\n",
    "y_test_domains_np = np.array(y_test_domains)\n",
    "\n",
    "# Apply OneHotEncoding to the domain labels\n",
    "y_train_domains_np_onehot = one_hot_encoder.fit_transform(y_train_domains_np.reshape(-1, 1))\n",
    "y_test_domains_np_onehot = one_hot_encoder.transform(y_test_domains_np.reshape(-1, 1))\n",
    "\n",
    "# Define the neural network encoder (this is separate from the OneHotEncoder)\n",
    "encoding_dim = 64  # Example latent space dimension\n",
    "input_dim = X_train.shape[1]  # Number of features in your input data\n",
    "\n",
    "encoder_input = layers.Input(shape=(input_dim,))\n",
    "encoder_hidden = layers.Dense(128, activation='relu')(encoder_input)\n",
    "encoder_output = layers.Dense(encoding_dim, activation='relu')(encoder_hidden)\n",
    "encoder = models.Model(inputs=encoder_input, outputs=encoder_output)\n",
    "\n",
    "# Early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    autoencoder.fit(X_train, X_train, epochs=1, batch_size=batch_size, shuffle=True, validation_split=0.2, callbacks=[early_stopping])\n",
    "    \n",
    "    # Get encoded data from the neural network encoder, not the OneHotEncoder\n",
    "    encoded_data = encoder.predict(X_train)  # The neural network encoder model\n",
    "    \n",
    "    # Train the domain discriminator using the encoded data and one-hot encoded domain labels\n",
    "    discriminator.fit(encoded_data, y_train_domains_np_onehot, epochs=1, batch_size=batch_size, shuffle=True, validation_split=0.2)\n",
    "    \n",
    "    # Train the adversarial model (encoder trying to fool the discriminator)\n",
    "    adversarial_model.fit(X_train, [X_train, y_train_domains_np_onehot], epochs=1, batch_size=batch_size, validation_split=0.2, shuffle=True)\n",
    "\n",
    "# After training, get the encoded representations\n",
    "encoded_representations = encoder.predict(gene_expression_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a346970-983c-4392-85a6-69795dca5ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0496 - val_loss: 0.1259\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 2.0329 - accuracy: 0.1736 - val_loss: 1.9432 - val_accuracy: 0.1667\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 9) and (32, 450) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m     discriminator\u001b[38;5;241m.\u001b[39mfit(encoded_data, y_train_domains_np_onehot, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Train the adversarial model (encoder trying to fool the discriminator)\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     \u001b[43madversarial_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_domains_np_onehot\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# After training, get the encoded representations\u001b[39;00m\n\u001b[1;32m     64\u001b[0m encoded_representations \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mpredict(gene_expression_data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filebzewudvw.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 9) and (32, 450) are incompatible\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import layers, models\n",
    "\n",
    "# OneHotEncode domain labels (not the neural network encoder)\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train_domains, y_test_domains = train_test_split(\n",
    "    gene_expression_data, domain_labels, test_size=0.2\n",
    ")\n",
    "\n",
    "# Convert y_train_domains and y_test_domains to NumPy arrays (if needed)\n",
    "y_train_domains_np = np.array(y_train_domains)\n",
    "y_test_domains_np = np.array(y_test_domains)\n",
    "\n",
    "# Apply OneHotEncoding to the domain labels\n",
    "y_train_domains_np_onehot = one_hot_encoder.fit_transform(y_train_domains_np.reshape(-1, 1))\n",
    "y_test_domains_np_onehot = one_hot_encoder.transform(y_test_domains_np.reshape(-1, 1))\n",
    "\n",
    "# Define the neural network encoder (this is separate from the OneHotEncoder)\n",
    "encoding_dim = 32  # Adjusted to match the discriminator input\n",
    "input_dim = X_train.shape[1]  # Number of features in your input data\n",
    "\n",
    "encoder_input = layers.Input(shape=(input_dim,))\n",
    "encoder_hidden = layers.Dense(128, activation='relu')(encoder_input)\n",
    "encoder_output = layers.Dense(encoding_dim, activation='relu')(encoder_hidden)\n",
    "encoder = models.Model(inputs=encoder_input, outputs=encoder_output)\n",
    "\n",
    "# Define the domain discriminator\n",
    "discriminator_input = layers.Input(shape=(encoding_dim,))\n",
    "discriminator_hidden = layers.Dense(64, activation='relu')(discriminator_input)\n",
    "discriminator_output = layers.Dense(y_train_domains_np_onehot.shape[1], activation='softmax')(discriminator_hidden)\n",
    "discriminator = models.Model(inputs=discriminator_input, outputs=discriminator_output)\n",
    "\n",
    "# Compile models\n",
    "encoder.compile(optimizer='adam', loss='mse')\n",
    "discriminator.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    autoencoder.fit(X_train, X_train, epochs=1, batch_size=batch_size, shuffle=True, validation_split=0.2, callbacks=[early_stopping])\n",
    "    \n",
    "    # Get encoded data from the neural network encoder, not the OneHotEncoder\n",
    "    encoded_data = encoder.predict(X_train)  # The neural network encoder model\n",
    "    \n",
    "    # Train the domain discriminator using the encoded data and one-hot encoded domain labels\n",
    "    discriminator.fit(encoded_data, y_train_domains_np_onehot, epochs=1, batch_size=batch_size, shuffle=True, validation_split=0.2)\n",
    "    \n",
    "    # Train the adversarial model (encoder trying to fool the discriminator)\n",
    "    adversarial_model.fit(X_train, [X_train, y_train_domains_np_onehot], epochs=1, batch_size=batch_size, validation_split=0.2, shuffle=True)\n",
    "\n",
    "# After training, get the encoded representations\n",
    "encoded_representations = encoder.predict(gene_expression_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d22638c4-373d-4d60-8cc1-65ae09096e62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0070 - val_loss: 0.1735\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 2.3847 - accuracy: 0.0938 - val_loss: 2.1801 - val_accuracy: 0.0694\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 240, in __call__\n        self.build(y_pred)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 181, in build\n        self._losses = self._conform_to_outputs(y_pred, self._losses)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 60, in _conform_to_outputs\n        struct = map_to_output_names(outputs, self._output_names, struct)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 805, in map_to_output_names\n        raise ValueError(\n\n    ValueError: Found unexpected losses or metrics that do not correspond to any Model output: dict_keys(['encoder', 'discriminator']). Valid mode output names: ['model_10', 'model_11']. Received struct is: {'encoder': 'mse', 'discriminator': 'categorical_crossentropy'}.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m discriminator\u001b[38;5;241m.\u001b[39mfit(encoded_data, y_train_domains_np_onehot, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Train the adversarial model\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[43madversarial_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_domains_np_onehot\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Assuming the model is defined with appropriate outputs\u001b[39;00m\n\u001b[1;32m     77\u001b[0m adversarial_model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mModel(inputs\u001b[38;5;241m=\u001b[39minput_layer, outputs\u001b[38;5;241m=\u001b[39m[encoder_output, discriminator_output])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filebzewudvw.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 240, in __call__\n        self.build(y_pred)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 181, in build\n        self._losses = self._conform_to_outputs(y_pred, self._losses)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 60, in _conform_to_outputs\n        struct = map_to_output_names(outputs, self._output_names, struct)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 805, in map_to_output_names\n        raise ValueError(\n\n    ValueError: Found unexpected losses or metrics that do not correspond to any Model output: dict_keys(['encoder', 'discriminator']). Valid mode output names: ['model_10', 'model_11']. Received struct is: {'encoder': 'mse', 'discriminator': 'categorical_crossentropy'}.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import layers, models\n",
    "\n",
    "# OneHotEncode domain labels\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train_domains, y_test_domains = train_test_split(\n",
    "    gene_expression_data, domain_labels, test_size=0.2\n",
    ")\n",
    "\n",
    "# Convert y_train_domains and y_test_domains to NumPy arrays (if needed)\n",
    "y_train_domains_np = np.array(y_train_domains)\n",
    "y_test_domains_np = np.array(y_test_domains)\n",
    "\n",
    "# Apply OneHotEncoding to the domain labels\n",
    "y_train_domains_np_onehot = one_hot_encoder.fit_transform(y_train_domains_np.reshape(-1, 1))\n",
    "y_test_domains_np_onehot = one_hot_encoder.transform(y_test_domains_np.reshape(-1, 1))\n",
    "\n",
    "# Define the neural network encoder\n",
    "encoding_dim = 32  # Set to match the expected input of the discriminator\n",
    "input_dim = X_train.shape[1]  # Number of features in your input data\n",
    "\n",
    "# Encoder Model\n",
    "encoder_input = layers.Input(shape=(input_dim,))\n",
    "encoder_hidden = layers.Dense(128, activation='relu')(encoder_input)\n",
    "encoder_output = layers.Dense(encoding_dim, activation='relu')(encoder_hidden)\n",
    "encoder = models.Model(inputs=encoder_input, outputs=encoder_output)\n",
    "\n",
    "# Define the domain discriminator\n",
    "discriminator_input = layers.Input(shape=(encoding_dim,))\n",
    "discriminator_hidden = layers.Dense(64, activation='relu')(discriminator_input)\n",
    "discriminator_output = layers.Dense(y_train_domains_np_onehot.shape[1], activation='softmax')(discriminator_hidden)\n",
    "discriminator = models.Model(inputs=discriminator_input, outputs=discriminator_output)\n",
    "\n",
    "# Compile models\n",
    "encoder.compile(optimizer='adam', loss='mse')\n",
    "discriminator.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define the adversarial model\n",
    "# This model takes the original input and the domain label and outputs the original input\n",
    "adversarial_input = layers.Input(shape=(input_dim,))\n",
    "adversarial_encoded = encoder(adversarial_input)\n",
    "adversarial_output = discriminator(adversarial_encoded)  # Use the discriminator's output\n",
    "adversarial_model = models.Model(inputs=adversarial_input, outputs=[adversarial_encoded, adversarial_output])\n",
    "\n",
    "# Compile the adversarial model\n",
    "adversarial_model.compile(optimizer='adam', \n",
    "                          loss={'encoder': 'mse', 'discriminator': 'categorical_crossentropy'},\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    autoencoder.fit(X_train, X_train, epochs=1, batch_size=batch_size, shuffle=True, validation_split=0.2, callbacks=[early_stopping])\n",
    "    \n",
    "    # Get encoded data from the neural network encoder\n",
    "    encoded_data = encoder.predict(X_train)  # The neural network encoder model\n",
    "    \n",
    "    # Train the domain discriminator using the encoded data and one-hot encoded domain labels\n",
    "    discriminator.fit(encoded_data, y_train_domains_np_onehot, epochs=1, batch_size=batch_size, shuffle=True, validation_split=0.2)\n",
    "    \n",
    "    # Train the adversarial model\n",
    "    adversarial_model.fit(X_train, [X_train, y_train_domains_np_onehot], epochs=1, batch_size=batch_size, validation_split=0.2, shuffle=True)\n",
    "\n",
    "    # Assuming the model is defined with appropriate outputs\n",
    "    adversarial_model = models.Model(inputs=input_layer, outputs=[encoder_output, discriminator_output])\n",
    "    \n",
    "    # Compile the model with correct loss functions\n",
    "    adversarial_model.compile(optimizer='adam', loss={'model_7': 'mse', 'model_8': 'categorical_crossentropy'})\n",
    "    \n",
    "    # Fit the model with appropriate data\n",
    "    adversarial_model.fit(X_train, [X_train, y_train_domains_np_onehot], epochs=1, batch_size=batch_size, validation_split=0.2, shuffle=True)\n",
    "\n",
    "# After training, get the encoded representations\n",
    "encoded_representations = encoder.predict(gene_expression_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71db80ca-5e01-47b7-bf95-23a16b6019d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 12165)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6931e2c9-7173-4d98-98eb-ce131df45100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 9)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_domains_np_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb225c0-6b01-4e2c-87a1-2b0d77eb6847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
