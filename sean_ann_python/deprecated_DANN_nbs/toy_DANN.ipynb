{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16359867-dc45-4b9d-92eb-55f1aacfe833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/anndata/__init__.py:51: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import anndata\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "adata = sc.read('/tmp/work/RCproject_code/sce_export.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3712eda-b59e-4f6a-9577-e5b1cd13c63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate numerical values for each batch category\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# set up categories variable\n",
    "categories = adata.obs['batch']\n",
    "# Create a LabelEncoder instance\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit and transform the categories to integers\n",
    "numerical_categories = label_encoder.fit_transform(categories)\n",
    "numerical_categories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1253fc8-6a82-45d2-ab7b-4fd2dea3dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizaiton\n",
    "gene_expression_data = adata.layers['logcounts']\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Min-max normalization\n",
    "scaler = MinMaxScaler()\n",
    "gene_expression_data = scaler.fit_transform(gene_expression_data)\n",
    "number_samples = adata.shape[0]\n",
    "number_genes = adata.shape[1]\n",
    "input_dim = number_genes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19158126-d04b-4a8c-9369-bf62c7477228",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = gene_expression_data[0,:].shape\n",
    "encoding_dim = 64  # This is the size of the encoded representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9302ec6-8fc4-4d3f-8131-ac24a1ff2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder function\n",
    "def build_encoder():    \n",
    "    model = models.Sequential()    \n",
    "\n",
    "    # Input layer\n",
    "    model.add(layers.Input(shape=input_shape)) \n",
    "\n",
    "    # 1st hidden layer\n",
    "    model.add(layers.Dense(256, activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())    \n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 2nd hidden layer\n",
    "    model.add(layers.Dense(128, activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())    \n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 3rd hidden layer (encoded representation)\n",
    "    model.add(layers.Dense(encoding_dim, activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6805b4c5-f4ab-446d-bec4-df22121aa54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the decoder function\n",
    "def build_decoder():    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Input layer with the size of the encoded representation\n",
    "    model.add(layers.Input(shape=(encoding_dim,)))  \n",
    "\n",
    "    # 1st hidden layer\n",
    "    model.add(layers.Dense(64, activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # 2nd hidden layer\n",
    "    model.add(layers.Dense(128, activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # 3rd hidden layer\n",
    "    model.add(layers.Dense(256, activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # Output layer (reconstruction of the original input)\n",
    "    model.add(layers.Dense(input_shape[0], activation='sigmoid'))  # Use 'sigmoid' if input is normalized to [0, 1]\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a39700d3-e33c-4ce3-8a26-8e650406f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_decoder():    \n",
    "#     model = models.Sequential()\n",
    "\n",
    "#     # Input layer with the size of the encoded representation\n",
    "#     model.add(layers.Input(shape=(encoding_dim,)))  \n",
    "\n",
    "#     # 1st hidden layer\n",
    "#     model.add(layers.Dense(64, activation='linear'))  \n",
    "#     model.add(layers.LeakyReLU())    \n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     #model.add(layers.Dropout(0.3))\n",
    "\n",
    "\n",
    "#     # 2nd hidden layer\n",
    "#     model.add(layers.Dense(128, activation='linear'))  \n",
    "#     model.add(layers.LeakyReLU())    \n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     #model.add(layers.Dropout(0.3))\n",
    "\n",
    "#     # 3rd hidden layer\n",
    "#     model.add(layers.Dense(256, activation='linear'))  \n",
    "#     model.add(layers.LeakyReLU())    \n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     #model.add(layers.Dropout(0.3))\n",
    "\n",
    "#     # Output layer (reconstruction of the original input)\n",
    "#     model.add(layers.Dense(input_shape[0], activation='sigmoid'))  # Use 'sigmoid' if input is normalized to [0, 1]\n",
    "\n",
    "#     return model \n",
    "\n",
    "# # Print the model summary to check the architecture\n",
    "\n",
    "# decoder = build_decoder()\n",
    "\n",
    "# #decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcb2dd78-f4a0-49be-8a38-7b24c75a834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_domain_classifier(input_shape, num_domains):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Input layer (matching the output shape of the feature extractor)\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    \n",
    "    # Hidden layer 1\n",
    "    model.add(layers.Dense(128, activation=None))\n",
    "    model.add(layers.LeakyReLU()) \n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    # Hidden layer 2\n",
    "    model.add(layers.Dense(64, activation=None))\n",
    "    model.add(layers.LeakyReLU()) \n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    # Hidden layer 3\n",
    "    model.add(layers.Dense(32, activation=None))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    # Output layer (softmax for multi-class classification)\n",
    "    model.add(layers.Dense(num_domains, activation='softmax'))  # num_domains is the number of classes\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape = (64,)  # Example input shape (output from encoder)\n",
    "num_domains = 9  # Number of different domains\n",
    "domain_classifier = build_domain_classifier(input_shape, num_domains)\n",
    "\n",
    "# Print the model summary to check the architecture\n",
    "\n",
    "#domain_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feb6de37-c5f7-4c4e-896b-aaa537ac3417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 64), found shape=(None, 12165)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Train the autoencoder\u001b[39;00m\n\u001b[1;32m     17\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 18\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgene_expression_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgene_expression_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Now you can use the autoencoder to reconstruct data and extract the encoded features\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file_if53neh.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 64), found shape=(None, 12165)\n"
     ]
    }
   ],
   "source": [
    "# Build the encoder and decoder\n",
    "encoder = build_encoder()\n",
    "decoder = build_decoder()\n",
    "\n",
    "# Create the full autoencoder model\n",
    "input_layer = layers.Input(shape=input_shape)  # This shape should match the original input data\n",
    "encoded_output = encoder(input_layer)  # This should output shape (None, 64)\n",
    "decoded_output = decoder(encoded_output)  # This should receive the encoded shape (None, 64)\n",
    "\n",
    "# Create the full autoencoder model\n",
    "autoencoder = models.Model(inputs=input_layer, outputs=decoded_output)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = autoencoder.fit(gene_expression_data, gene_expression_data,\n",
    "                          epochs=100,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          validation_split=0.2,\n",
    "                          callbacks=[early_stopping])\n",
    "\n",
    "# Now you can use the autoencoder to reconstruct data and extract the encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950872f-33ca-4b33-8c0c-696bf33846e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the low-dimensional representation\n",
    "low_dimensional_representation = encoder.predict(gene_expression_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61801e5-1f1b-4249-a01f-e41f95637d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8843d4-40a2-4e92-a0bc-7c7506782300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build autoencoder\n",
    "\n",
    "input_layer = layers.Input(shape=input_shape)\n",
    "encoded_output = encoder(input_layer)\n",
    "decoded_output = decoder(encoded_output)\n",
    "\n",
    "# Create the full autoencoder model\n",
    "autoencoder = models.Model(inputs=input_layer, outputs=decoded_output)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')  # Use appropriate loss based on your data\n",
    "\n",
    "# Train the autoencoder\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = autoencoder.fit(gene_expression_data, gene_expression_data,\n",
    "                          epochs=100,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          validation_split=0.2,\n",
    "                          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4dff4b-778d-4479-996d-4bf6593ce207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the low-dimensional representation\n",
    "low_dimensional_representation = encoder.predict(gene_expression_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6862f4ef-f8d1-4a16-ba66-7944ba9949a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories =  adata.obs['batch']\n",
    "# categories =  adata.obs['Platform']\n",
    "\n",
    "# Step 3: Run UMAP\n",
    "umap_model = umap.UMAP(n_neighbors=15,   # Controls local vs global structure\n",
    "                        min_dist=0.1,    # Controls how tightly UMAP packs points together\n",
    "                        metric='euclidean')  # Distance metric to use\n",
    "umap_result = umap_model.fit_transform(low_dimensional_representation)\n",
    "\n",
    "# Step 4: Create a scatter plot with labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Define a colormap\n",
    "unique_categories = np.unique(categories)\n",
    "cmap = ListedColormap(plt.cm.viridis(np.linspace(0, 1, len(unique_categories))))\n",
    "\n",
    "# Create a scatter plot with color coding based on categories\n",
    "scatter = plt.scatter(umap_result[:, 0], umap_result[:, 1], \n",
    "                      c=pd.Categorical(categories).codes, alpha=0.5, cmap=cmap)\n",
    "\n",
    "# Add a title and labels\n",
    "plt.title('UMAP Visualization of Data Matrix with Categories')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "\n",
    "# Create a legend\n",
    "# Create a legend with unique categories\n",
    "handles = []\n",
    "for i, cat in enumerate(unique_categories):\n",
    "    handles.append(plt.Line2D([0], [0], marker='o', color='w', label=cat,\n",
    "                                markerfacecolor=cmap(i), markersize=10))\n",
    "\n",
    "# Add legend to the plot\n",
    "plt.legend(handles=handles, title=\"Categories\")\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb01d4f3-f0f8-495d-aeac-b4c0447f6837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 2s 74ms/step - loss: 0.0552 - val_loss: 0.0148\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.0120 - val_loss: 0.0148\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.0108 - val_loss: 0.0148\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.0097 - val_loss: 0.0148\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.0091 - val_loss: 0.0148\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.0087 - val_loss: 0.0148\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.0084 - val_loss: 0.0146\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.0082 - val_loss: 0.0143\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.0085 - val_loss: 0.0138\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.0081 - val_loss: 0.0131\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.0076 - val_loss: 0.0126\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.0072 - val_loss: 0.0113\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.0082 - val_loss: 0.0113\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.0100 - val_loss: 0.0092\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.0096 - val_loss: 0.0075\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.0059 - val_loss: 0.0123\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.0086 - val_loss: 0.0117\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.0060 - val_loss: 0.0140\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.0059 - val_loss: 0.0123\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.0048 - val_loss: 0.0116\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "input_shape = gene_expression_data[0, :].shape  # This should be (12165,) if that's the feature count\n",
    "encoding_dim = 64  # This is the size of the encoded representation\n",
    "\n",
    "# Define the encoder function\n",
    "def build_encoder():    \n",
    "    model = models.Sequential()    \n",
    "\n",
    "    # Input layer\n",
    "    model.add(layers.Input(shape=input_shape)) \n",
    "\n",
    "    # 1st hidden layer\n",
    "    model.add(layers.Dense(256, activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())    \n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 2nd hidden layer\n",
    "    model.add(layers.Dense(128, activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())    \n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 3rd hidden layer (encoded representation)\n",
    "    model.add(layers.Dense(encoding_dim, activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    return model \n",
    "\n",
    "# Define the decoder function\n",
    "def build_decoder():    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Input layer with the size of the encoded representation\n",
    "    model.add(layers.Input(shape=(encoding_dim,)))  \n",
    "\n",
    "    # 1st hidden layer\n",
    "    model.add(layers.Dense(64, activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # 2nd hidden layer\n",
    "    model.add(layers.Dense(128, activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # 3rd hidden layer\n",
    "    model.add(layers.Dense(256, activation='linear'))  \n",
    "    model.add(layers.LeakyReLU())    \n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # Output layer (reconstruction of the original input)\n",
    "    model.add(layers.Dense(input_shape[0], activation='sigmoid'))  # Use 'sigmoid' if input is normalized to [0, 1]\n",
    "\n",
    "    return model \n",
    "\n",
    "# Build the encoder and decoder\n",
    "encoder = build_encoder()\n",
    "decoder = build_decoder()\n",
    "\n",
    "# Create the full autoencoder model\n",
    "input_layer = layers.Input(shape=input_shape)  # This shape should match the original input data\n",
    "encoded_output = encoder(input_layer)  # This should output shape (None, 64)\n",
    "decoded_output = decoder(encoded_output)  # This should receive the encoded shape (None, 64)\n",
    "\n",
    "# Create the full autoencoder model\n",
    "autoencoder = models.Model(inputs=input_layer, outputs=decoded_output)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = autoencoder.fit(gene_expression_data, gene_expression_data,\n",
    "                          epochs=100,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          validation_split=0.2,\n",
    "                          callbacks=[early_stopping])\n",
    "\n",
    "# Now you can use the autoencoder to reconstruct data and extract the encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9794108-22cd-44c5-9e0d-7ec51791d28d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
